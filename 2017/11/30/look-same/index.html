<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="技术是手段而不是目的"><title>Paper Survey about Do They All Look the Same Deciphering Chinese, Japanese and Koreans by Fine-Grained Deep Learning. | Tsukiyo</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + 'e5af71dc023cd2b9b7691d458518d79a';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
  </script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Paper Survey about Do They All Look the Same Deciphering Chinese, Japanese and Koreans by Fine-Grained Deep Learning.</h1><a id="logo" href="/.">Tsukiyo</a><p class="description">天道宮</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/history/"><i class="fa fa-history"> 历史</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Paper Survey about Do They All Look the Same Deciphering Chinese, Japanese and Koreans by Fine-Grained Deep Learning.</h1><div class="post-meta">Nov 30, 2017<span> | </span><span class="category"><a href="/categories/研究/">研究</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><a class="disqus-comment-count" data-disqus-identifier="2017/11/30/look-same/" href="/2017/11/30/look-same/#disqus_thread"></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-number">1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction"><span class="toc-number">2.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Related-Literature"><span class="toc-number">3.</span> <span class="toc-text">Related Literature</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Data-Collection-and-Pre-processing"><span class="toc-number">4.</span> <span class="toc-text">Data Collection and Pre-processing</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Experiments"><span class="toc-number">5.</span> <span class="toc-text">Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#We-conduct-two-experiments"><span class="toc-number">5.1.</span> <span class="toc-text">We conduct two experiments.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Face-Classification"><span class="toc-number">5.2.</span> <span class="toc-text">Face Classification</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Attribute-Classification"><span class="toc-number">5.3.</span> <span class="toc-text">Attribute Classification</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Female"><span class="toc-number">5.4.</span> <span class="toc-text">Female</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Male"><span class="toc-number">5.5.</span> <span class="toc-text">Male</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Discussion"><span class="toc-number">6.</span> <span class="toc-text">Discussion</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Limitations"><span class="toc-number">7.</span> <span class="toc-text">Limitations</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Conclusion"><span class="toc-number">8.</span> <span class="toc-text">Conclusion</span></a></li></ol></div></div><div class="post-content"><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>This study is about what extend Chinese, Japanese and Korean faces can be classified and which facial attributes offer the most important cues. First, we propose a novel way of obtaining large numbers of facial images with nationality labels. Then we train state-of-the-art neural networks with these labeled images. We are able to achieve an accuracy of 75.03% in the classification task, with chances being 33.33% and human accuracy 38.89% . Further, we train multiple facial attribute classifiers to identify the most distinctive features for each group. </p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>China, Japan and Korea are three of the world’s largest economies  with a large population. And they look very similar. </li>
<li><p>Some suggest that the main differences derive from mannerism and fashion. There also exist quite a few websites that put up Asian face classification chall-enges, which prove the difficulty of distinguishing them.</p>
</li>
<li><p>Randomly shuffled<br><img src="http://oonaavjvi.bkt.clouddn.com/look01.jpg" alt="image"></p>
</li>
<li>Grouped<br><img src="http://oonaavjvi.bkt.clouddn.com/look02.jpg" alt="image"></li>
</ul>
<h2 id="Related-Literature"><a href="#Related-Literature" class="headerlink" title="Related Literature"></a>Related Literature</h2><ul>
<li>(Farfade, Saberian, and Li 2015; Levi and Hassner 2015; Fu, He, and Hou 2014; Wang, Li, and Luo 2016) have both achieved very high accuracy in face detection, gender and race classification.</li>
<li>(Liu et al. 2015), which has achieved state-of-the-art perfor-mance, trains two networks, the first one for face localization and the second for attribute classification. Our work follows (Liu et al. 2015), and uses the same dataset as theirs, except that (1) for simplicity we use OpenCV to locate faces and (2) for ac- curacy we train a separate neural network for each attribute. </li>
</ul>
<h2 id="Data-Collection-and-Pre-processing"><a href="#Data-Collection-and-Pre-processing" class="headerlink" title="Data Collection and Pre-processing"></a>Data Collection and Pre-processing</h2><ul>
<li>We have two main data sources: Twitter and the CelebA dataset. We derive from Twitter the labeled Chinese, Japanese and Korean images, which are later used as input to the Resnet. We use CelebA to train the facial attribute classifiers. These classifiers are then used to classify the labeled Twitter images. </li>
<li>Twitter Images<br><img src="http://oonaavjvi.bkt.clouddn.com/look03.jpg" alt="image"></li>
<li>CelebA Images,<br>The CelebA dataset contains 202,599 images taken from ten thousand individuals. In this work, we follow Liu.’s practice in dividing the training, developing, and testing dataset. We use OpenCV to locate faces in each subset and eventually have 148,829 images for training, 18,255 images for development, and 18,374 images for testing.</li>
</ul>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="We-conduct-two-experiments"><a href="#We-conduct-two-experiments" class="headerlink" title="We conduct two experiments."></a>We conduct two experiments.</h3><ul>
<li>In the first experiment, we use the labeled Twitter images to fine-tune the Resnet and investigate to what extent Chinese, Japanese and Koreans can be classified. </li>
<li>In our second experiment, we train 40 facial attribute classifiers and examine which attributes contain the most important cues in distinguishing the three groups. </li>
</ul>
<h3 id="Face-Classification"><a href="#Face-Classification" class="headerlink" title="Face Classification"></a>Face Classification</h3><ul>
<li>We split our dataset into development set, validation set, and test set ( 8: 1: 1) and experiment with different architectures from shallow networks (3-5 layers) to the 16-layer VGG and 50-layer ResNet. In our experiments, all networks would converge (Figure 4), but we observe that accuracy of 75.03% with Resnet shows the best result.<br><img src="http://oonaavjvi.bkt.clouddn.com/look04.jpg" alt="image"></li>
<li>In Table 2, we report the confusion matrix for the testing images. Note that all the three peoples look equally “confusing” to the computer: the off-diagonal elements are roughly equal. The result we achieve answers in a definitive manner that Chinese, Japanese and Koreans are distinguishable. But it also suggests that this is a challenging task, which leads to our experiment on facial attribute classificatio<br><img src="http://oonaavjvi.bkt.clouddn.com/look05.jpg" alt="image"></li>
</ul>
<h3 id="Attribute-Classification"><a href="#Attribute-Classification" class="headerlink" title="Attribute Classification"></a>Attribute Classification</h3><p>We construct a separate neural network for each of the 40 attributes in the CelebA dataset. The neural nets all share the same structure.<br><img src="http://oonaavjvi.bkt.clouddn.com/look06.jpg" alt="image"></p>
<h3 id="Female"><a href="#Female" class="headerlink" title="Female"></a>Female</h3><p><img src="http://oonaavjvi.bkt.clouddn.com/look07.jpg" alt="image"></p>
<h3 id="Male"><a href="#Male" class="headerlink" title="Male"></a>Male</h3><p><img src="http://oonaavjvi.bkt.clouddn.com/look08.jpg" alt="image"></p>
<p>In Figures, we report the percentage of individuals that possess the corresponding facial attributes.</p>
<ol>
<li>Bangs are most popular among Japanese and least popular among Chinese. </li>
<li>Japanese smile the most and Chinese the least. </li>
<li>Japanese have the most eyebags, followed by Koreans. </li>
<li>Chinese are the most likely to have bushy eyebrows. </li>
<li>Koreans are the mostly likely to have black hair and Japanese are the least likely. </li>
</ol>
<h2 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h2><p>While our paper focuses on country comparisons, we also briefly summarize some of the significant findings on cross- country gender differentials that are either cultural or social in nature. </p>
<ul>
<li>females tend to smile more than males </li>
<li>men are twice more likely to be wearing glasses than women </li>
</ul>
<h2 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h2><p>Our work is built on the assumption that Twitter users, celebrity followers in particular, are representative of the demographics of the entire population. This assumption may not exactly hold as various demographic dimensions such as gender and age are skewed in Twitter (Mislove et al. 2011). Nonetheless, we believe the direction of our estimates will remain consistent, as several of our findings are confirmed by social stereotypes and research on other regions. Also, this concern could be alleviated to some extent by examining several other celebrities.</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul>
<li>In this paper, we have demonstrated that Chinese, Japanese and Koreans do look different. By assembling a large data set of labeled images and experimenting with different neural network architectures, we have achieved a remarkable accuracy 75.03%, almost twice as high as the human average accuracy. </li>
<li>We have also examined 40 facial attributes of the three populations in an effort to identify the important cues that assist classification. Our study has shown that Chinese, Japanese and Koreans do differ in several dimensions but overall are very similar. </li>
<li>Our work, which complements existing APIs such as Microsoft Cognitive Services and Face++, could find wide applications in tourism, e-commerce, social media marketing, criminal justice and even counter-terrorism.</li>
</ul>
</div><iframe src="/donate/?AliPayQR=/images/alipay.jpg&amp;WeChatQR=/images/wechatpay.jpg&amp;GitHub=null&amp;BTCQR=null&amp;BTCKEY=null&amp;PayPal=null" style="overflow-x:hidden; overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;" frameborder="0" scrolling="no"></iframe><div><ul class="post-copyright"><li class="post-copyright-link"><span>本文链接：</span><a href="/2017/11/30/look-same/">Paper Survey about Do They All Look the Same Deciphering Chinese, Japanese and Koreans by Fine-Grained Deep Learning.</a></li><li class="post-copyright-author"><span>本文作者：</span><a href="/.">Tsukiyo</a></li><li class="post-copyright-license"><span>版权声明：</span>本文基于<a href="https://creativecommons.org/licenses/by-nc-nd/3.0/cn/deed.zh">署名-非商业性使用-禁止演绎 3.0 中国大陆许可协议 (CC BY-NC-ND 3.0 CN)</a>发布，欢迎转载，但是必须保留本文的署名<a href="https://xhxt2008.github.io/">Tsukiyo</a>及链接。如果要用于商业目的，请联系作者。</li></ul></div><br><div class="tags"><a href="/tags/未翻译/">未翻译</a><a href="/tags/英文/">英文</a><a href="/tags/人脸表情识别/">人脸表情识别</a><a href="/tags/人脸识别/">人脸识别</a><a href="/tags/表情识别/">表情识别</a><a href="/tags/神经网络/">神经网络</a></div><div class="post-nav"><a class="pre" href="/2017/12/21/Combining-feature-selection/">Combining multiple feature selection methods for stock prediction Union, intersection, and multi-intersection approaches</a><a class="next" href="/2017/11/30/OLM/">Paper Survey about Mining Opinion Leaders in Big Social Network</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论（请确保 Disqus 可以正常加载）</button></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'https://xhxt2008.github.io/2017/11/30/look-same/';
    this.page.identifier = '2017/11/30/look-same/';
    this.page.title = 'Paper Survey about Do They All Look the Same Deciphering Chinese, Japanese and Koreans by Fine-Grained Deep Learning.';
  };</script><script type="text/javascript" id="disqus-lazy-load-script">$.ajax({
url: 'https://disqus.com/next/config.json',
timeout: 2500,
type: 'GET',
success: function(){
  var d = document;
  var s = d.createElement('script');
  s.src = '//https-xhxt2008-github-io.disqus.com/embed.js';
  s.setAttribute('data-timestamp', + new Date());
  (d.head || d.body).appendChild(s);
  $('.disqus_click_btn').css('display', 'none');
},
error: function() {
  $('.disqus_click_btn').css('display', 'block');
}
});</script><script type="text/javascript" id="disqus-click-load">$('.btn_click_load').click(() => {  //click to load comments
    (() => { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//https-xhxt2008-github-io.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
    $('.disqus_click_btn').css('display','none');
});</script><script type="text/javascript" id="disqus-count-script">$(function() {
     var xhr = new XMLHttpRequest();
     xhr.open('GET', '//disqus.com/next/config.json', true);
     xhr.timeout = 2500;
     xhr.onreadystatechange = function () {
       if (xhr.readyState === 4 && xhr.status === 200) {
         $('.post-meta .post-comments-count').show();
         var s = document.createElement('script');
         s.id = 'dsq-count-scr';
         s.src = 'https://https-xhxt2008-github-io.disqus.com/count.js';
         s.async = true;
         (document.head || document.body).appendChild(s);
       }
     };
     xhr.ontimeout = function () { xhr.abort(); };
     xhr.send(null);
   });
</script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><input class="st-default-search-input" placeholder="Search" type="text"/></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/技术/">技术</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/日志/">日志</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/杂记/">杂记</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/研究/">研究</a><span class="category-list-count">7</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/神经网络/" style="font-size: 15px;">神经网络</a> <a href="/tags/特征选择/" style="font-size: 15px;">特征选择</a> <a href="/tags/未翻译/" style="font-size: 15px;">未翻译</a> <a href="/tags/英文/" style="font-size: 15px;">英文</a> <a href="/tags/人脸表情识别/" style="font-size: 15px;">人脸表情识别</a> <a href="/tags/人脸识别/" style="font-size: 15px;">人脸识别</a> <a href="/tags/表情识别/" style="font-size: 15px;">表情识别</a> <a href="/tags/论文查找/" style="font-size: 15px;">论文查找</a> <a href="/tags/日文/" style="font-size: 15px;">日文</a> <a href="/tags/社交网络/" style="font-size: 15px;">社交网络</a> <a href="/tags/意见领袖/" style="font-size: 15px;">意见领袖</a> <a href="/tags/数据挖掘/" style="font-size: 15px;">数据挖掘</a> <a href="/tags/聚类/" style="font-size: 15px;">聚类</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/VC维/" style="font-size: 15px;">VC维</a> <a href="/tags/深度学习/" style="font-size: 15px;">深度学习</a> <a href="/tags/特征工程/" style="font-size: 15px;">特征工程</a> <a href="/tags/动态规划/" style="font-size: 15px;">动态规划</a> <a href="/tags/时间序列/" style="font-size: 15px;">时间序列</a> <a href="/tags/动态表情/" style="font-size: 15px;">动态表情</a> <a href="/tags/数据分析/" style="font-size: 15px;">数据分析</a> <a href="/tags/竞赛/" style="font-size: 15px;">竞赛</a> <a href="/tags/平安产险/" style="font-size: 15px;">平安产险</a> <a href="/tags/动漫/" style="font-size: 15px;">动漫</a> <a href="/tags/人渣的本愿/" style="font-size: 15px;">人渣的本愿</a> <a href="/tags/剧透/" style="font-size: 15px;">剧透</a> <a href="/tags/评论/" style="font-size: 15px;">评论</a> <a href="/tags/茶太/" style="font-size: 15px;">茶太</a> <a href="/tags/歌词翻译/" style="font-size: 15px;">歌词翻译</a> <a href="/tags/同人音乐/" style="font-size: 15px;">同人音乐</a> <a href="/tags/数据可视化/" style="font-size: 15px;">数据可视化</a> <a href="/tags/knn/" style="font-size: 15px;">knn</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/09/19/pinganA2/">平安产险2018极客挑战赛·初赛（二）</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/08/11/pinganA1/">平安产险2018极客挑战赛·初赛（一）</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/11/VC-dimension/">通俗易懂，什么是VC维</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/07/deep-learning/">上帝归上帝，恺撒归凯撒，传统方法与深度学习之辨。</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/21/Combining-feature-selection/">Combining multiple feature selection methods for stock prediction Union, intersection, and multi-intersection approaches</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/30/look-same/">Paper Survey about Do They All Look the Same Deciphering Chinese, Japanese and Koreans by Fine-Grained Deep Learning.</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/30/OLM/">Paper Survey about Mining Opinion Leaders in Big Social Network</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/22/paper-survey-kyoto/">Paper survey about Timing-Based Facial Expression Recognition of Kyoto University by Prof. Kawashima 　</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/22/Face-expression-recognition/">Paper survey about Face Expression Recognition</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/06/sk-learn-001/">sk-learn学习笔记，简单的knn分类和数据可视化实践</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-comment-o"> 最近评论</i></div><script type="text/javascript" src="//https-xhxt2008-github-io.disqus.com/recent_comments_widget.js?num_items=5&amp;hide_avatars=1&amp;avatar_size=32&amp;excerpt_length=20&amp;hide_mods=1"></script></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://www.sitixi.com" title="STONEX" target="_blank">STONEX</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">Tsukiyo.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"><script>(function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
(w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
})(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');
_st('install','HyWyJnTdw-UKr1LBb7JD','2.0.0');
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>