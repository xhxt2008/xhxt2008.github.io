<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="技术是手段而不是目的"><title>Combining multiple feature selection methods for stock prediction Union, intersection, and multi-intersection approaches | Tsukiyo</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/7.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Combining multiple feature selection methods for stock prediction Union, intersection, and multi-intersection approaches</h1><a id="logo" href="/.">Tsukiyo</a><p class="description">天道宮</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/history/"><i class="fa fa-history"> 历史</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Combining multiple feature selection methods for stock prediction Union, intersection, and multi-intersection approaches</h1><div class="post-meta">Dec 21, 2017<span> | </span><span class="category"><a href="/categories/研究/">研究</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><a data-disqus-identifier="2017/12/21/Combining-feature-selection/" href="/2017/12/21/Combining-feature-selection/#disqus_thread" class="disqus-comment-count"></a><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-number">1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction"><span class="toc-number">2.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Related-work"><span class="toc-number">3.</span> <span class="toc-text">Related work</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Literature-review"><span class="toc-number">4.</span> <span class="toc-text">Literature review</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Stock-price-theory"><span class="toc-number">4.1.</span> <span class="toc-text">Stock price theory</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Stock-price-analysis-methods"><span class="toc-number">4.2.</span> <span class="toc-text">Stock price analysis methods</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Feature-selection"><span class="toc-number">4.3.</span> <span class="toc-text">Feature selection</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Principal-Component-Analysis-PCA"><span class="toc-number">4.4.</span> <span class="toc-text">Principal Component Analysis (PCA)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GA-SVM"><span class="toc-number">4.5.</span> <span class="toc-text">GA-SVM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Classification-and-Regression-Trees-CART"><span class="toc-number">4.6.</span> <span class="toc-text">Classification and Regression Trees (CART)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Experimental-design"><span class="toc-number">5.</span> <span class="toc-text">Experimental design</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#The-first-experimental-stage"><span class="toc-number">5.1.</span> <span class="toc-text">The first experimental stage</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#The-second-experimental-stage"><span class="toc-number">5.2.</span> <span class="toc-text">The second experimental stage</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Results"><span class="toc-number">6.</span> <span class="toc-text">Results</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Single-feature-selection-methods"><span class="toc-number">6.1.</span> <span class="toc-text">Single feature selection methods</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multiple-feature-selection-methods"><span class="toc-number">6.2.</span> <span class="toc-text">Multiple feature selection methods</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Conclusion"><span class="toc-number">7.</span> <span class="toc-text">Conclusion</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Reference"><span class="toc-number">7.1.</span> <span class="toc-text">Reference</span></a></li></ol></li></ol></div></div><div class="post-content"><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>To effectively predict stock price for investors is a very important research problem. In literature, data mining techniques have been applied to stock (market) prediction. Feature selection, a pre-processing step of data mining, aims at filtering out unrepresentative variables from a given dataset for effective prediction. As using different feature selection methods will lead to different features selected and thus affect the prediction performance, the purpose of this paper is to combine multiple feature selection methods to identify more representative variables for better prediction. </p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Stock investments are a very popular investment activity around the world. In literature, some basic important factors, such as financial ratios, technical indexes, and macroeconomic indexes have been proved as the important factors of affecting stocks’ rise and fall. However, different studies select their factors (i.e. input variables) differently for their prediction models [3]. That is, the opinion of the important factors for stock prediction is somewhat different in related work since there is no exact answer to the question of what are the most representative variables. On the other hand, it is the fact that using different input variables can make the same prediction model performs differently. Therefore, constructing the optimal stock prediction model for investors is very challenging.<br><strong>Feature selection</strong> can be used to filter out redundant and/or irrelevant features from a chosen dataset resulting in more representative features for better prediction performances.<br>That is, the chosen feature selection method is supposed to select usable features for stock prediction. However, using different feature selection methods is likely to produce different results Therefore, if we could apply a number of different feature selection methods and then combine the selection results, we can not only understand the most important and representative variables that all the feature selection methods ‘agree’, but also further improve prediction performances over using one single feature selection methods. </p>
<h2 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h2><p><img src="http://oonaavjvi.bkt.clouddn.com/FS01.png" alt="Related work"></p>
<h2 id="Literature-review"><a href="#Literature-review" class="headerlink" title="Literature review"></a>Literature review</h2><h3 id="Stock-price-theory"><a href="#Stock-price-theory" class="headerlink" title="Stock price theory"></a>Stock price theory</h3><ul>
<li>Stock prices mean the actual transaction price through the buyers and sellers in the market. Stock prices are determined by the laws of supply and demand. In theory, whether the price of a stock is high or low, it is decided by the buyers and sellers’ transactions in the open market. When supply and demand change, the stock price must be changed. </li>
<li>Fama‘s Efficient Market Hypothesis supposes that the investment activity is a “Fair-Game Market”. It means all information has disclosed in the stock market, and reflects on stock prices. According to the difference of disclosed information, there are three kinds of Efficient Market Hypothesis：<ul>
<li>The Weak Form Efficient Market </li>
<li>The Semi-strong Form Efficient Market</li>
<li>The Strong Form Efficient Market </li>
</ul>
</li>
</ul>
<h3 id="Stock-price-analysis-methods"><a href="#Stock-price-analysis-methods" class="headerlink" title="Stock price analysis methods"></a>Stock price analysis methods</h3><ul>
<li><strong>Fundamental analysis</strong>: Fundamental analysis believes that every stock has its intrinsic value. If the share prices lower than the intrinsic value, it means the stock is undervalued. In addition, economic factors also belong to this category. </li>
<li><strong>Technical analysis</strong>. Technical analysis, also known as “charting”, has been a part of financial practice for many decades. It studies the historical price and volume movement of a stock by using charts as the primary tool to forecast future price movements. This theory believes that the trends and patterns of an investment instrument’s price, volume, breadth, and the trading activities reflect most of the relevant market information that a decision maker can utilize to determine its value. </li>
</ul>
<h3 id="Feature-selection"><a href="#Feature-selection" class="headerlink" title="Feature selection"></a>Feature selection</h3><p>In many research problems, such as pattern recognition, it is important to choose a group of set of attributions with more prediction information. That is, if the number of irrelevant or redundant features is reduced drastically, the running time of a learning algorithm is also reduced. Moreover, a more general concept can be yielded. Performing feature selection can lead to many potential benefits, which are facilitating data visualization and data understanding, reducing the measurement and storage requirements, reducing training and utilization times, defying the curse of dimensionality to improve prediction performances, etc. </p>
<h3 id="Principal-Component-Analysis-PCA"><a href="#Principal-Component-Analysis-PCA" class="headerlink" title="Principal Component Analysis (PCA)"></a>Principal Component Analysis (PCA)</h3><p>Principal Component Analysis (PCA) is a multivariate statistical technique. It aims at reducing the dimensionality of a dataset with a large number of interrelated variables. In particular, it extracts a small set of factors or components that are constituted of highly correlated elements, while retaining their original characters. After performing PCA, the uncorrelated variables which are called components, will replace the original variables.<br><img src="http://oonaavjvi.bkt.clouddn.com/FS02.png" alt="Principal Component Analysis"></p>
<h3 id="GA-SVM"><a href="#GA-SVM" class="headerlink" title="GA-SVM"></a>GA-SVM</h3><p>The main idea of Genetic Algorithms (GA) is from Darwin’s theory of evolution from natural selection in the survival of the fittest. GA attempts to computationally mimic the processes by which natural selection operates.<br><img src="http://oonaavjvi.bkt.clouddn.com/FS03.jpg" alt="GA-SVM"></p>
<h3 id="Classification-and-Regression-Trees-CART"><a href="#Classification-and-Regression-Trees-CART" class="headerlink" title="Classification and Regression Trees (CART)"></a>Classification and Regression Trees (CART)</h3><p>The Classification and Regression Trees (CART) is a statistical technique that can select from a large number of explanatory variables those that are most important in determining the response variable to be explained. The decision trees produced by CART are strictly binary, containing exactly two branches for each decision tree. The root node t is separated into two samples based on some condition. The samples that fit the condition will be separated into the left nodes (tl), and the others will be separated into the right nodes (tr).<br>In particular, a decision tree is based on the entropy theory that the attribute (or feature) with the highest information gain (or greatest entropy reduction) is chosen as the test attribute for the non-leaf node<br>$$g\left ( Y,A \right )=H\left ( Y \right )-H\left ( Y|A \right )$$</p>
<p>Related work only applies one chosen feature selection method to filter out irrelevant variables. This motivates us to collect all relevant variables used for stock prediction in literature and then combining multiple feature selection methods to identify more representative variables for improving prediction performances. </p>
<h2 id="Experimental-design"><a href="#Experimental-design" class="headerlink" title="Experimental design"></a>Experimental design</h2><h3 id="The-first-experimental-stage"><a href="#The-first-experimental-stage" class="headerlink" title="The first experimental stage"></a>The first experimental stage</h3><p><img src="http://oonaavjvi.bkt.clouddn.com/FS04.jpg" alt="The first experimental stage"></p>
<h3 id="The-second-experimental-stage"><a href="#The-second-experimental-stage" class="headerlink" title="The second experimental stage"></a>The second experimental stage</h3><p><img src="http://oonaavjvi.bkt.clouddn.com/FS05.jpg" alt="The second experimental stage"></p>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><h3 id="Single-feature-selection-methods"><a href="#Single-feature-selection-methods" class="headerlink" title="Single feature selection methods"></a>Single feature selection methods</h3><p><img src="http://oonaavjvi.bkt.clouddn.com/FS06.png" alt="Single method"><br><img src="http://oonaavjvi.bkt.clouddn.com/FS07.png" alt="Single method"><br>Figures show the rate of prediction accuracy of the four different MLP models based on the one quarter and other-quarter based testing datasets respectively. As we can see, the results are slightly different if different testing datasets are considered.<br>We can find the models with feature selection are obviously better. And They have similar effects</p>
<h3 id="Multiple-feature-selection-methods"><a href="#Multiple-feature-selection-methods" class="headerlink" title="Multiple feature selection methods"></a>Multiple feature selection methods</h3><p><img src="http://oonaavjvi.bkt.clouddn.com/FS08.png" alt="Multiple method"><br><img src="http://oonaavjvi.bkt.clouddn.com/FS09.png" alt="Multiple method"></p>
<p>The results indicate that the intersection between PCA and GA outperforms the other combination approaches over the one quarter based testing dataset. On the other hand, combining multiple feature selection methods by the multi-intersection approach performs the best based on the other-quarter based testing dataset. However, the rates of prediction accuracy by both combination approaches over the two testing datasets do not have a big difference, i.e. less than 0.3%. </p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul>
<li>This paper compares three different feature selection methods, i.e. Principal Component Analysis (PCA), Genetic Algorithms (GA), and decision trees (CART) and combines them based on union, intersection, and multi- intersection approaches to examine their prediction accuracy and errors. </li>
<li>The experimental results show that combining multiple feature selection methods can provide better prediction performances than using single feature selection methods. In particular, the intersection between PCA and GA and the multi-intersection of PCA, GA, and CART perform the best, which provide the highest rate of prediction accuracy and the lowest error rate of predicting stocks’ rise. </li>
<li>Moreover, these two combined approaches select 14 and 17 important variables respectively from the 85 original variables, which filter out many unrepresentative variables. These variables can be used for practical investment decisions.</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li>Qing-Guo Wang, “Linear, Adaptive and Nonlinear Trading Models for Singapore Stock Market with Random Forests”, 2012</li>
</ul>
</div><iframe src="/donate/?AliPayQR=/images/alipay.jpg&amp;WeChatQR=/images/wechatpay.jpg&amp;GitHub=null&amp;BTCQR=null&amp;BTCKEY=null&amp;PayPal=null" style="overflow-x:hidden;overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;" frameborder="0" scrolling="no"></iframe><div class="tags"><a href="/tags/特征选择/">特征选择</a><a href="/tags/特征工程/">特征工程</a><a href="/tags/未翻译/">未翻译</a><a href="/tags/英文/">英文</a></div><div class="post-nav"><a href="/2017/11/30/look-same/" class="next">Paper Survey about Do They All Look the Same Deciphering Chinese, Japanese and Koreans by Fine-Grained Deep Learning.</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论 「请确保 disqus.com 可以正常加载」</button></div><script>var disqus_shortname = 'https-xhxt2008-github-io';
var disqus_identifier = '2017/12/21/Combining-feature-selection/';
var disqus_title = 'Combining multiple feature selection methods for stock prediction Union, intersection, and multi-intersection approaches';
var disqus_url = 'https://xhxt2008.github.io/2017/12/21/Combining-feature-selection/';
$('.btn_click_load').click(function() {
  (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
  $('.btn_click_load').css('display','none');
});
$.ajax({
  url: 'https://disqus.com/next/config.json',
  timeout: 3000,
  type: 'GET',
  success: (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    $('.btn_click_load').css('display','none');
  })(),
  error: function() {
    $('.btn_click_load').css('display','block');
  }
});</script><script id="dsq-count-scr" src="//https-xhxt2008-github-io.disqus.com/count.js" async></script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><input placeholder="Search" type="text" class="st-default-search-input"/></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/技术/">技术</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/日志/">日志</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/杂记/">杂记</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/研究/">研究</a><span class="category-list-count">5</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/神经网络/" style="font-size: 15px;">神经网络</a> <a href="/tags/特征选择/" style="font-size: 15px;">特征选择</a> <a href="/tags/未翻译/" style="font-size: 15px;">未翻译</a> <a href="/tags/英文/" style="font-size: 15px;">英文</a> <a href="/tags/人脸表情识别/" style="font-size: 15px;">人脸表情识别</a> <a href="/tags/人脸识别/" style="font-size: 15px;">人脸识别</a> <a href="/tags/表情识别/" style="font-size: 15px;">表情识别</a> <a href="/tags/论文查找/" style="font-size: 15px;">论文查找</a> <a href="/tags/日文/" style="font-size: 15px;">日文</a> <a href="/tags/社交网络/" style="font-size: 15px;">社交网络</a> <a href="/tags/意见领袖/" style="font-size: 15px;">意见领袖</a> <a href="/tags/数据挖掘/" style="font-size: 15px;">数据挖掘</a> <a href="/tags/聚类/" style="font-size: 15px;">聚类</a> <a href="/tags/特征工程/" style="font-size: 15px;">特征工程</a> <a href="/tags/时间序列/" style="font-size: 15px;">时间序列</a> <a href="/tags/动态表情/" style="font-size: 15px;">动态表情</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/数据可视化/" style="font-size: 15px;">数据可视化</a> <a href="/tags/knn/" style="font-size: 15px;">knn</a> <a href="/tags/茶太/" style="font-size: 15px;">茶太</a> <a href="/tags/歌词翻译/" style="font-size: 15px;">歌词翻译</a> <a href="/tags/同人音乐/" style="font-size: 15px;">同人音乐</a> <a href="/tags/动漫/" style="font-size: 15px;">动漫</a> <a href="/tags/人渣的本愿/" style="font-size: 15px;">人渣的本愿</a> <a href="/tags/剧透/" style="font-size: 15px;">剧透</a> <a href="/tags/评论/" style="font-size: 15px;">评论</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/12/21/Combining-feature-selection/">Combining multiple feature selection methods for stock prediction Union, intersection, and multi-intersection approaches</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/30/look-same/">Paper Survey about Do They All Look the Same Deciphering Chinese, Japanese and Koreans by Fine-Grained Deep Learning.</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/30/OLM/">Paper Survey about Mining Opinion Leaders in Big Social Network</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/22/paper-survey-kyoto/">Paper survey about Timing-Based Facial Expression Recognition of Kyoto University by Prof. Kawashima 　</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/22/Face-expression-recognition/">Paper survey about Face Expression Recognition</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/06/sk-learn-001/">sk-learn学习笔记，简单的knn分类和数据可视化实践</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/11/笑/">(笑)</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/05/02/kuzunohonkai/">【剧透！】深挖剧情（三观正直），如何正确观看《人渣的本愿》</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-comment-o"> 最近评论</i></div><script type="text/javascript" src="//https-xhxt2008-github-io.disqus.com/recent_comments_widget.js?num_items=5&amp;hide_avatars=1&amp;avatar_size=32&amp;excerpt_length=20&amp;hide_mods=1"></script></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://www.sitixi.com" title="STONEX" target="_blank">STONEX</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">Tsukiyo.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.css"><script>(function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
(w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
})(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');
_st('install','HyWyJnTdw-UKr1LBb7JD','2.0.0');
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?e5af71dc023cd2b9b7691d458518d79a";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>