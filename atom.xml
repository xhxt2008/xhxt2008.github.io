<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Tsukiyo</title>
  <subtitle>天道宮</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://xhxt2008.github.io/"/>
  <updated>2018-09-19T09:25:29.524Z</updated>
  <id>https://xhxt2008.github.io/</id>
  
  <author>
    <name>Tsukiyo</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>平安产险2018极客挑战赛·初赛（二）</title>
    <link href="https://xhxt2008.github.io/2018/09/19/pinganA2/"/>
    <id>https://xhxt2008.github.io/2018/09/19/pinganA2/</id>
    <published>2018-09-19T06:56:00.000Z</published>
    <updated>2018-09-19T09:25:29.524Z</updated>
    
    <content type="html"><![CDATA[<h2 id="初赛数据预处理"><a href="#初赛数据预处理" class="headerlink" title="初赛数据预处理"></a>初赛数据预处理</h2><h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p> 经过上一阶段对数据分析，我们大体上根据情况的不同，有以下四种处理方式：</p>
<ol>
<li><strong>删除特征</strong>：null值过多的特征</li>
<li><strong>填充特征的null值</strong>：有一些null值，可以依据一些规则，使填充值的影响最小。如填充平均值，众数，奇异值等等。</li>
<li><strong>二值化</strong>：对于只有两种状体啊的特征。如：male，female。</li>
<li><strong>独热编码</strong>：对于有多种状态的特征，如：地区信息</li>
<li><strong>特殊处理方法</strong>：依据具体情况</li>
</ol>
<h3 id="删除特征"><a href="#删除特征" class="headerlink" title="删除特征"></a>删除特征</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">for delete: grade,zip_code,inq_last_12m,total_cu_tl,open_acc_6m,open_il_12m,open_il_24m,open_il_6m,total_bal_il,</span></span><br><span class="line"><span class="string">open_rv_12m,open_rv_24m,max_bal_bc,all_util,inq_fi,policy_code,emp_title,emp_title,desc,title,zip_code,dti_joint,</span></span><br><span class="line"><span class="string">annual_inc_joint,varification_status_joint,il_uti,mths_since_last_record,maddr_state,</span></span><br><span class="line"><span class="string">issue_d,earliest_cr_line</span></span><br><span class="line"><span class="string">新增：loan_status,addr_state,application_type,pymnt_plan</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>feature_delete = &#123;<span class="string">'grade'</span>,<span class="string">'zip_code'</span>,<span class="string">'inq_last_12m'</span>,<span class="string">'total_cu_tl'</span>,<span class="string">'open_acc_6m'</span>,<span class="string">'open_il_12m'</span>,<span class="string">'open_il_24m'</span>,</span><br><span class="line">                  <span class="string">'open_il_6m'</span>,<span class="string">'total_bal_il'</span>,<span class="string">'open_rv_24m'</span>,<span class="string">'open_rv_12m'</span>,<span class="string">'max_bal_bc'</span>,<span class="string">'all_util'</span>,<span class="string">'inq_fi'</span>,</span><br><span class="line">                 <span class="string">'emp_title'</span>,<span class="string">'policy_code'</span>,<span class="string">'desc'</span>,<span class="string">'title'</span>,<span class="string">'dti_joint'</span>,<span class="string">'annual_inc_joint'</span>,<span class="string">'addr_state'</span>,<span class="string">'mths_since_rcnt_il'</span>,</span><br><span class="line">                 <span class="string">'verification_status_joint'</span>,<span class="string">'il_util'</span>,<span class="string">'mths_since_last_record'</span>,<span class="string">'issue_d'</span>,<span class="string">'earliest_cr_line'</span>,</span><br><span class="line">                  <span class="string">'loan_status'</span>,<span class="string">'addr_state'</span>,<span class="string">'application_type'</span>,<span class="string">'pymnt_plan'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = data.drop(feature_delete,axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>在上一篇博客中，我们分析了数据集中的空值情况。以上的特征由于各种原因我们予以删除的。除了null值过多的特征意外，比方说’zip_code’邮政编码，过于细化，而且跟地理信息重复。两个特征如果相关性过高（协方差大于0.95），我们也会删除到只剩一个。另外如果我们把某个特征转化成别的信息之后，我们也会删除原特征。</p>
<h3 id="填充null值"><a href="#填充null值" class="headerlink" title="填充null值"></a>填充null值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">少量null值的处理，用均值替代：emp_length, revol_util, annual_inc, total_acc, earliest_cr_line, tot_cur_bal, tot_coll_amt</span></span><br><span class="line"><span class="string">total_rev_hi_lim, revol_util, pub_rec, collections_12_mths_ex_med</span></span><br><span class="line"><span class="string">减少：emp_length</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>feature_fill_null = &#123;<span class="string">'revol_util'</span>,<span class="string">'annual_inc'</span>,<span class="string">'total_acc'</span>,<span class="string">'earliest_cr_line_year'</span>,<span class="string">'earliest_cr_line_month'</span>,<span class="string">'tot_cur_bal'</span>,</span><br><span class="line">                    <span class="string">'tot_coll_amt'</span>,<span class="string">'total_rev_hi_lim'</span>,<span class="string">'revol_util'</span>,<span class="string">'pub_rec'</span>,<span class="string">'collections_12_mths_ex_med'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">fill_null</span><span class="params">(feature_fill_null,data)</span>:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    <span class="keyword">for</span> items <span class="keyword">in</span> feature_fill_null:</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>        <span class="keyword">try</span>:</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>            time.strptime(data[items].mode()[<span class="number">0</span>],<span class="string">"%b-%Y"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>            data[items] = data[items].fillna(data[items].mode()[<span class="number">0</span>]) <span class="comment">#日期的空值用众数替代</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>            <span class="keyword">print</span> (data[items].mode()[<span class="number">0</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>        <span class="keyword">except</span>:</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>            data[items] = data[items].fillna(data[items].mean()) <span class="comment">#其他空值用均值替代</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>            <span class="keyword">print</span> (data[items].mean())</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fill_null(feature_fill_null,data)</span><br></pre></td></tr></table></figure>
<p>填充null值这里有两种<br>（或三种）情况。</p>
<ul>
<li>如果特征是日期型变量，我们取其中最多的日期作为填充值，以减小null值的冲击。</li>
<li>如果特征值是数值类型，如雇佣时长，总贷款额等。我们计算正常值的平均数作为填充值。</li>
<li>如果null值有特殊的意义（我们想象逾期未补款时间，null为没有逾期或者没有借款，区别于0），我们应该赋予null一个异常值，比如说一个较大的数。</li>
</ul>
<h3 id="二值化"><a href="#二值化" class="headerlink" title="二值化"></a>二值化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#for binarilization：pymnt_plan,term, application_type, initial_list_status</span></span><br><span class="line"><span class="comment">#减少：pymnt_plan，application_type  增加：loan_condition,region</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>feature_for_binary = &#123;<span class="string">'term'</span>,<span class="string">'initial_list_status'</span>,<span class="string">'loan_condition'</span>,<span class="string">'region'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">binary</span><span class="params">(feature_for_binary,data)</span>:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    <span class="keyword">for</span> items <span class="keyword">in</span> feature_for_binary: </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>        mapping = &#123;label:idx <span class="keyword">for</span> idx,label <span class="keyword">in</span> enumerate(set(data[items]))&#125;  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>        data[items] = data[items].map(mapping) </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    <span class="keyword">return</span> data</span><br><span class="line">    </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>binary(feature_for_binary,data)</span><br></pre></td></tr></table></figure>
<p>二值化很好理解，经过分析，我们认为上面的两个属于只有两个取值的离散型变量。</p>
<h3 id="独热编码"><a href="#独热编码" class="headerlink" title="独热编码"></a>独热编码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#for get_dummies: home_ownership, verification_status, loan_status,purpose</span></span><br><span class="line"><span class="comment">#减少：loan_status</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>feature_for_dummy = &#123;<span class="string">'home_ownership'</span>,<span class="string">'verification_status'</span>,<span class="string">'purpose'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">multi_get_dummies</span><span class="params">(feature_for_dummy,data)</span>:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    <span class="keyword">for</span> items <span class="keyword">in</span> feature_for_dummy: </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>        data = data.join(pd.get_dummies(data[items],prefix=items))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>        data.pop(items)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    <span class="keyword">return</span> data    </span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = multi_get_dummies(feature_for_dummy,data)</span><br></pre></td></tr></table></figure>
<p>对于离散多类型变量，我们为了取消类型之间的线性相关性，不能直接对他们编号。需要使用独热编码对不同类型进行编号。<br>值得注意的是，独热编码在特征类型很多的情况下会显著增加特征的维度，造成特征过于稀疏不利于学习。因此对于特征种类过多的我们进行了特殊处理，后面会详细介绍。</p>
<h3 id="特殊的处理"><a href="#特殊的处理" class="headerlink" title="特殊的处理"></a>特殊的处理</h3><h4 id="信用评级sub-grade和工作年限emp-length"><a href="#信用评级sub-grade和工作年限emp-length" class="headerlink" title="信用评级sub_grade和工作年限emp_length"></a>信用评级sub_grade和工作年限emp_length</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>sub_grade_mapping = &#123;<span class="string">'A1'</span>:<span class="number">0</span>,<span class="string">'A2'</span>:<span class="number">1</span>,<span class="string">'A3'</span>:<span class="number">2</span>,<span class="string">'A4'</span>:<span class="number">3</span>,<span class="string">'A5'</span>:<span class="number">4</span>,</span><br><span class="line">                     <span class="string">'B1'</span>:<span class="number">5</span>,<span class="string">'B2'</span>:<span class="number">6</span>,<span class="string">'B3'</span>:<span class="number">7</span>,<span class="string">'B4'</span>:<span class="number">8</span>,<span class="string">'B5'</span>:<span class="number">9</span>,</span><br><span class="line">                     <span class="string">'C1'</span>:<span class="number">10</span>,<span class="string">'C2'</span>:<span class="number">11</span>,<span class="string">'C3'</span>:<span class="number">12</span>,<span class="string">'C4'</span>:<span class="number">13</span>,<span class="string">'C5'</span>:<span class="number">14</span>,</span><br><span class="line">                     <span class="string">'D1'</span>:<span class="number">15</span>,<span class="string">'D2'</span>:<span class="number">16</span>,<span class="string">'D3'</span>:<span class="number">17</span>,<span class="string">'D4'</span>:<span class="number">18</span>,<span class="string">'D5'</span>:<span class="number">19</span>,</span><br><span class="line">                     <span class="string">'E1'</span>:<span class="number">20</span>,<span class="string">'E2'</span>:<span class="number">21</span>,<span class="string">'E3'</span>:<span class="number">22</span>,<span class="string">'E4'</span>:<span class="number">23</span>,<span class="string">'E5'</span>:<span class="number">24</span>,</span><br><span class="line">                     <span class="string">'F1'</span>:<span class="number">25</span>,<span class="string">'F2'</span>:<span class="number">26</span>,<span class="string">'F3'</span>:<span class="number">27</span>,<span class="string">'F4'</span>:<span class="number">28</span>,<span class="string">'F5'</span>:<span class="number">29</span>,</span><br><span class="line">                     <span class="string">'G1'</span>:<span class="number">30</span>,<span class="string">'G2'</span>:<span class="number">31</span>,<span class="string">'G3'</span>:<span class="number">32</span>,<span class="string">'G4'</span>:<span class="number">33</span>,<span class="string">'G5'</span>:<span class="number">34</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data.sub_grade = data.sub_grade.map(sub_grade_mapping)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>emp_length_mapping = &#123;<span class="string">'10+ years'</span>:<span class="number">10</span>,<span class="string">'9 years'</span>:<span class="number">9</span>,<span class="string">'8 years'</span>:<span class="number">8</span>,<span class="string">'7 years'</span>:<span class="number">7</span>,<span class="string">'6 years'</span>:<span class="number">6</span>,</span><br><span class="line">                       <span class="string">'5 years'</span>:<span class="number">5</span>,<span class="string">'4 years'</span>:<span class="number">4</span>,<span class="string">'3 years'</span>:<span class="number">3</span>,<span class="string">'2 years'</span>:<span class="number">2</span>,<span class="string">'1 year'</span>:<span class="number">1</span>,<span class="string">'&lt; 1 year'</span>:<span class="number">0.5</span>,<span class="string">'n/a'</span>:<span class="number">0</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data.emp_length = data.emp_length.map(emp_length_mapping)</span><br></pre></td></tr></table></figure>
<p>不同的信用评级和工作年限是有线性关系的，为了准确学出这种关系，我们把这两个字段映射成对应的数字。<br>再对null值的处理上，信用评级我们采用平均值填充，但是对于工作年限，我们认为null可能是没有工作，我们填充0来表示。</p>
<h4 id="地址所在的州addr-state"><a href="#地址所在的州addr-state" class="headerlink" title="地址所在的州addr_state"></a>地址所在的州addr_state</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 地域划分addr_state-&gt;region</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>west = [<span class="string">'CA'</span>, <span class="string">'OR'</span>, <span class="string">'UT'</span>,<span class="string">'WA'</span>, <span class="string">'CO'</span>, <span class="string">'NV'</span>, <span class="string">'AK'</span>, <span class="string">'MT'</span>, <span class="string">'HI'</span>, <span class="string">'WY'</span>, <span class="string">'ID'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>south_west = [<span class="string">'AZ'</span>, <span class="string">'TX'</span>, <span class="string">'NM'</span>, <span class="string">'OK'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>south_east = [<span class="string">'GA'</span>, <span class="string">'NC'</span>, <span class="string">'VA'</span>, <span class="string">'FL'</span>, <span class="string">'KY'</span>, <span class="string">'SC'</span>, <span class="string">'LA'</span>, <span class="string">'AL'</span>, <span class="string">'WV'</span>, <span class="string">'DC'</span>, <span class="string">'AR'</span>, <span class="string">'DE'</span>, <span class="string">'MS'</span>, <span class="string">'TN'</span> ]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mid_west = [<span class="string">'IL'</span>, <span class="string">'MO'</span>, <span class="string">'MN'</span>, <span class="string">'OH'</span>, <span class="string">'WI'</span>, <span class="string">'KS'</span>, <span class="string">'MI'</span>, <span class="string">'SD'</span>, <span class="string">'IA'</span>, <span class="string">'NE'</span>, <span class="string">'IN'</span>, <span class="string">'ND'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>north_east = [<span class="string">'CT'</span>, <span class="string">'NY'</span>, <span class="string">'PA'</span>, <span class="string">'NJ'</span>, <span class="string">'RI'</span>,<span class="string">'MA'</span>, <span class="string">'MD'</span>, <span class="string">'VT'</span>, <span class="string">'NH'</span>, <span class="string">'ME'</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data[<span class="string">'region'</span>] = np.nan</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">finding_regions</span><span class="params">(state)</span>:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    <span class="keyword">if</span> state <span class="keyword">in</span> west:</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>        <span class="keyword">return</span> <span class="string">'West'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    <span class="keyword">elif</span> state <span class="keyword">in</span> south_west:</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>        <span class="keyword">return</span> <span class="string">'SouthWest'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    <span class="keyword">elif</span> state <span class="keyword">in</span> south_east:</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>        <span class="keyword">return</span> <span class="string">'SouthEast'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    <span class="keyword">elif</span> state <span class="keyword">in</span> mid_west:</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>        <span class="keyword">return</span> <span class="string">'MidWest'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    <span class="keyword">elif</span> state <span class="keyword">in</span> north_east:</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>        <span class="keyword">return</span> <span class="string">'NorthEast'</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data[<span class="string">'region'</span>] = data[<span class="string">'addr_state'</span>].apply(finding_regions)</span><br></pre></td></tr></table></figure>
<p>美国有50个州，如果对州这个字段进行独热编码将会多出50个特征。这显然是不划算的，因此，我们把50个州根据美国的地理特点划分成东北，西北，东南，西南，西部五个区域，然后在进行独热编码。</p>
<h4 id="日期类特征issue-d，earliest-cr-line"><a href="#日期类特征issue-d，earliest-cr-line" class="headerlink" title="日期类特征issue_d，earliest_cr_line"></a>日期类特征issue_d，earliest_cr_line</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>data.issue_d = pd.to_datetime(data[<span class="string">'issue_d'</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data.earliest_cr_line = pd.to_datetime(data[<span class="string">'earliest_cr_line'</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data[<span class="string">'issue_d_year'</span>]=data.issue_d.dt.year</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data[<span class="string">'issue_d_month'</span>] = data.issue_d.dt.month</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data[<span class="string">'earliest_cr_line_year'</span>]=data.earliest_cr_line.dt.year</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data[<span class="string">'earliest_cr_line_month'</span>] = data.earliest_cr_line.dt.month</span><br></pre></td></tr></table></figure>
<p>通过上一篇的分析我们发现，贷款量跟年份是有关系的，因此我们把原本的年-月信息处理分开成两个特征，分别记录年月。这样我们的模型就能学到贷款状态与年份，和月份的信息。</p>
<h4 id="mths-since-last-major-derog"><a href="#mths-since-last-major-derog" class="headerlink" title="mths_since_last_major_derog"></a>mths_since_last_major_derog</h4><p>这个字段记录了上次违约之后过去的月数，有大量（约70%）的null值，我们最初把这个特征删除了，怎么训练精度都很难提高，这里要提出来注意一下。<br>这里都null值更多指的是压根没有违约，所以不应该赋值为0，那将表示刚刚违约了。我们应当将null赋一个较大的数值，或者如果使用基于决策树都模型，也可能直接留着null值不用处理。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;初赛数据预处理&quot;&gt;&lt;a href=&quot;#初赛数据预处理&quot; class=&quot;headerlink&quot; title=&quot;初赛数据预处理&quot;&gt;&lt;/a&gt;初赛数据预处理&lt;/h2&gt;&lt;h3 id=&quot;概览&quot;&gt;&lt;a href=&quot;#概览&quot; class=&quot;headerlink&quot; title=&quot;概
    
    </summary>
    
      <category term="技术" scheme="https://xhxt2008.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="机器学习" scheme="https://xhxt2008.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="数据分析" scheme="https://xhxt2008.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
      <category term="竞赛" scheme="https://xhxt2008.github.io/tags/%E7%AB%9E%E8%B5%9B/"/>
    
      <category term="平安产险" scheme="https://xhxt2008.github.io/tags/%E5%B9%B3%E5%AE%89%E4%BA%A7%E9%99%A9/"/>
    
  </entry>
  
  <entry>
    <title>平安产险2018极客挑战赛·初赛（一）</title>
    <link href="https://xhxt2008.github.io/2018/08/11/pinganA1/"/>
    <id>https://xhxt2008.github.io/2018/08/11/pinganA1/</id>
    <published>2018-08-11T01:31:53.000Z</published>
    <updated>2018-09-19T06:53:27.285Z</updated>
    
    <content type="html"><![CDATA[<p>今年四五月份参加的平安的机器学习挑战赛获了二等奖奖，本来想忙完这阵好好吧比赛的经过分享一下的，但是做完比赛之后很快又忙于毕业设计。在细节淡忘之前想要好好把做的东西整理记录一下。</p>
<h2 id="初赛赛题解析"><a href="#初赛赛题解析" class="headerlink" title="初赛赛题解析"></a>初赛赛题解析</h2><p><a href="http://pingancx.zhaopin.com/" target="_blank" rel="noopener">题目</a>非常简单，就是利用借款人的身份信息，借款记录，信用评级等信息，预测这个借款人是否会欺诈的而分类问题。<br>提供了test集，train集，特征的说明文件和提交样品sample。参赛队伍有近一个月的时间建立模型，训练模型，提交结果。一个队伍最多有3个人，最多提交五次，取最高分作为成绩。</p>
<h3 id="特征的理解"><a href="#特征的理解" class="headerlink" title="特征的理解"></a>特征的理解</h3><p>由于初赛时间充裕，我们可以逐个（64个）对特征进行分析，进而选择最好的处理方法。首先为了便于理解我们翻译了主办方提供的特征说明<a href="https://github.com/xhxt2008/LoanPrediction/blob/master/files/DataDictionary_cn.xlsx" target="_blank" rel="noopener">文档</a>。其中acc_now_deling是我们的Label，下面的都是提供的特征。有些特征的理解需要一些金融知识，我们这些外行只有强行看懂了。<br><img src="http://oonaavjvi.bkt.clouddn.com/pinanA001.png" alt="特征说明"></p>
<h3 id="特征的分析"><a href="#特征的分析" class="headerlink" title="特征的分析"></a>特征的分析</h3><p>光看懂文档当然是不够了，我们还调用Python的Pandas包对特征数据进行了初步的分析。</p>
<h4 id="Label"><a href="#Label" class="headerlink" title="Label"></a>Label</h4><p>首先对Y值进行分析，可以看出，正负类高度不平衡，达到了200:1的水平。这会对预测的结果造成深刻影响。我们后面会说一些我们解决这个问题的方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_train.value_counts()</span><br><span class="line"><span class="number">0</span>    <span class="number">706610</span></span><br><span class="line"><span class="number">1</span>      <span class="number">3293</span></span><br><span class="line">Name: acc_now_delinq, dtype: int64</span><br></pre></td></tr></table></figure>
<p><img src="http://oonaavjvi.bkt.clouddn.com/pinanA008.png" alt="Y值正负例比较"></p>
<h4 id="年份对贷款量对影响"><a href="#年份对贷款量对影响" class="headerlink" title="年份对贷款量对影响"></a>年份对贷款量对影响</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>plt.figure(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sns.barplot(<span class="string">'year'</span>, <span class="string">'loan_amnt'</span>, data=all_df,  palette=<span class="string">'tab10'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>plt.title(<span class="string">'Issuance of Loans'</span>, fontsize=<span class="number">16</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>plt.xlabel(<span class="string">'Year'</span>, fontsize=<span class="number">14</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>plt.ylabel(<span class="string">'Average loan amount issued'</span>, fontsize=<span class="number">14</span>)</span><br></pre></td></tr></table></figure>
<p><img src="http://oonaavjvi.bkt.clouddn.com/pinanA003.png" alt="年份对贷款量对影响"></p>
<p>贷款量随年份逐年增加，上面的短线事贷款量的方差。</p>
<h4 id="loan-status对Y值对影响"><a href="#loan-status对Y值对影响" class="headerlink" title="loan_status对Y值对影响"></a>loan_status对Y值对影响</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>td1=train_df[train_df[<span class="string">'acc_now_delinq'</span>] == <span class="number">1</span>].groupby([<span class="string">'loan_status'</span>, <span class="string">'acc_now_delinq'</span>]).size()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>td2=train_df[train_df[<span class="string">'acc_now_delinq'</span>] == <span class="number">0</span>].groupby([<span class="string">'loan_status'</span>, <span class="string">'acc_now_delinq'</span>]).size()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = td2.values</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = td1.values.astype(float)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>values= b/a</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>td3=pd.Series(values, index=index)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fig, ax1= plt.subplots(nrows=<span class="number">1</span>, ncols=<span class="number">1</span>, figsize=(<span class="number">14</span>,<span class="number">6</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cmap = plt.cm.coolwarm_r</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>td3.unstack().plot(kind=<span class="string">'bar'</span>, stacked=<span class="keyword">True</span>, colormap=cmap, ax=ax1, grid=<span class="keyword">False</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ax1.set_title(<span class="string">'Type of Loans by Grade'</span>, fontsize=<span class="number">14</span>)</span><br></pre></td></tr></table></figure>
<p><img src="http://oonaavjvi.bkt.clouddn.com/pinanA004.png" alt="loan_status对Y值对影响"></p>
<p>贷款状态对违约的影响，我们可以清楚看到处在LATE状态下的违约率明显偏高。Changed off字段下则完全没有违约问题。</p>
<h4 id="各个特征相对于Y值的协方差"><a href="#各个特征相对于Y值的协方差" class="headerlink" title="各个特征相对于Y值的协方差"></a>各个特征相对于Y值的协方差</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>data_corr.acc_now_delinq.sort_values(ascending=<span class="keyword">False</span>)</span><br><span class="line">acc_now_delinq                         <span class="number">1.000000</span></span><br><span class="line">sub_grade                              <span class="number">0.029486</span></span><br><span class="line">int_rate                               <span class="number">0.027148</span></span><br><span class="line">total_acc                              <span class="number">0.027063</span></span><br><span class="line">tot_cur_bal                            <span class="number">0.024064</span></span><br><span class="line">issue_d_year                           <span class="number">0.023436</span></span><br><span class="line">collections_12_mths_ex_med             <span class="number">0.021125</span></span><br><span class="line">home_ownership_MORTGAGE                <span class="number">0.015780</span></span><br><span class="line">annual_inc                             <span class="number">0.015264</span></span><br><span class="line">out_prncp                              <span class="number">0.013659</span></span><br><span class="line">out_prncp_inv                          <span class="number">0.013650</span></span><br><span class="line">total_rev_hi_lim                       <span class="number">0.009018</span></span><br><span class="line">emp_length                             <span class="number">0.008985</span></span><br><span class="line">verification_status_Source Verified    <span class="number">0.007455</span></span><br><span class="line">installment                            <span class="number">0.007122</span></span><br><span class="line">verification_status_Verified           <span class="number">0.006521</span></span><br><span class="line">region                                 <span class="number">0.005690</span></span><br><span class="line">term                                   <span class="number">0.005633</span></span><br><span class="line">purpose_home_improvement               <span class="number">0.005144</span></span><br><span class="line">funded_amnt_inv                        <span class="number">0.004894</span></span><br><span class="line">funded_amnt                            <span class="number">0.004691</span></span><br><span class="line">loan_amnt                              <span class="number">0.004580</span></span><br><span class="line">purpose_debt_consolidation             <span class="number">0.004071</span></span><br><span class="line">total_rec_late_fee                     <span class="number">0.003755</span></span><br><span class="line">dti                                    <span class="number">0.003273</span></span><br><span class="line">total_rec_int                          <span class="number">0.002244</span></span><br><span class="line">home_ownership_OWN                     <span class="number">0.002197</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>协方差衡量了每个特征与Y值的互相惯性，协方差的绝对值越小，特征越不重要。不过这种方法没有考虑到特征的组合，不能完全依照这个。</p>
<h4 id="每个特征值NULL的数量"><a href="#每个特征值NULL的数量" class="headerlink" title="每个特征值NULL的数量"></a>每个特征值NULL的数量</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>LoanStats_df.isnull().sum().sort_values(ascending=<span class="keyword">True</span>)</span><br><span class="line">idx                                               <span class="number">19</span></span><br><span class="line">hardship_flag                                     <span class="number">19</span></span><br><span class="line">zip_code                                          <span class="number">19</span></span><br><span class="line">addr_state                                        <span class="number">19</span></span><br><span class="line">application_type                                  <span class="number">19</span></span><br><span class="line">policy_code                                       <span class="number">19</span></span><br><span class="line">last_pymnt_amnt                                   <span class="number">19</span></span><br><span class="line">collection_recovery_fee                           <span class="number">19</span></span><br><span class="line">recoveries                                        <span class="number">19</span></span><br><span class="line">total_rec_late_fee                                <span class="number">19</span></span><br><span class="line">total_rec_int                                     <span class="number">19</span></span><br><span class="line">revol_bal                                         <span class="number">19</span></span><br><span class="line">total_rec_prncp                                   <span class="number">19</span></span><br><span class="line">total_pymnt_inv                                   <span class="number">19</span></span><br><span class="line">initial_list_status                               <span class="number">19</span></span><br><span class="line">out_prncp                                         <span class="number">19</span></span><br><span class="line">purpose                                           <span class="number">19</span></span><br><span class="line">out_prncp_inv                                     <span class="number">19</span></span><br><span class="line">total_pymnt                                       <span class="number">19</span></span><br><span class="line">pymnt_plan                                        <span class="number">19</span></span><br><span class="line">debt_settlement_flag                              <span class="number">19</span></span><br><span class="line">loan_amnt                                         <span class="number">19</span></span><br><span class="line">funded_amnt                                       <span class="number">19</span></span><br><span class="line">funded_amnt_inv                                   <span class="number">19</span></span><br><span class="line">term                                              <span class="number">19</span></span><br><span class="line">int_rate                                          <span class="number">19</span></span><br><span class="line">installment                                       <span class="number">19</span></span><br><span class="line">grade                                             <span class="number">19</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>原则是NULL数量超过80%弃用这个特征，只有少量NULL的会填补特征，视具体情况补充均值，最大最小值，或者其他值。特殊情况会删除整条数据。</p>
<h4 id="独热编码"><a href="#独热编码" class="headerlink" title="独热编码"></a>独热编码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>train_df.home_ownership.unique()</span><br><span class="line">array([<span class="string">'MORTGAGE'</span>, <span class="string">'RENT'</span>, <span class="string">'OWN'</span>, <span class="string">'OTHER'</span>, <span class="string">'NONE'</span>, <span class="string">'ANY'</span>], dtype=object)</span><br></pre></td></tr></table></figure>
<p>像这样的非数值类型变量要数值化，具体方法有二值化和独热编码。</p>
<p>经过对题目给的所有数据对分析之后，下一步就是对数据的预处理了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今年四五月份参加的平安的机器学习挑战赛获了二等奖奖，本来想忙完这阵好好吧比赛的经过分享一下的，但是做完比赛之后很快又忙于毕业设计。在细节淡忘之前想要好好把做的东西整理记录一下。&lt;/p&gt;
&lt;h2 id=&quot;初赛赛题解析&quot;&gt;&lt;a href=&quot;#初赛赛题解析&quot; class=&quot;hea
    
    </summary>
    
      <category term="技术" scheme="https://xhxt2008.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="机器学习" scheme="https://xhxt2008.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="数据分析" scheme="https://xhxt2008.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
      <category term="竞赛" scheme="https://xhxt2008.github.io/tags/%E7%AB%9E%E8%B5%9B/"/>
    
      <category term="平安产险" scheme="https://xhxt2008.github.io/tags/%E5%B9%B3%E5%AE%89%E4%BA%A7%E9%99%A9/"/>
    
  </entry>
  
  <entry>
    <title>通俗易懂，什么是VC维</title>
    <link href="https://xhxt2008.github.io/2018/05/11/VC-dimension/"/>
    <id>https://xhxt2008.github.io/2018/05/11/VC-dimension/</id>
    <published>2018-05-11T06:50:03.000Z</published>
    <updated>2018-08-31T08:25:53.274Z</updated>
    
    <content type="html"><![CDATA[<p>因为研究涉及到过拟合（overfitting），了解到VC维这个概念。VC维已经不是一个新鲜事物了，机器学习教材里面把他列为“进阶知识”。关于VC维介绍普遍比较晦涩难懂，我觉得也没必要用数学语言来定量了解这个概念，抽象的理解VC维的物理意义我觉得就够用了。毕竟据说VC维由于没办法很好的解释深度学习，已经是一个比较边缘化的知识了。</p>
<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><blockquote>
<p>VC维（外文名Vapnik-Chervonenkis Dimension）的概念是为了研究学习过程一致收敛的速度和推广性（Generalization performance），由统计学理论定义的有关函数集学习性能的一个重要指标。</p>
</blockquote>
<p>简单来说，VC维是用来衡量研究对象（数据集与学习模型）<strong>可学习性</strong>的指标。</p>
<h2 id="如何理解可学习性？"><a href="#如何理解可学习性？" class="headerlink" title="如何理解可学习性？"></a>如何理解可学习性？</h2><p>对于机器学习（数据驱动的学习），首先我们要知道他的要素：<strong>训练集</strong>，<strong>测试集</strong>和<strong>学习算法</strong>。</p>
<ol>
<li>对于训练集来说，训练集要足够大，才能使结果收敛。</li>
<li>对于测试集来说，测试集要有足够的代表性，不能偏差太大。</li>
<li>对于学习算法来说，要足够复杂，以表达特征（X值）与学习目标（y值）之间的逻辑关系。</li>
</ol>
<p><img src="http://oonaavjvi.bkt.clouddn.com/VCD01.png" alt="可学习的条件"></p>
<p>台大教授林轩田把他归纳成两点：</p>
<ol>
<li>我们能否使测试集的误差与训练集足够接近？</li>
<li>我们能否使训练集上的误差足够小？</li>
</ol>
<p>笔者认为，前者可能更加好懂，但不是那么严谨吧。在<a href="https://xhxt2008.github.io/2018/05/07/deep-learning/">上一篇博客</a>中，那个关于水环境的例子。例子中我们认为数据条数太少，结果无法收敛，也就是说不符合训练集上误差足够小这一条。</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/VCD07.jpg" alt="模型复杂度与拟合的关系"><br>另外模型过于复杂或者过于简单，也会分别导致不符合条件1，2的情况。这也就是所谓的过拟合与欠拟合，网上的介绍很多，具体不在这里展开了。</p>
<p>从这里我们可以尝试得出一个结论：模型的可学习型，只与数据量与模型复杂度有关。与具体的研究对象，输入数据（X）的分布都无关。</p>
<h2 id="回到VC维"><a href="#回到VC维" class="headerlink" title="回到VC维"></a>回到VC维</h2><p>从上面的推论，我们知道了可学习性与<strong>数据量</strong>和<strong>模型的复杂度</strong>有关。但只是泛泛但知道了，如何定量的计算和表达这种关系就需要引入VC维但概念了。网上关于VC维的介绍非常晦涩难懂，我们直接跳过公式先来看结论。详细请看：<a href="http://www.flickering.cn/machine_learning/2015/04/vc维的来龙去脉/" target="_blank" rel="noopener">VC维的来龙去脉</a></p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/vc_power2.png" alt="VC维"></p>
<p>在这张图里，纵轴是错误率（1-精度），横轴是VC维，out-of-sample指的是测试集错误率，越小越好。in-sample-error指的是训练集错误率。</p>
<p>我们可以看出：</p>
<ol>
<li>VC维跟模型复杂度是正相关的，以至于很多人误解VC维就是模型复杂度，当然这么理解好像也不会有什么不好的后果。</li>
<li>测试集的错误率一开始很高，随着VC维增大而减小，在${d{vc}}^{*}$ 达到最小值，然后开始增大。我们就称这个$d{vc}$处为这个模型的VC维。</li>
<li>训练集的错误率一直在减小。</li>
</ol>
<p>我们来看一下VC维的定义：</p>
<blockquote>
<p>一个假设空间H的VC dimension，是这个H最多能够shatter掉的点的数量，记为dvc(H)。</p>
</blockquote>
<ol>
<li>假设空间可以看作模型的复杂度。</li>
<li>shatter翻译成打散，指的是不管数据的分布如何，H都要把它区分开。</li>
<li><em>“这个H最多能够shatter掉的点的数量”</em>，这句话翻译成人话是，不管数据是怎样分布的，H最多能区分多少个数据。我们可以想像，越是复杂的H能够区分的数据点就越多，VC维也就越大。</li>
</ol>
<p>这个概念真是看的人头皮发麻，结合公式口感更佳，我们其实不妨就可以把VC维理解成模型复杂度没有关系。</p>
<p><a href="http://www.flickering.cn" target="_blank" rel="noopener">火光摇曳</a>提到：</p>
<blockquote>
<p>根据前面的推导，我们知道VC维的大小：与学习算法A无关，与输入变量X的分布也无关，与我们求解的目标函数f 无关。它只与<strong>模型</strong>和<strong>假设空间</strong>有关。</p>
</blockquote>
<p>我们不管他说了什么，他这句话应该跟我前面尝试总结出的：“模型的可学习性，只与数据量与模型复杂度有关。”是等价的。</p>
<h2 id="VC维理论的边缘化"><a href="#VC维理论的边缘化" class="headerlink" title="VC维理论的边缘化"></a>VC维理论的边缘化</h2><p>说了这么多，其实我还想告诉你，虽然这个理论读起来很晦涩，但是实际情况是它目前是一个被边缘化的知识。理由根据VC维理论，神经网络的VC维巨大，学习是不可行的。但是实际情况，神经网络却在一些领域表现良好。这又是什么原因呢？可能有以下的解释。</p>
<ul>
<li>数据量增多了</li>
<li>神经网络共享权值导致参数变少，导致VC维下降了</li>
<li>训练结束的时候，模型实际上对训练集还是欠拟合的</li>
</ul>
<p>Whatever，VC维在神经网络中看似是失效了。各位同学们也不要高兴得太早，我们总是需要有一个理论来定量的计算学习的可行性。而不是像现如今的大部分研究一样，只有一个实验结果，而对方法的改进拿不出靠谱的评价标准。</p>
<p>我在这里抛砖引玉，希望各位同学们能戒骄戒躁，在理论建设的大楼上添砖加瓦。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li><a href="http://www.flickering.cn/machine_learning/2015/04/vc%E7%BB%B4%E7%9A%84%E6%9D%A5%E9%BE%99%E5%8E%BB%E8%84%89/" target="_blank" rel="noopener">VC维的来龙去脉</a></li>
<li><a href="https://www.youtube.com/watch?v=XxPB9GlJEUk" target="_blank" rel="noopener">The VC Dimension :: Definition of VC Dimension @ Machine Learning Foundations (機器學習基石)
</a></li>
<li><a href="https://www.zhihu.com/question/38607822/answer/151561258" target="_blank" rel="noopener">如何通俗的理解机器学习中的VC维、shatter和break point？</a></li>
<li><a href="https://www.zhihu.com/question/52398145/answer/209358209/" target="_blank" rel="noopener">机器学习中的目标函数、损失函数、代价函数有什么区别？</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;因为研究涉及到过拟合（overfitting），了解到VC维这个概念。VC维已经不是一个新鲜事物了，机器学习教材里面把他列为“进阶知识”。关于VC维介绍普遍比较晦涩难懂，我觉得也没必要用数学语言来定量了解这个概念，抽象的理解VC维的物理意义我觉得就够用了。毕竟据说VC维由于
    
    </summary>
    
      <category term="研究" scheme="https://xhxt2008.github.io/categories/%E7%A0%94%E7%A9%B6/"/>
    
    
      <category term="机器学习" scheme="https://xhxt2008.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="VC维" scheme="https://xhxt2008.github.io/tags/VC%E7%BB%B4/"/>
    
  </entry>
  
  <entry>
    <title>上帝归上帝，恺撒归凯撒，传统方法与深度学习之辨。</title>
    <link href="https://xhxt2008.github.io/2018/05/07/deep-learning/"/>
    <id>https://xhxt2008.github.io/2018/05/07/deep-learning/</id>
    <published>2018-05-06T17:21:14.000Z</published>
    <updated>2018-08-31T08:25:53.275Z</updated>
    
    <content type="html"><![CDATA[<p><script type="text/javascript" language="javascript"><br>function iFrameHeight() {<br>var ifm= document.getElementById(“iframepage”);<br>var subWeb = document.frames ? document.frames[“iframepage”].document : ifm.contentDocument;<br>if(ifm != null &amp;&amp; subWeb != null) {<br>   ifm.height = subWeb.body.scrollHeight;<br>   ifm.width = subWeb.body.scrollWidth;<br>}<br>}<br></script><br>以<strong>数据作为驱动</strong>的机器学习，正在快速的在各个领域，替代传统的<strong>基于规则</strong>的方法。那么有没有某个领域目前还无法替代呢？<br>答案是肯定的。我们来看一个例子。</p>
<blockquote>
<p>我的一个朋友做的是环境方面的研究，具体对象是使用微生物处理工厂排放的污水。这就需要对污水中的生态环境进行建模。我们假设特征（X值）是水的温度，PH值，以及各种元素的含量等等，目标（Y值）是某种微生物的活性。<br><a id="more"></a><br><img src="http://oonaavjvi.bkt.clouddn.com/CVD02.png" alt="数据驱动的方法"></p>
</blockquote>
<p>乍一看，这个例子完全可以用最时髦的机器学习的方法来做，比如说神经网络什么的。然而现状是：</p>
<ul>
<li>对每一份污水对水样我们都要用十分昂贵对器材进行有限的分析，大概一次最多只能获得300条数据。</li>
<li>水中的微量元素有上千种，也就是变量，或者说输入的维度，或者特征，有上千个。</li>
</ul>
<p>我们直观的感受，数据太少了，机器学习模型根本无法收敛。那这个问题通常是怎么处理的呢？</p>
<h2 id="传统方法是怎么做的？"><a href="#传统方法是怎么做的？" class="headerlink" title="传统方法是怎么做的？"></a>传统方法是怎么做的？</h2><p>在各种机器学习论文中，被爆的体无完肤的传统方法到底是什么方法？可能现在都没有人感兴趣了。其实现在的“机器学习”这个概念里，除了深度学习，基本上都是和传统的最优化，运筹学相通的。比方说：决策树，模拟退火，蚁群算法，GA，EDA，PSO之类的。<br>传统最优化领域里面，有一个万金油叫做“动态规划（Dynamic programming）”，一些比较保守的老教授非常喜欢这个东西。我们看一下百度对他的定义。</p>
<blockquote>
<p>动态规划(dynamic programming)是运筹学的一个分支，是求解决策过程(decision process)最优化的数学方法。20世纪50年代初美国数学家R.E.Bellman等人在研究多阶段决策过程(multistep decision process)的优化问题时，提出了著名的最优化原理(principle of optimality)，把多阶段过程转化为一系列单阶段问题，利用各阶段之间的关系，逐个求解，创立了解决这类过程优化问题的新方法——动态规划。</p>
</blockquote>
<p><img src="http://oonaavjvi.bkt.clouddn.com/CVD03.jpg" alt="动态规划"></p>
<p>动态规划就是典型的基于规则（Rule base）的，回到我们的例子中，污水里每一种元素是一个方块，连线可以表示他们之间的关系，有一个公式可以表示这种关系。比方说H+离子和PH值就是相关的。阶段可以表示谁和谁会先发生反应，谁和谁后发生反应。最后对于我们的研究对象，可以表示为一个目标函数（Target function），我们一般是要求他的最大值或者最小值。<br>朋友告诉我，即使温度或者PH值发生一丁点改变，反应的方程式或者顺序都会有很大变化。在这个例子中，这种传统的方法某种程度上更加接近科学的本质，因为这种生物化学的反应，在大自然中本来就是客观的，我们的研究只是去发现它。而用机器学习来做，把数据直接扔给黑箱更像是在偷鸡。</p>
<h2 id="反思一下"><a href="#反思一下" class="headerlink" title="反思一下"></a>反思一下</h2><p>我举这个例子并不是想说机器学习不好，只是想说它并不是万金油。那些在各种论文中被爆的体无完肤的“传统方法”之所以存在是有他的必要性的。只是因为研究对象不同，我们就要应用相对应的方法。<br>那机器学习，尤其是深度学习的优势在哪里呢？这几年深度学习之所以火了，很大的原因归功于它在<strong>计算机视觉</strong>和<strong>自然语言处理</strong>领域的应用。他们对应的传统方法是什么呢？</p>
<ul>
<li>在计算机视觉领域，表情识别和人脸识别曾经都是使用基于规则的方法的，他们通过计算五官的大小，和他们之间的距离等等，来对人脸或者表情进行分类。京都大学情报学院在这方面做得非常优秀，他们使用一些力学模型对时系列的表情进行识别。我之前做的paper survey涉及到这方面，感兴趣可以读一下。<a href="https://xhxt2008.github.io/2017/11/22/paper-survey-kyoto/">Paper survey about Timing-Based Facial Expression Recognition of Kyoto University by Prof. Kawashima</a></li>
<li>在自然语言处理领域，词语预测，句子纠错，机器翻译等等曾经用的都是基于规则的方法。使用的方法包括形式语言自动机理论，用例翻译等等。和我们大学学过的一门叫做《编译原理》的课程有点类似，都是基于某种规则（语法）的。</li>
</ul>
<p>在深度学习应用之前，传统方法在这两个领域的表现都不佳。我们人类的大脑可以快速的区分两张人脸，即使他们相差得很小。一张照片有若干个像素点构成，如何通过计算这些像素点来让计算机区分两张脸呢？我们相信规则是有的，但是很大程度上无法做到，因为过于复杂。<br>人脸尚且有五官之间距离等可以测算，那对象识别呢？比方说识别一张图片是猫还是狗。找到这种规则来区分几乎是不可想象的。</p>
<p>在自然语言处理领域，自然语言天生就不是形式语言（如编程语言），虽有语法，但是例外却很多。再加上现在的网络用语，不断有新的，奇怪的东西加入。基于用例的翻译用武之地并不太多。</p>
<h2 id="那我们人脑又是如何实现的呢？"><a href="#那我们人脑又是如何实现的呢？" class="headerlink" title="那我们人脑又是如何实现的呢？"></a>那我们人脑又是如何实现的呢？</h2><p>推荐莫烦的视频说得非常通俗易懂<a href="https://www.bilibili.com/video/av15997699" target="_blank" rel="noopener">科普: 人工神经网络 VS 生物神经网络
</a>。</p>
<iframe src="//player.bilibili.com/player.html?aid=16938887&cid=27691025&page=1" id="iframepage" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" onload="iFrameHeight()"> </iframe>

<p>简单说，生物神经网络是通过神经元细胞突触的链接完成电信号的传递，进行语言理解图像识别甚至思维的。神经元之间的连接可以通过后天的训练用进废退。人工神经网络充其量只能算是对生物神经网络一种粗糙的模仿。</p>
<p>有人说人工神经网络是一种基于传统数理模型的东西。实际上它也确实是基于数理模型的。不过我想换个角度解读它，<strong>人工神经网络是仿生的</strong>。它之所以有效，是因为它在一定程度上模仿了生物的神经系统，因此，他才会在像视觉，语言理解等方面接近或者达到人类的水平。</p>
<p>所以我们大胆假设，人工神经网络在一些人类本身并不擅长，或者存在固定的客观规则，或者规则并不是太复杂的情况下。将会是不合适或者不经济的。</p>
<p>说了这么多，作为结论我想说。上帝的归上帝，恺撒的归凯撒，没有不好的学习算法，只有不合适的学习算法。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li><a href="https://www.bilibili.com/video/av15997699" target="_blank" rel="noopener">科普: 人工神经网络 VS 生物神经网络</a></li>
<li><a href="http://vision.kuee.kyoto-u.ac.jp/~hiroaki/index-j.html" target="_blank" rel="noopener">京都大学大学院情報学研究科 知能情報学専攻</a></li>
<li><a href="https://www.kitakyu-u.ac.jp/subject/graduate/env/" target="_blank" rel="noopener">北九州市立大学国際環境工学研究科</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;script type=&quot;text/javascript&quot; language=&quot;javascript&quot;&gt;&lt;br&gt;function iFrameHeight() {&lt;br&gt;var ifm= document.getElementById(“iframepage”);&lt;br&gt;var subWeb = document.frames ? document.frames[“iframepage”].document : ifm.contentDocument;&lt;br&gt;if(ifm != null &amp;amp;&amp;amp; subWeb != null) {&lt;br&gt;   ifm.height = subWeb.body.scrollHeight;&lt;br&gt;   ifm.width = subWeb.body.scrollWidth;&lt;br&gt;}&lt;br&gt;}&lt;br&gt;&lt;/script&gt;&lt;br&gt;以&lt;strong&gt;数据作为驱动&lt;/strong&gt;的机器学习，正在快速的在各个领域，替代传统的&lt;strong&gt;基于规则&lt;/strong&gt;的方法。那么有没有某个领域目前还无法替代呢？&lt;br&gt;答案是肯定的。我们来看一个例子。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;我的一个朋友做的是环境方面的研究，具体对象是使用微生物处理工厂排放的污水。这就需要对污水中的生态环境进行建模。我们假设特征（X值）是水的温度，PH值，以及各种元素的含量等等，目标（Y值）是某种微生物的活性。&lt;br&gt;
    
    </summary>
    
      <category term="研究" scheme="https://xhxt2008.github.io/categories/%E7%A0%94%E7%A9%B6/"/>
    
    
      <category term="机器学习" scheme="https://xhxt2008.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://xhxt2008.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://xhxt2008.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="动态规划" scheme="https://xhxt2008.github.io/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>Combining multiple feature selection methods for stock prediction Union, intersection, and multi-intersection approaches</title>
    <link href="https://xhxt2008.github.io/2017/12/21/Combining-feature-selection/"/>
    <id>https://xhxt2008.github.io/2017/12/21/Combining-feature-selection/</id>
    <published>2017-12-21T11:07:05.000Z</published>
    <updated>2018-05-03T20:40:08.366Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>To effectively predict stock price for investors is a very important research problem. In literature, data mining techniques have been applied to stock (market) prediction. Feature selection, a pre-processing step of data mining, aims at filtering out unrepresentative variables from a given dataset for effective prediction. As using different feature selection methods will lead to different features selected and thus affect the prediction performance, the purpose of this paper is to combine multiple feature selection methods to identify more representative variables for better prediction. </p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Stock investments are a very popular investment activity around the world. In literature, some basic important factors, such as financial ratios, technical indexes, and macroeconomic indexes have been proved as the important factors of affecting stocks’ rise and fall. However, different studies select their factors (i.e. input variables) differently for their prediction models [3]. That is, the opinion of the important factors for stock prediction is somewhat different in related work since there is no exact answer to the question of what are the most representative variables. On the other hand, it is the fact that using different input variables can make the same prediction model performs differently. Therefore, constructing the optimal stock prediction model for investors is very challenging.<br><strong>Feature selection</strong> can be used to filter out redundant and/or irrelevant features from a chosen dataset resulting in more representative features for better prediction performances.<br>That is, the chosen feature selection method is supposed to select usable features for stock prediction. However, using different feature selection methods is likely to produce different results Therefore, if we could apply a number of different feature selection methods and then combine the selection results, we can not only understand the most important and representative variables that all the feature selection methods ‘agree’, but also further improve prediction performances over using one single feature selection methods. </p>
<h2 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h2><p><img src="http://oonaavjvi.bkt.clouddn.com/FS01.png" alt="Related work"></p>
<h2 id="Literature-review"><a href="#Literature-review" class="headerlink" title="Literature review"></a>Literature review</h2><h3 id="Stock-price-theory"><a href="#Stock-price-theory" class="headerlink" title="Stock price theory"></a>Stock price theory</h3><ul>
<li>Stock prices mean the actual transaction price through the buyers and sellers in the market. Stock prices are determined by the laws of supply and demand. In theory, whether the price of a stock is high or low, it is decided by the buyers and sellers’ transactions in the open market. When supply and demand change, the stock price must be changed. </li>
<li>Fama‘s Efficient Market Hypothesis supposes that the investment activity is a “Fair-Game Market”. It means all information has disclosed in the stock market, and reflects on stock prices. According to the difference of disclosed information, there are three kinds of Efficient Market Hypothesis：<ul>
<li>The Weak Form Efficient Market </li>
<li>The Semi-strong Form Efficient Market</li>
<li>The Strong Form Efficient Market </li>
</ul>
</li>
</ul>
<h3 id="Stock-price-analysis-methods"><a href="#Stock-price-analysis-methods" class="headerlink" title="Stock price analysis methods"></a>Stock price analysis methods</h3><ul>
<li><strong>Fundamental analysis</strong>: Fundamental analysis believes that every stock has its intrinsic value. If the share prices lower than the intrinsic value, it means the stock is undervalued. In addition, economic factors also belong to this category. </li>
<li><strong>Technical analysis</strong>. Technical analysis, also known as “charting”, has been a part of financial practice for many decades. It studies the historical price and volume movement of a stock by using charts as the primary tool to forecast future price movements. This theory believes that the trends and patterns of an investment instrument’s price, volume, breadth, and the trading activities reflect most of the relevant market information that a decision maker can utilize to determine its value. </li>
</ul>
<h3 id="Feature-selection"><a href="#Feature-selection" class="headerlink" title="Feature selection"></a>Feature selection</h3><p>In many research problems, such as pattern recognition, it is important to choose a group of set of attributions with more prediction information. That is, if the number of irrelevant or redundant features is reduced drastically, the running time of a learning algorithm is also reduced. Moreover, a more general concept can be yielded. Performing feature selection can lead to many potential benefits, which are facilitating data visualization and data understanding, reducing the measurement and storage requirements, reducing training and utilization times, defying the curse of dimensionality to improve prediction performances, etc. </p>
<h3 id="Principal-Component-Analysis-PCA"><a href="#Principal-Component-Analysis-PCA" class="headerlink" title="Principal Component Analysis (PCA)"></a>Principal Component Analysis (PCA)</h3><p>Principal Component Analysis (PCA) is a multivariate statistical technique. It aims at reducing the dimensionality of a dataset with a large number of interrelated variables. In particular, it extracts a small set of factors or components that are constituted of highly correlated elements, while retaining their original characters. After performing PCA, the uncorrelated variables which are called components, will replace the original variables.<br><img src="http://oonaavjvi.bkt.clouddn.com/FS02.png" alt="Principal Component Analysis"></p>
<h3 id="GA-SVM"><a href="#GA-SVM" class="headerlink" title="GA-SVM"></a>GA-SVM</h3><p>The main idea of Genetic Algorithms (GA) is from Darwin’s theory of evolution from natural selection in the survival of the fittest. GA attempts to computationally mimic the processes by which natural selection operates.<br><img src="http://oonaavjvi.bkt.clouddn.com/FS03.jpg" alt="GA-SVM"></p>
<h3 id="Classification-and-Regression-Trees-CART"><a href="#Classification-and-Regression-Trees-CART" class="headerlink" title="Classification and Regression Trees (CART)"></a>Classification and Regression Trees (CART)</h3><p>The Classification and Regression Trees (CART) is a statistical technique that can select from a large number of explanatory variables those that are most important in determining the response variable to be explained. The decision trees produced by CART are strictly binary, containing exactly two branches for each decision tree. The root node t is separated into two samples based on some condition. The samples that fit the condition will be separated into the left nodes (tl), and the others will be separated into the right nodes (tr).<br>In particular, a decision tree is based on the entropy theory that the attribute (or feature) with the highest information gain (or greatest entropy reduction) is chosen as the test attribute for the non-leaf node<br>$$g\left ( Y,A \right )=H\left ( Y \right )-H\left ( Y|A \right )$$</p>
<p>Related work only applies one chosen feature selection method to filter out irrelevant variables. This motivates us to collect all relevant variables used for stock prediction in literature and then combining multiple feature selection methods to identify more representative variables for improving prediction performances. </p>
<h2 id="Experimental-design"><a href="#Experimental-design" class="headerlink" title="Experimental design"></a>Experimental design</h2><h3 id="The-first-experimental-stage"><a href="#The-first-experimental-stage" class="headerlink" title="The first experimental stage"></a>The first experimental stage</h3><p><img src="http://oonaavjvi.bkt.clouddn.com/FS04.jpg" alt="The first experimental stage"></p>
<h3 id="The-second-experimental-stage"><a href="#The-second-experimental-stage" class="headerlink" title="The second experimental stage"></a>The second experimental stage</h3><p><img src="http://oonaavjvi.bkt.clouddn.com/FS05.jpg" alt="The second experimental stage"></p>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><h3 id="Single-feature-selection-methods"><a href="#Single-feature-selection-methods" class="headerlink" title="Single feature selection methods"></a>Single feature selection methods</h3><p><img src="http://oonaavjvi.bkt.clouddn.com/FS06.png" alt="Single method"><br><img src="http://oonaavjvi.bkt.clouddn.com/FS07.png" alt="Single method"><br>Figures show the rate of prediction accuracy of the four different MLP models based on the one quarter and other-quarter based testing datasets respectively. As we can see, the results are slightly different if different testing datasets are considered.<br>We can find the models with feature selection are obviously better. And They have similar effects</p>
<h3 id="Multiple-feature-selection-methods"><a href="#Multiple-feature-selection-methods" class="headerlink" title="Multiple feature selection methods"></a>Multiple feature selection methods</h3><p><img src="http://oonaavjvi.bkt.clouddn.com/FS08.png" alt="Multiple method"><br><img src="http://oonaavjvi.bkt.clouddn.com/FS09.png" alt="Multiple method"></p>
<p>The results indicate that the intersection between PCA and GA outperforms the other combination approaches over the one quarter based testing dataset. On the other hand, combining multiple feature selection methods by the multi-intersection approach performs the best based on the other-quarter based testing dataset. However, the rates of prediction accuracy by both combination approaches over the two testing datasets do not have a big difference, i.e. less than 0.3%. </p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul>
<li>This paper compares three different feature selection methods, i.e. Principal Component Analysis (PCA), Genetic Algorithms (GA), and decision trees (CART) and combines them based on union, intersection, and multi- intersection approaches to examine their prediction accuracy and errors. </li>
<li>The experimental results show that combining multiple feature selection methods can provide better prediction performances than using single feature selection methods. In particular, the intersection between PCA and GA and the multi-intersection of PCA, GA, and CART perform the best, which provide the highest rate of prediction accuracy and the lowest error rate of predicting stocks’ rise. </li>
<li>Moreover, these two combined approaches select 14 and 17 important variables respectively from the 85 original variables, which filter out many unrepresentative variables. These variables can be used for practical investment decisions.</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li>Qing-Guo Wang, “Linear, Adaptive and Nonlinear Trading Models for Singapore Stock Market with Random Forests”, 2012</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;p&gt;To effectively predict stock price for investo
    
    </summary>
    
      <category term="研究" scheme="https://xhxt2008.github.io/categories/%E7%A0%94%E7%A9%B6/"/>
    
    
      <category term="特征选择" scheme="https://xhxt2008.github.io/tags/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"/>
    
      <category term="特征工程" scheme="https://xhxt2008.github.io/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
      <category term="未翻译" scheme="https://xhxt2008.github.io/tags/%E6%9C%AA%E7%BF%BB%E8%AF%91/"/>
    
      <category term="英文" scheme="https://xhxt2008.github.io/tags/%E8%8B%B1%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>Paper Survey about Do They All Look the Same Deciphering Chinese, Japanese and Koreans by Fine-Grained Deep Learning.</title>
    <link href="https://xhxt2008.github.io/2017/11/30/look-same/"/>
    <id>https://xhxt2008.github.io/2017/11/30/look-same/</id>
    <published>2017-11-30T07:07:45.000Z</published>
    <updated>2018-05-03T20:51:34.272Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>This study is about what extend Chinese, Japanese and Korean faces can be classified and which facial attributes offer the most important cues. First, we propose a novel way of obtaining large numbers of facial images with nationality labels. Then we train state-of-the-art neural networks with these labeled images. We are able to achieve an accuracy of 75.03% in the classification task, with chances being 33.33% and human accuracy 38.89% . Further, we train multiple facial attribute classifiers to identify the most distinctive features for each group. </p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>China, Japan and Korea are three of the world’s largest economies  with a large population. And they look very similar. </li>
<li><p>Some suggest that the main differences derive from mannerism and fashion. There also exist quite a few websites that put up Asian face classification chall-enges, which prove the difficulty of distinguishing them.</p>
</li>
<li><p>Randomly shuffled<br><img src="http://oonaavjvi.bkt.clouddn.com/look01.jpg" alt="image"></p>
</li>
<li>Grouped<br><img src="http://oonaavjvi.bkt.clouddn.com/look02.jpg" alt="image"></li>
</ul>
<h2 id="Related-Literature"><a href="#Related-Literature" class="headerlink" title="Related Literature"></a>Related Literature</h2><ul>
<li>(Farfade, Saberian, and Li 2015; Levi and Hassner 2015; Fu, He, and Hou 2014; Wang, Li, and Luo 2016) have both achieved very high accuracy in face detection, gender and race classification.</li>
<li>(Liu et al. 2015), which has achieved state-of-the-art perfor-mance, trains two networks, the first one for face localization and the second for attribute classification. Our work follows (Liu et al. 2015), and uses the same dataset as theirs, except that (1) for simplicity we use OpenCV to locate faces and (2) for ac- curacy we train a separate neural network for each attribute. </li>
</ul>
<h2 id="Data-Collection-and-Pre-processing"><a href="#Data-Collection-and-Pre-processing" class="headerlink" title="Data Collection and Pre-processing"></a>Data Collection and Pre-processing</h2><ul>
<li>We have two main data sources: Twitter and the CelebA dataset. We derive from Twitter the labeled Chinese, Japanese and Korean images, which are later used as input to the Resnet. We use CelebA to train the facial attribute classifiers. These classifiers are then used to classify the labeled Twitter images. </li>
<li>Twitter Images<br><img src="http://oonaavjvi.bkt.clouddn.com/look03.jpg" alt="image"></li>
<li>CelebA Images,<br>The CelebA dataset contains 202,599 images taken from ten thousand individuals. In this work, we follow Liu.’s practice in dividing the training, developing, and testing dataset. We use OpenCV to locate faces in each subset and eventually have 148,829 images for training, 18,255 images for development, and 18,374 images for testing.</li>
</ul>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="We-conduct-two-experiments"><a href="#We-conduct-two-experiments" class="headerlink" title="We conduct two experiments."></a>We conduct two experiments.</h3><ul>
<li>In the first experiment, we use the labeled Twitter images to fine-tune the Resnet and investigate to what extent Chinese, Japanese and Koreans can be classified. </li>
<li>In our second experiment, we train 40 facial attribute classifiers and examine which attributes contain the most important cues in distinguishing the three groups. </li>
</ul>
<h3 id="Face-Classification"><a href="#Face-Classification" class="headerlink" title="Face Classification"></a>Face Classification</h3><ul>
<li>We split our dataset into development set, validation set, and test set ( 8: 1: 1) and experiment with different architectures from shallow networks (3-5 layers) to the 16-layer VGG and 50-layer ResNet. In our experiments, all networks would converge (Figure 4), but we observe that accuracy of 75.03% with Resnet shows the best result.<br><img src="http://oonaavjvi.bkt.clouddn.com/look04.jpg" alt="image"></li>
<li>In Table 2, we report the confusion matrix for the testing images. Note that all the three peoples look equally “confusing” to the computer: the off-diagonal elements are roughly equal. The result we achieve answers in a definitive manner that Chinese, Japanese and Koreans are distinguishable. But it also suggests that this is a challenging task, which leads to our experiment on facial attribute classificatio<br><img src="http://oonaavjvi.bkt.clouddn.com/look05.jpg" alt="image"></li>
</ul>
<h3 id="Attribute-Classification"><a href="#Attribute-Classification" class="headerlink" title="Attribute Classification"></a>Attribute Classification</h3><p>We construct a separate neural network for each of the 40 attributes in the CelebA dataset. The neural nets all share the same structure.<br><img src="http://oonaavjvi.bkt.clouddn.com/look06.jpg" alt="image"></p>
<h3 id="Female"><a href="#Female" class="headerlink" title="Female"></a>Female</h3><p><img src="http://oonaavjvi.bkt.clouddn.com/look07.jpg" alt="image"></p>
<h3 id="Male"><a href="#Male" class="headerlink" title="Male"></a>Male</h3><p><img src="http://oonaavjvi.bkt.clouddn.com/look08.jpg" alt="image"></p>
<p>In Figures, we report the percentage of individuals that possess the corresponding facial attributes.</p>
<ol>
<li>Bangs are most popular among Japanese and least popular among Chinese. </li>
<li>Japanese smile the most and Chinese the least. </li>
<li>Japanese have the most eyebags, followed by Koreans. </li>
<li>Chinese are the most likely to have bushy eyebrows. </li>
<li>Koreans are the mostly likely to have black hair and Japanese are the least likely. </li>
</ol>
<h2 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h2><p>While our paper focuses on country comparisons, we also briefly summarize some of the significant findings on cross- country gender differentials that are either cultural or social in nature. </p>
<ul>
<li>females tend to smile more than males </li>
<li>men are twice more likely to be wearing glasses than women </li>
</ul>
<h2 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h2><p>Our work is built on the assumption that Twitter users, celebrity followers in particular, are representative of the demographics of the entire population. This assumption may not exactly hold as various demographic dimensions such as gender and age are skewed in Twitter (Mislove et al. 2011). Nonetheless, we believe the direction of our estimates will remain consistent, as several of our findings are confirmed by social stereotypes and research on other regions. Also, this concern could be alleviated to some extent by examining several other celebrities.</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul>
<li>In this paper, we have demonstrated that Chinese, Japanese and Koreans do look different. By assembling a large data set of labeled images and experimenting with different neural network architectures, we have achieved a remarkable accuracy 75.03%, almost twice as high as the human average accuracy. </li>
<li>We have also examined 40 facial attributes of the three populations in an effort to identify the important cues that assist classification. Our study has shown that Chinese, Japanese and Koreans do differ in several dimensions but overall are very similar. </li>
<li>Our work, which complements existing APIs such as Microsoft Cognitive Services and Face++, could find wide applications in tourism, e-commerce, social media marketing, criminal justice and even counter-terrorism.</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;p&gt;This study is about what extend Chinese, Japan
    
    </summary>
    
      <category term="研究" scheme="https://xhxt2008.github.io/categories/%E7%A0%94%E7%A9%B6/"/>
    
    
      <category term="未翻译" scheme="https://xhxt2008.github.io/tags/%E6%9C%AA%E7%BF%BB%E8%AF%91/"/>
    
      <category term="英文" scheme="https://xhxt2008.github.io/tags/%E8%8B%B1%E6%96%87/"/>
    
      <category term="人脸表情识别" scheme="https://xhxt2008.github.io/tags/%E4%BA%BA%E8%84%B8%E8%A1%A8%E6%83%85%E8%AF%86%E5%88%AB/"/>
    
      <category term="人脸识别" scheme="https://xhxt2008.github.io/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"/>
    
      <category term="表情识别" scheme="https://xhxt2008.github.io/tags/%E8%A1%A8%E6%83%85%E8%AF%86%E5%88%AB/"/>
    
      <category term="神经网络" scheme="https://xhxt2008.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Paper Survey about Mining Opinion Leaders in Big Social Network</title>
    <link href="https://xhxt2008.github.io/2017/11/30/OLM/"/>
    <id>https://xhxt2008.github.io/2017/11/30/OLM/</id>
    <published>2017-11-30T06:14:21.000Z</published>
    <updated>2017-11-30T06:13:05.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p> By identifying the opinion leaders, companies or governments can manipulate the selling or guiding public opinion, respectively. Additionally, detecting the influential comments is able to understand the source and trend of public opinion formation. However, mining opinion leaders in a huge social network is a challenge task because of the complexity of graph processing and leadership analysis. In this study, a novel algorithm, OLMiner, is proposed to efficiently find the opinion leaders from a huge social network.<br><a id="more"></a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="Opinion-leaders"><a href="#Opinion-leaders" class="headerlink" title="Opinion leaders"></a>Opinion leaders</h3><p>In a social network, the opinion leader means the influential person who may be an expert in a specific domain and have lots of people following his/her comments or ideas. opinion leaders are usually the information generator and message senders who familiar with the media by a secondary transmission. </p>
<h3 id="Past-research-utilize-the-characteristic-analysis-to-find-opinion-leaders"><a href="#Past-research-utilize-the-characteristic-analysis-to-find-opinion-leaders" class="headerlink" title="Past research: utilize the characteristic analysis to find opinion leaders."></a>Past research: utilize the characteristic analysis to find opinion leaders.</h3><ul>
<li>the number of posted articles</li>
<li>the number of articles replied by others</li>
<li>the number of followers</li>
</ul>
<h3 id="Limitation"><a href="#Limitation" class="headerlink" title="Limitation"></a>Limitation</h3><ul>
<li>Influence overlapping problem: The discovered opinion leaders may have many common followers, hence, the influence only can affect a small set of people. </li>
<li>Time-consuming: The discovered opinion leaders may have many common followers  the analysis of network structure is time-consuming, especially for large social graphs.</li>
</ul>
<h3 id="Opinion-Leader-Miner-OLMiner"><a href="#Opinion-Leader-Miner-OLMiner" class="headerlink" title="Opinion Leader Miner(OLMiner)"></a>Opinion Leader Miner(OLMiner)</h3><ol>
<li>OLMiner utilizes the post-and-follow relationships among individuals to quickly construct a social network. The time of user writing a post is also considered.</li>
<li>To tackle the influence overlapping problem, OLMiner proposes a clustering algorithm to effectively and efficiently discover the community structure of social network.</li>
<li>OLMiner utilizes K-mean clustering algorithm to exam the leadership quality Of each node and effective filtering out unpromising nodes.</li>
<li>OLMiner checks the size of input data to decide the execution environment, running on single computer or cloud environment.</li>
</ol>
<h2 id="Related-Works"><a href="#Related-Works" class="headerlink" title="Related Works"></a>Related Works</h2><h3 id="Opinion-Analysis-And-Mining"><a href="#Opinion-Analysis-And-Mining" class="headerlink" title="Opinion Analysis And Mining"></a>Opinion Analysis And Mining</h3><ul>
<li>Opinion Observer system: An opinion Observer system could analyze and compare the consumer opinions among competing products. Opinion Observer also uses language pattern mining method to extract product features from the Pros and Cons in a specific type of reviews. CopeOpi is an opinion mining system.</li>
<li>Thesauruses: When mining opinion or analyzing sentiment, many thesauruses are generally utilized for measuring the orientation or the property of comment, such as WordNet and General Inquire for English lexicon, HowNet and NTUSD for Chinese lexicon. </li>
</ul>
<h3 id="Opinion-Leader-Mining"><a href="#Opinion-Leader-Mining" class="headerlink" title="Opinion Leader Mining"></a>Opinion Leader Mining</h3><ul>
<li>Opinion leaders are domain-sensitive</li>
<li>There are many papers modified PageRank algorithm to find opinion leaders, such as OpinionRank, LeaderRank and Dynamic OpinionRank. </li>
</ul>
<h2 id="Proposed-Algorithm-OLMiner"><a href="#Proposed-Algorithm-OLMiner" class="headerlink" title="Proposed Algorithm: OLMiner"></a>Proposed Algorithm: OLMiner</h2><p><img src="http://oonaavjvi.bkt.clouddn.com/OLM1.png" alt="image"></p>
<ol>
<li>We first check the size of input data to decide running on a single computer or cloud environment. </li>
<li>Then, OLMiner utilizes the post-and-follow relationships among individuals to quickly construct a social network. When calculating the similarity (weight) of edge, we also consider the average time of posted article to tune the similarities among individuals. </li>
<li>Then, for detecting opinion leaders, we propose an efficient algorithm to find out community structure and extract qualified community. </li>
<li>Based on the discovered communities, we use clustering method to significantly shrink the candidate size of opinion leaders and avoid the influence overlapping problem. </li>
<li>Finally, OLMiner selects the k best-quality users from the candidate set according to the score of measuring function. </li>
</ol>
<h3 id="Algorithm-1-OLMiner-A-U-k"><a href="#Algorithm-1-OLMiner-A-U-k" class="headerlink" title="Algorithm 1: OLMiner (A, U, k)"></a>Algorithm 1: OLMiner (A, U, k)</h3><blockquote>
<p>Input: A: set of all articles published in a social website,<br>U: set of all users in a social website,<br>k: desired number of opinion leaders<br>Output: OL: a set of opinion leaders</p>
<ol>
<li>OL  ; G  ; Cs  ; Candi_set;</li>
<li>Granularity_Checking (G);</li>
<li>G  Network_Construction (U, A);</li>
<li>Cs Community_Detection (G, k);</li>
<li>Candi_set  Candidate_Generation (Cs, k);</li>
<li>OL  Leader_Selection (Candi_set, k);</li>
<li>output OL;</li>
</ol>
</blockquote>
<h3 id="1-granularity-checking"><a href="#1-granularity-checking" class="headerlink" title="1. granularity checking"></a>1. granularity checking</h3><p>check the size of input data to decide running on a single computer or cloud environment. </p>
<h3 id="2-Social-Network-Construction"><a href="#2-Social-Network-Construction" class="headerlink" title="2. Social Network Construction"></a>2. Social Network Construction</h3><ul>
<li>We segment 24 hours into four sections:<br><img src="http://oonaavjvi.bkt.clouddn.com/OLM02.png" alt="image"></li>
</ul>
<ul>
<li>Given a set of articles A and a set of users U, to build a social network <strong>G(V, E, W)</strong>.</li>
<li>If users u, v∈U have reply-article-relation in A, an edge (u, v) exists in E.</li>
<li>Set of adjacent nodes of a node u is defined as <strong>adj(u)</strong>.</li>
<li>The similarity <strong>sim(u, v)</strong> is defined as the common adjacent users of u and v,<br><img src="http://oonaavjvi.bkt.clouddn.com/OLM03.png" alt="image"></li>
<li>The weight <strong>w(u, v)</strong> of edge (u, v) in E.<br><img src="http://oonaavjvi.bkt.clouddn.com/OLM04.png" alt="image"></li>
</ul>
<h3 id="3-Community-Structure-Detection"><a href="#3-Community-Structure-Detection" class="headerlink" title="3. Community Structure Detection"></a>3. Community Structure Detection</h3><p><img src="http://oonaavjvi.bkt.clouddn.com/OLM05.jpg" alt="image"></p>
<ol>
<li>Firstly, capture the community structure of social network to reduce the execution time for finding opinion leaders. </li>
<li>Then, we use 2nd-stage clustering to analyze the characteristics of uses and shrink the size of the candidate set.</li>
<li>Finally, we pick k users have better leadership quality from candidate set as opinion leaders. <h4 id="Modularity-gain"><a href="#Modularity-gain" class="headerlink" title="Modularity gain"></a>Modularity gain</h4></li>
</ol>
<ul>
<li>Given a social network G = (V, E, W) and its clustering result C = {c1, c2, …, cp}, the modularity function is defined as:<br><img src="http://oonaavjvi.bkt.clouddn.com/OLM06.jpg" alt="image"></li>
<li>OLMiner utilizes the modularity gain as the terminated criteria.<h4 id="Significant-Community"><a href="#Significant-Community" class="headerlink" title="Significant Community"></a>Significant Community</h4></li>
<li>The set of significant community Cs is defined as follows:<br><img src="http://oonaavjvi.bkt.clouddn.com/OLM07.jpg" alt="image"></li>
</ul>
<h3 id="4-Opinion-Leader-Candidate-Generation"><a href="#4-Opinion-Leader-Candidate-Generation" class="headerlink" title="4. Opinion Leader Candidate Generation"></a>4. Opinion Leader Candidate Generation</h3><p><img src="http://oonaavjvi.bkt.clouddn.com/OLM08.jpg" alt="image"><br>Different from others, we use kmean clustering to build the candidate set, the kmean clustering can effectively shrink the size of candidate set.</p>
<ul>
<li>Candidate Generation Algorithm(kmean)</li>
</ul>
<blockquote>
<p>Input: Cs = {c1, c2,…, cn}: significant communities, k: user-specified number of opinion leaders<br>Output: Candi_set: a candidate set </p>
<ol>
<li>Candi_set;</li>
<li>for each ci ∈ Cs do </li>
<li>randomly select k nodes in ci as centroids s1,…, sk; </li>
<li>CKi = { cki1, cki2,…, ckik} set each s1,…, sk as a cluster in CKi; </li>
<li>while true do // kmean clustering </li>
<li>for each u ∈ ci do </li>
<li>calculate the distance to s1,…, sk; // based on four characteristics in Table 1 </li>
<li>if distance(sj) is shortest </li>
<li>ckij ckij ∪ u; </li>
<li>re-calculate the centroids of each cluster in CKi; // re-calculate each sj in ckij </li>
<li>if no more node changes cluster </li>
<li>break; </li>
<li>…</li>
</ol>
</blockquote>
<ul>
<li>Given a clustering result CKi = {cki1, cki2, …, ckin} in a significant community ci, the score of ckij is evaluated as,<br><img src="http://oonaavjvi.bkt.clouddn.com/OLM09.jpg" alt="image"></li>
</ul>
<h3 id="5-Opinion-Leader-Selection"><a href="#5-Opinion-Leader-Selection" class="headerlink" title="5. Opinion Leader Selection"></a>5. Opinion Leader Selection</h3><ul>
<li>First, we use the total number of nodes in significant communities to decide how many opinion leaders are allocated to each significant community.<br><img src="http://oonaavjvi.bkt.clouddn.com/OLM10.jpg" alt="image"></li>
<li>Then, we first sort the clusters in decreasing order based on the score of cluster (line 4, Algorithm 5). Then, OLMiner picks the nodes in high-score cluster until we reach the number of desired opinion leaders k</li>
</ul>
<h2 id="EXPERIMENTAL-RESULTS"><a href="#EXPERIMENTAL-RESULTS" class="headerlink" title="EXPERIMENTAL RESULTS"></a>EXPERIMENTAL RESULTS</h2><p><img src="http://oonaavjvi.bkt.clouddn.com/OLM11.jpg" alt="image"></p>
<ul>
<li>The content of dataset include the subjects of article, article content, author, reply content, and the time of publish or reply articles.</li>
<li>We compare OLMiner with att_clustering which use a clustering method only based on the characteristics of individuals to find opinion leaders. </li>
<li>we use the influence spread as metric<br><img src="http://oonaavjvi.bkt.clouddn.com/OLM12.jpg" alt="image"></li>
<li>The result of OLMiner is better than att_clustering.<br><img src="http://oonaavjvi.bkt.clouddn.com/OLM13.jpg" alt="image"><img src="http://oonaavjvi.bkt.clouddn.com/OLM14.jpg" alt="image"><br>By our observation, the influence of att_clustering increase slowdown when n is greater than 130. We can clearly point out that influence overlapping problem is serious in this criterion. The steady increase of influence of OLMiner indicates that the proposed method can effectively reduce the impact of influence overlapping problems and discover opinion leaders with better qualities. </li>
</ul>
<h2 id="CONCLUSION"><a href="#CONCLUSION" class="headerlink" title="CONCLUSION"></a>CONCLUSION</h2><p>Recently, opinion leader discovery has drawn much attention due to its widespread applicability. In this paper, we develop a novel algorithm, OLMiner, which integrates network structure and leadership quality analysis methods to efficiently find opinion leaders in a large social network. We utilize the two-stage clustering methods to significantly reduce the impact of influence overlapping problem in social network. OLMiner also utilizes the sentiment analysis to distinguish the opinion trend of discovered opinion leaders. Finally, to mention the practicability, we perform the proposed algorithm on real datasets. The experimental results show that OLMiner could detect more qualified opinion leaders under different criteria and effectively solve the influence overlapping problem.</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;p&gt; By identifying the opinion leaders, companies or governments can manipulate the selling or guiding public opinion, respectively. Additionally, detecting the influential comments is able to understand the source and trend of public opinion formation. However, mining opinion leaders in a huge social network is a challenge task because of the complexity of graph processing and leadership analysis. In this study, a novel algorithm, OLMiner, is proposed to efficiently find the opinion leaders from a huge social network.&lt;br&gt;
    
    </summary>
    
      <category term="研究" scheme="https://xhxt2008.github.io/categories/%E7%A0%94%E7%A9%B6/"/>
    
    
      <category term="未翻译" scheme="https://xhxt2008.github.io/tags/%E6%9C%AA%E7%BF%BB%E8%AF%91/"/>
    
      <category term="英文" scheme="https://xhxt2008.github.io/tags/%E8%8B%B1%E6%96%87/"/>
    
      <category term="社交网络" scheme="https://xhxt2008.github.io/tags/%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%BB%9C/"/>
    
      <category term="意见领袖" scheme="https://xhxt2008.github.io/tags/%E6%84%8F%E8%A7%81%E9%A2%86%E8%A2%96/"/>
    
      <category term="数据挖掘" scheme="https://xhxt2008.github.io/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="聚类" scheme="https://xhxt2008.github.io/tags/%E8%81%9A%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>Paper survey about Timing-Based Facial Expression Recognition of Kyoto University by Prof. Kawashima 　</title>
    <link href="https://xhxt2008.github.io/2017/11/22/paper-survey-kyoto/"/>
    <id>https://xhxt2008.github.io/2017/11/22/paper-survey-kyoto/</id>
    <published>2017-11-22T09:23:11.000Z</published>
    <updated>2017-11-22T08:47:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>顔画像の時系列変化を分析する研究は面白くて、すごい研究と思います。この研究の核心は：ハイブリッド・ダイナミカル・システム（HDS）、その意味は、離散事象系と力学系モデルの統合である、複雑な技術です。さらに、離散事象系はただ、力学系線形システムのパラメーターを推定するため使われています、一番重要なのは力学系線形システム。</p>
<h2 id="プロセスとしては「１」："><a href="#プロセスとしては「１」：" class="headerlink" title="プロセスとしては「１」："></a>プロセスとしては「１」：</h2><p>１.    特徴抽出。マイクやカメラでキャプチャーしたデー タを特徴抽出することで時系列特徴ベクトル(観測ベク トル)が得られる「２」、って書きましたけど、特徴抽出の方法に対して、全く説明されていません。私なりに考えたですけど、このように目や鼻と口をはっきり分別できるっていう時点で、幾何学的な特徴抽出と思われます。<br><img src="http://oonaavjvi.bkt.clouddn.com/kyoto%20face1.png" alt="image"><br> <a id="more"></a><br>２.    線形システムの階層的クラスタリング。適当な基準で比較的短い区間に分節化し，分節化された各区間で，それぞれ線形システムを同定する。そして、全てのシステムのKL距離（Kullback- Leibler (KL) divergence）を元付き、クラスタリングをやりつづ。（ここは離散事象系を意味するステップです、時間と関係なく、クラスタリングします）<br>３.    EM アルゴリズムによるパラメタ調整。一段階目で得られた線形システムの個数を，この段階 では固定し，EM アルゴリズムを適用する.一段階目の 線形システムのクラスタリングによって，各線形システ ムのパラメタは大まかに推定されている.これにより， EM アルゴリズムの初期値依存性が解決されることになる.<br> <img src="http://oonaavjvi.bkt.clouddn.com/kyoto%20face2.png" alt="image"><br>このステップは一番大事です、詳しくは「２、３」で説明されました、まだよく理解していません。抽象的に理解すると：笑い出すという動きは、どうやって線形システムで描写できますか。例えば、目を細めにして、その同時に、口があけまし、口の両端は上になど。時間とともに線的人の表情をまとめる。そのパラメーター調整はEMアルゴリズムで学習します。</p>
<h2 id="応用として："><a href="#応用として：" class="headerlink" title="応用として："></a>応用として：</h2><p>このシステムは静止画像ではなく、動画に応用しています。作り笑いと心からの笑いを区別できます（口は笑いましたけど、目はまだわらていません）（違う線形システム？）。ロボット視覚で応用できます。<br><img src="http://oonaavjvi.bkt.clouddn.com/kyoto%20face3.png" alt="image"></p>
<h2 id="感想："><a href="#感想：" class="headerlink" title="感想："></a>感想：</h2><p>この研究は長年に渡して、力学系モデリングをベースにして、面白くて、役に立てる研究と思います。<br>私のプランでは、データ科学をベースにする、学習方法の改善、あるいは、新し応用を見つけ。具体的には、まだいろいろ調べて、考え中ですけれど、一応先生に報告します。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol>
<li><a href="http://vision.kuee.kyoto-u.ac.jp/~hiroaki/publication/Kawashima_2010_ISCIE.pdf" target="_blank" rel="noopener">http://vision.kuee.kyoto-u.ac.jp/~hiroaki/publication/Kawashima_2010_ISCIE.pdf</a></li>
<li><a href="http://vision.kuee.kyoto-u.ac.jp/~hiroaki/publication/Kawashima_2004_IBIS.pdf" target="_blank" rel="noopener">http://vision.kuee.kyoto-u.ac.jp/~hiroaki/publication/Kawashima_2004_IBIS.pdf</a></li>
<li><a href="http://vision.kuee.kyoto-u.ac.jp/~hiroaki/publication/Kawashima_2005_IEICE-EA.pdf" target="_blank" rel="noopener">http://vision.kuee.kyoto-u.ac.jp/~hiroaki/publication/Kawashima_2005_IEICE-EA.pdf</a></li>
<li><a href="http://vision.kuee.kyoto-u.ac.jp/~hiroaki/research/05-hyojyofu.html" target="_blank" rel="noopener">http://vision.kuee.kyoto-u.ac.jp/~hiroaki/research/05-hyojyofu.html</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;顔画像の時系列変化を分析する研究は面白くて、すごい研究と思います。この研究の核心は：ハイブリッド・ダイナミカル・システム（HDS）、その意味は、離散事象系と力学系モデルの統合である、複雑な技術です。さらに、離散事象系はただ、力学系線形システムのパラメーターを推定するため使われています、一番重要なのは力学系線形システム。&lt;/p&gt;
&lt;h2 id=&quot;プロセスとしては「１」：&quot;&gt;&lt;a href=&quot;#プロセスとしては「１」：&quot; class=&quot;headerlink&quot; title=&quot;プロセスとしては「１」：&quot;&gt;&lt;/a&gt;プロセスとしては「１」：&lt;/h2&gt;&lt;p&gt;１.    特徴抽出。マイクやカメラでキャプチャーしたデー タを特徴抽出することで時系列特徴ベクトル(観測ベク トル)が得られる「２」、って書きましたけど、特徴抽出の方法に対して、全く説明されていません。私なりに考えたですけど、このように目や鼻と口をはっきり分別できるっていう時点で、幾何学的な特徴抽出と思われます。&lt;br&gt;&lt;img src=&quot;http://oonaavjvi.bkt.clouddn.com/kyoto%20face1.png&quot; alt=&quot;image&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="研究" scheme="https://xhxt2008.github.io/categories/%E7%A0%94%E7%A9%B6/"/>
    
    
      <category term="未翻译" scheme="https://xhxt2008.github.io/tags/%E6%9C%AA%E7%BF%BB%E8%AF%91/"/>
    
      <category term="人脸表情识别" scheme="https://xhxt2008.github.io/tags/%E4%BA%BA%E8%84%B8%E8%A1%A8%E6%83%85%E8%AF%86%E5%88%AB/"/>
    
      <category term="表情识别" scheme="https://xhxt2008.github.io/tags/%E8%A1%A8%E6%83%85%E8%AF%86%E5%88%AB/"/>
    
      <category term="论文查找" scheme="https://xhxt2008.github.io/tags/%E8%AE%BA%E6%96%87%E6%9F%A5%E6%89%BE/"/>
    
      <category term="日文" scheme="https://xhxt2008.github.io/tags/%E6%97%A5%E6%96%87/"/>
    
      <category term="时间序列" scheme="https://xhxt2008.github.io/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/"/>
    
      <category term="动态表情" scheme="https://xhxt2008.github.io/tags/%E5%8A%A8%E6%80%81%E8%A1%A8%E6%83%85/"/>
    
  </entry>
  
  <entry>
    <title>Paper survey about Face Expression Recognition</title>
    <link href="https://xhxt2008.github.io/2017/11/22/Face-expression-recognition/"/>
    <id>https://xhxt2008.github.io/2017/11/22/Face-expression-recognition/</id>
    <published>2017-11-22T08:41:12.000Z</published>
    <updated>2018-04-28T15:56:34.000Z</updated>
    
    <content type="html"><![CDATA[<p>表情認識（Face Expression Recognition）は、成熟した技術でした。表情認識確かに国籍認識と共通点が多かったです。</p>
<h2 id="まずは顔に表す感情の定義として。"><a href="#まずは顔に表す感情の定義として。" class="headerlink" title="まずは顔に表す感情の定義として。"></a>まずは顔に表す感情の定義として。</h2><p>普通六つの分類があります：楽しく、悲しく、怒る、驚く、嫌い、怖い。<br>    そして、表情認識のプロセスは、主に：</p>
<ul>
<li>前の処理。サイズ、明るさ、仕草の平均化。</li>
<li>特徴の抽出。</li>
<li>分類。<a id="more"></a>
<h2 id="特徴の抽出の方法にしては："><a href="#特徴の抽出の方法にしては：" class="headerlink" title="特徴の抽出の方法にしては："></a>特徴の抽出の方法にしては：</h2></li>
</ul>
<ol>
<li>幾何学的特徴、人の五官の位置を探して、それぞれの大きさ、距離、形状などを用いて、表情を識別します。</li>
<li>PCAやICA。総体的に特徴を抽出方法</li>
<li>LBP。局部の特徴を抽出。</li>
<li>Gabor filters。画像信号を時空から頻度に変えて、特徴を抽出。（フェーリエやガウシャン変化を用いる）、よくANNやSVMと一緒に使われる。</li>
<li>optical flow。動画で使う方法。「７」</li>
<li>neural networkで特徴を選ぶ研究もあります。</li>
</ol>
<h2 id="分類にしては："><a href="#分類にしては：" class="headerlink" title="分類にしては："></a>分類にしては：</h2><ol>
<li>Linear classifier「１」</li>
<li>ANN</li>
<li>SVM</li>
<li>HMM(Hidden Markov Model)</li>
</ol>
<h2 id="今の主な研究方向："><a href="#今の主な研究方向：" class="headerlink" title="今の主な研究方向："></a>今の主な研究方向：</h2><ol>
<li>視覚と聴覚（audio-visual）を組み合わせして、表情の識別、音声認識の性能向上「３」。</li>
<li>サンプルが少ない場合の識別「４」。</li>
<li>表情認識を顔識別の性能向上での応用「５、６」。</li>
<li>表情認識を使って、生徒が授業に集中しているのかどうかの研究「8」。（応用が新しい）</li>
<li>アルゴリズム的な革新</li>
</ol>
<h2 id="基礎な表情認識は携帯アプリまであります。（Polygram）"><a href="#基礎な表情認識は携帯アプリまであります。（Polygram）" class="headerlink" title="基礎な表情認識は携帯アプリまであります。（Polygram）"></a>基礎な表情認識は携帯アプリまであります。（Polygram）</h2><p> <img src="http://oonaavjvi.bkt.clouddn.com/%E5%9B%BE%E7%89%87%201.jpg" alt="image"><br>Polygramは自撮りをシェアするアプリで、他の観客の写真を見る時の気分を認識できます。画像に示しているのは驚きの表情。</p>
<h2 id="自分の考え："><a href="#自分の考え：" class="headerlink" title="自分の考え："></a>自分の考え：</h2><ol>
<li>視覚と聴覚（audio-visual）を組み合わせだけではなく、体温や心拍を含めれば、認識の性能はさらにアップできますはずです。このような技術は、運転手状態管理に役に立てるかもしれません。</li>
<li>片方の光源のジャミングに関してけれど。去年参加した連携大学の授業で、アイシン精機はとても完成度高いな技術を展示しました。</li>
<li>新し応用については、今オンライン教育が盛んて、どうやって表情認識をオンライン教育に応用できるのは、役に立てます。オンラインテストも同じです。</li>
</ol>
<h2 id="後書き"><a href="#後書き" class="headerlink" title="後書き"></a>後書き</h2><p>表情認識は面白い研究方向です、今後の研究や就職に役に立てると思います。しかし、私はそれについて専門知識は不十分です、そして成熟した技術なため、本当に役に立てると方向を決めても、新しい方法を提案して、実現し難い場合もあります。<br>そこで、表情認識研究する同時に、前のテーマ：株の予測も同時にやると思います。新し成果や自分の考えがあれば、先生に報告します。</p>
<h2 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h2><ol>
<li>J Anil, “Literature survey on face and face expression recognition”, 2016</li>
<li>Yani Zhu, “Face expression recognition based on equable principal component analysis and linear regression classification”, 2016</li>
<li>Ashish Tawari, “Face Expression Recognition by Cross Modal Data Association”, 2013</li>
<li>Guodong Guo, “Learning from examples in the small sample case: face expression recognition”, 2005</li>
<li>Ali Moeini, “Real-World and Rapid Face Recognition Toward Pose and Expression Variations via Feature Library Matrix”, 2015</li>
<li>Mihai Gavrilescu, “Study on using individual differences in facial expressions for a face recognition system immune to spoofing attack”, 2016</li>
<li>Chao-Kuei Hsieh, “An Optical Flow-Based Approach to Robust Face Recognition Under Expression Variations”, 2010</li>
<li>Jacob Whitehill, “he Faces of Engagement: Automatic Recognition of Student Engagementfrom Facial Expressions”, 2014</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;表情認識（Face Expression Recognition）は、成熟した技術でした。表情認識確かに国籍認識と共通点が多かったです。&lt;/p&gt;
&lt;h2 id=&quot;まずは顔に表す感情の定義として。&quot;&gt;&lt;a href=&quot;#まずは顔に表す感情の定義として。&quot; class=&quot;headerlink&quot; title=&quot;まずは顔に表す感情の定義として。&quot;&gt;&lt;/a&gt;まずは顔に表す感情の定義として。&lt;/h2&gt;&lt;p&gt;普通六つの分類があります：楽しく、悲しく、怒る、驚く、嫌い、怖い。&lt;br&gt;    そして、表情認識のプロセスは、主に：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;前の処理。サイズ、明るさ、仕草の平均化。&lt;/li&gt;
&lt;li&gt;特徴の抽出。&lt;/li&gt;
&lt;li&gt;分類。
    
    </summary>
    
      <category term="研究" scheme="https://xhxt2008.github.io/categories/%E7%A0%94%E7%A9%B6/"/>
    
    
      <category term="未翻译" scheme="https://xhxt2008.github.io/tags/%E6%9C%AA%E7%BF%BB%E8%AF%91/"/>
    
      <category term="人脸表情识别" scheme="https://xhxt2008.github.io/tags/%E4%BA%BA%E8%84%B8%E8%A1%A8%E6%83%85%E8%AF%86%E5%88%AB/"/>
    
      <category term="人脸识别" scheme="https://xhxt2008.github.io/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"/>
    
      <category term="表情识别" scheme="https://xhxt2008.github.io/tags/%E8%A1%A8%E6%83%85%E8%AF%86%E5%88%AB/"/>
    
      <category term="论文查找" scheme="https://xhxt2008.github.io/tags/%E8%AE%BA%E6%96%87%E6%9F%A5%E6%89%BE/"/>
    
      <category term="日文" scheme="https://xhxt2008.github.io/tags/%E6%97%A5%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>sk-learn学习笔记，简单的knn分类和数据可视化实践</title>
    <link href="https://xhxt2008.github.io/2017/10/06/sk-learn-001/"/>
    <id>https://xhxt2008.github.io/2017/10/06/sk-learn-001/</id>
    <published>2017-10-06T05:54:32.000Z</published>
    <updated>2017-10-16T19:19:31.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="官方示例"><a href="#官方示例" class="headerlink" title="官方示例"></a>官方示例</h2> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;<span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">&gt;&gt;&gt;<span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">&gt;&gt;&gt;iris = datasets.load_iris()</span><br><span class="line">&gt;&gt;&gt;iris_X = iris.data   <span class="comment">#iris是一组关于花蕊四个性状的数据集</span></span><br><span class="line">&gt;&gt;&gt;iris_y = iris.target     <span class="comment">#这些花蕊相对均匀的分为三个类别</span></span><br><span class="line">&gt;&gt;&gt;np.unique(iris_y)</span><br><span class="line">array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<p>读出sk-learn自带的数据集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Split iris data in train and test data</span></span><br><span class="line"><span class="comment"># A random permutation, to split the data randomly</span></span><br><span class="line">&gt;&gt;&gt;np.random.seed(<span class="number">0</span>)</span><br><span class="line">&gt;&gt;&gt;indices = np.random.permutation(len(iris_X))</span><br><span class="line">&gt;&gt;&gt;iris_X_train = iris_X[indices[:<span class="number">-10</span>]]</span><br><span class="line">&gt;&gt;&gt;iris_y_train = iris_y[indices[:<span class="number">-10</span>]]</span><br><span class="line">&gt;&gt;&gt;iris_X_test  = iris_X[indices[<span class="number">-10</span>:]]</span><br><span class="line">&gt;&gt;&gt;iris_y_test  = iris_y[indices[<span class="number">-10</span>:]]</span><br><span class="line">&gt;&gt;&gt;len(iris_X_train)</span><br><span class="line"><span class="number">140</span></span><br></pre></td></tr></table></figure>
<p>先把数据打乱，再把形状数据和类别数据分别切成训练集和测试集。事实上iris数据集共有150条数据，我们取140条作为训练集，10条作为测试集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create and fit a nearest-neighbor classifier</span></span><br><span class="line">&gt;&gt;&gt;<span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line">&gt;&gt;&gt;knn = KNeighborsClassifier()</span><br><span class="line">&gt;&gt;&gt;knn.fit(iris_X_train, iris_y_train) </span><br><span class="line">&gt;&gt;&gt;knn.predict(iris_X_test)</span><br><span class="line">array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p>就这么轻巧的预测结果就出来了，简单的令人发指。<br><a id="more"></a></p>
<h2 id="一些思考"><a href="#一些思考" class="headerlink" title="一些思考"></a>一些思考</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;<span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  </span><br><span class="line">&gt;&gt;&gt;plt.plot(iris_X_train)</span><br><span class="line">&gt;&gt;&gt;plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://oonaavjvi.bkt.clouddn.com/sk001.jpg" alt="image"></p>
<p>我想直观的看一下这组数据究竟长什么样，直接plot的话结果是这样的，于是我想让他在坐标系里显示，为此，需要去掉一个维度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#把数组转化为dataFrame</span></span><br><span class="line">&gt;&gt;&gt;<span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">&gt;&gt;&gt;<span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">&gt;&gt;&gt;iris_X_train_1 = pd.DataFrame(iris_X_train)</span><br><span class="line">&gt;&gt;&gt;iris_Y_train_1 = pd.DataFrame(iris_y_train)</span><br><span class="line">&gt;&gt;&gt;iris_X_test_1 = pd.DataFrame(iris_X_test)</span><br><span class="line">&gt;&gt;&gt;iris_Y_test_1 = pd.DataFrame(iris_y_test)</span><br><span class="line">&gt;&gt;&gt;iris_X_train_1.head()</span><br></pre></td></tr></table></figure>
<p><img src="http://oonaavjvi.bkt.clouddn.com/WX20171016-044038@2x.png" alt="image"></p>
<p>转换成DataFrame格式其实最主要是为了执行删除列的操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;iris_X_train_2 = iris_X_train_1.drop([<span class="number">3</span>],axis=<span class="number">1</span>) <span class="comment">#除掉一个维度</span></span><br><span class="line">&gt;&gt;&gt;iris_X_test_2 = iris_X_test_1.drop([<span class="number">3</span>],axis=<span class="number">1</span>)</span><br><span class="line">&gt;&gt;&gt;iris_X_train_2.iloc[:,<span class="number">0</span>].values <span class="comment">#取其中一列并且转化成array</span></span><br></pre></td></tr></table></figure>
<p>使用drop和iloc函数分别对数组进行删除列和提取列的操作。顺便把df变回array加上.value就行了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cValue = iris_Y_train_1</span><br><span class="line">cValue.replace(&#123;<span class="number">0</span>:<span class="string">'r'</span>,<span class="number">1</span>:<span class="string">'g'</span>,<span class="number">2</span>:<span class="string">'b'</span>&#125;).values <span class="comment">#花蕊的3种类型“0，1，2”用“r，g，b”表示，便于上色</span></span><br></pre></td></tr></table></figure>
<p>X是花蕊的性状，Y是花蕊的分类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;<span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D  </span><br><span class="line">&gt;&gt;&gt;fig = plt.figure()</span><br><span class="line">&gt;&gt;&gt;ax = fig.add_subplot(<span class="number">111</span>, projection=<span class="string">'3d'</span>) </span><br><span class="line">&gt;&gt;&gt;ax.scatter(iris_X_train_2.iloc[:,<span class="number">0</span>].values,iris_X_train_2.iloc[:,<span class="number">1</span>].values,iris_X_train_2.iloc[:,<span class="number">2</span>].values,c=cValue.values)  </span><br><span class="line">&gt;&gt;&gt;plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://oonaavjvi.bkt.clouddn.com/sk002.jpg" alt="image"></p>
<p>这个好像被叫做特征空间都东西就出来了。我们可以看见被分类为三种不同类型的花蕊，他们的特征的空间。他们的边缘有一些交错，这可能会带来误差。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;knn = KNeighborsClassifier()</span><br><span class="line">&gt;&gt;&gt;knn.fit(iris_X_train_2, iris_Y_train_1) </span><br><span class="line">&gt;&gt;&gt;predict = knn.predict(iris_X_test_2)</span><br></pre></td></tr></table></figure>
<p>这里重复之前的步骤，直接调用knn的包，把预测结果赋给predict。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#计算预测精度的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuricy</span><span class="params">(test_y,predict)</span>:</span></span><br><span class="line">    same=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,len(test_y)):</span><br><span class="line">        a = test_y[i]</span><br><span class="line">        b = predict[i]</span><br><span class="line">        <span class="keyword">if</span> a==b:</span><br><span class="line"> <span class="comment">#       print(test_y[i])</span></span><br><span class="line">            same+=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> same*<span class="number">100</span> / len(test_y)</span><br></pre></td></tr></table></figure>
<p>一个简单的计算学习精度的函数。</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt;accuricy(iris_y_test,predict)</span><br><span class="line"><span class="number">90</span></span><br></pre></td></tr></table></figure>
<p>这里我们可以发现即使这组数据去掉第四个特征，也能达到同样的学习性能（90%）。通过比较，我发现在去掉第一个特征的情况下，能达到100%的精度。<br>这里我是手动的删掉某一个特征，好像有一些算法可以科学的降低维度，如我最近在论文里读到的二元主成分分析，这也是我今后要学习的。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;官方示例&quot;&gt;&lt;a href=&quot;#官方示例&quot; class=&quot;headerlink&quot; title=&quot;官方示例&quot;&gt;&lt;/a&gt;官方示例&lt;/h2&gt; &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; np&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; sklearn &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; datasets&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&amp;gt;&amp;gt;iris = datasets.load_iris()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&amp;gt;&amp;gt;iris_X = iris.data   &lt;span class=&quot;comment&quot;&gt;#iris是一组关于花蕊四个性状的数据集&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&amp;gt;&amp;gt;iris_y = iris.target     &lt;span class=&quot;comment&quot;&gt;#这些花蕊相对均匀的分为三个类别&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&amp;gt;&amp;gt;np.unique(iris_y)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;array([&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;])&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;读出sk-learn自带的数据集&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# Split iris data in train and test data&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# A random permutation, to split the data randomly&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&amp;gt;&amp;gt;np.random.seed(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&amp;gt;&amp;gt;indices = np.random.permutation(len(iris_X))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&amp;gt;&amp;gt;iris_X_train = iris_X[indices[:&lt;span class=&quot;number&quot;&gt;-10&lt;/span&gt;]]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&amp;gt;&amp;gt;iris_y_train = iris_y[indices[:&lt;span class=&quot;number&quot;&gt;-10&lt;/span&gt;]]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&amp;gt;&amp;gt;iris_X_test  = iris_X[indices[&lt;span class=&quot;number&quot;&gt;-10&lt;/span&gt;:]]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&amp;gt;&amp;gt;iris_y_test  = iris_y[indices[&lt;span class=&quot;number&quot;&gt;-10&lt;/span&gt;:]]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&amp;gt;&amp;gt;len(iris_X_train)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;number&quot;&gt;140&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;先把数据打乱，再把形状数据和类别数据分别切成训练集和测试集。事实上iris数据集共有150条数据，我们取140条作为训练集，10条作为测试集。&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# Create and fit a nearest-neighbor classifier&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; sklearn.neighbors &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; KNeighborsClassifier&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&amp;gt;&amp;gt;knn = KNeighborsClassifier()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&amp;gt;&amp;gt;knn.fit(iris_X_train, iris_y_train) &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&amp;gt;&amp;gt;knn.predict(iris_X_test)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;array([&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;])&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;就这么轻巧的预测结果就出来了，简单的令人发指。&lt;br&gt;
    
    </summary>
    
      <category term="技术" scheme="https://xhxt2008.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="数据挖掘" scheme="https://xhxt2008.github.io/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="机器学习" scheme="https://xhxt2008.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="数据可视化" scheme="https://xhxt2008.github.io/tags/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
      <category term="knn" scheme="https://xhxt2008.github.io/tags/knn/"/>
    
  </entry>
  
  <entry>
    <title>(笑)</title>
    <link href="https://xhxt2008.github.io/2017/07/11/%E7%AC%91/"/>
    <id>https://xhxt2008.github.io/2017/07/11/笑/</id>
    <published>2017-07-11T15:54:34.000Z</published>
    <updated>2017-11-02T09:39:51.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://oonaavjvi.bkt.clouddn.com/sunset.jpg" alt="image"></p>
<p>作曲 : bassy</p>
<p>作词 : 茶太</p>
<p>翻译 ：Tsukiyo</p>
<p>孤独に(笑)を足してみる<br>尝试着充实孤独（笑）</p>
<p>独り善がりの嗤いに変わる<br>变成了自以为是的嗤笑</p>
<p>感情をただ食み散らしたまま<br>感情被蚕食殆尽</p>
<p>渦巻きはそっと消えてゆく<br>最后的漩涡也悄悄的消失了</p>
<h3 id="「-笑-」"><a href="#「-笑-」" class="headerlink" title="「(笑)」"></a>「(笑)」</h3><a id="more"></a>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=640424&auto=1&height=66"></iframe>

<p>もっと　やれたはずと　後悔だけを抱きしめて<br>“当时我要是更努力的话。。” 现在却只剩下后悔</p>
<p>今度　次があれば　うまく出来るといいきかせ<br>“下次，要是再来一次的话。。” 借口我能做到</p>
<p>気づけば自分に積み重ねているんだ期待を<br>发现的时候积压在自己身上的期待</p>
<p>支えきれなくて崩してしまうだけなのに<br>已经快要支持不住的崩塌</p>
<p>ちょっと　間違っただけ　でも何処でかが解らない<br>稍微  就是有些地方出了错  但是我并不知道错在哪</p>
<p>なんて　本当は知ってる　踏み抜いてしまったその場所<br>骗你的  其实我都知道  应为我是故意的啊</p>
<p>同じ輪描いてぐるぐると泳ぎ続けながら<br>画着同一个圆圈  然后在里面一圈一圈的转</p>
<p>またその時にもどって　もどって　上手にできる夢を見て<br>又回到了那个时候  回去了  幻想着自己能做到</p>
<p>涙に(笑)を足してみる<br>尝试着充实眼泪（笑）</p>
<p>強がる顔に両手を当てる<br>双手贴在倔强的脸上</p>
<p>要らない物しか残らないから<br>留下的尽是些没用的东西</p>
<p>ならば　と僕は笑ってみる<br>那么  就让我笑吧</p>
<p>心に(笑)を足してみる<br>尝试着充实内心（笑）</p>
<p>脆すぎるからすぐ歪んじゃう<br>脆弱到立刻就变形</p>
<p>真実（本当）も嘘も　いつも紙一重<br>真实与虚假从来都是一纸之隔</p>
<p>ならば　と僕は笑って言う<br>那么  就让我笑着说</p>
<p>笑って　失って　欠けてって　もう戻らない<br>笑吧  失去吧  破碎吧  再也不会回来</p>
<p>笑って　失って　欠けてって　もう戻れない<br>笑吧  失去吧  破碎吧  再也回不来了</p>
<p>もっと　やれたのかな　辿る時間の逆回し<br>我这次进步了吗  随着时间的回溯</p>
<p>なんて　本当は知ってる　わかってるから目を閉じて<br>骗你的  其实我都知道  都知道了我就闭上眼</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://oonaavjvi.bkt.clouddn.com/sunset.jpg&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;
&lt;p&gt;作曲 : bassy&lt;/p&gt;
&lt;p&gt;作词 : 茶太&lt;/p&gt;
&lt;p&gt;翻译 ：Tsukiyo&lt;/p&gt;
&lt;p&gt;孤独に(笑)を足してみる&lt;br&gt;尝试着充实孤独（笑）&lt;/p&gt;
&lt;p&gt;独り善がりの嗤いに変わる&lt;br&gt;变成了自以为是的嗤笑&lt;/p&gt;
&lt;p&gt;感情をただ食み散らしたまま&lt;br&gt;感情被蚕食殆尽&lt;/p&gt;
&lt;p&gt;渦巻きはそっと消えてゆく&lt;br&gt;最后的漩涡也悄悄的消失了&lt;/p&gt;
&lt;h3 id=&quot;「-笑-」&quot;&gt;&lt;a href=&quot;#「-笑-」&quot; class=&quot;headerlink&quot; title=&quot;「(笑)」&quot;&gt;&lt;/a&gt;「(笑)」&lt;/h3&gt;
    
    </summary>
    
      <category term="杂记" scheme="https://xhxt2008.github.io/categories/%E6%9D%82%E8%AE%B0/"/>
    
    
      <category term="茶太" scheme="https://xhxt2008.github.io/tags/%E8%8C%B6%E5%A4%AA/"/>
    
      <category term="歌词翻译" scheme="https://xhxt2008.github.io/tags/%E6%AD%8C%E8%AF%8D%E7%BF%BB%E8%AF%91/"/>
    
      <category term="同人音乐" scheme="https://xhxt2008.github.io/tags/%E5%90%8C%E4%BA%BA%E9%9F%B3%E4%B9%90/"/>
    
  </entry>
  
  <entry>
    <title>【剧透！】深挖剧情（三观正直），如何正确观看《人渣的本愿》</title>
    <link href="https://xhxt2008.github.io/2017/05/02/kuzunohonkai/"/>
    <id>https://xhxt2008.github.io/2017/05/02/kuzunohonkai/</id>
    <published>2017-05-02T05:31:22.000Z</published>
    <updated>2018-08-31T08:25:53.276Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://oonaavjvi.bkt.clouddn.com/main1.jpg" alt="人渣的本愿"></p>
<p>《人渣的本愿》完结已经有两个月了，想看的应该早就看完了。这部动画描写了很多滥情的戏码，但是表达了一个非常积极正面的主题。这部动漫对情感问题的刻画入微，短短12话的内容可以说做到了面面俱到，是十分优秀的作品。但是，笔者观察到网上充满了对这部动画的曲解，和不公平的评论，也有一些好一点的，比如lex的<a href="http://www.bilibili.com/video/av9501531/" target="_blank" rel="noopener">【Lex】三观炸裂？男默女泪？小评《人渣的本愿》</a>。说的是不错的，但是太过照顾一部分人的心情了。另外一些称赞的声音也完全没有说到点子上。故作此文。</p>
<p>这部动漫细节非常多，大量使用插叙，用了很多漫画的分镜技术表达复杂的感情。当然，开车情节也很多，还有百合元素。如果想仔细看的话，可以说很有内容很有看头。吃瓜群众们光是看看开车，享受一下优秀的制作，也是非常不错的。<br>话还是要从女主角，花火，说起。</p>
<h1 id="警告！本文深度剧透，没看过作品的请谨慎观看"><a href="#警告！本文深度剧透，没看过作品的请谨慎观看" class="headerlink" title="==警告！本文深度剧透，没看过作品的请谨慎观看=="></a>==警告！本文深度剧透，没看过作品的请谨慎观看==</h1><a id="more"></a>
<h2 id="约1-3话的剧情"><a href="#约1-3话的剧情" class="headerlink" title="约1~3话的剧情"></a>约1~3话的剧情</h2><h3 id="花火有多可爱"><a href="#花火有多可爱" class="headerlink" title="花火有多可爱"></a>花火有多可爱</h3><p>看过这部动漫的人，多半是花火的同情者，理由多是花火很可爱。我们来看一下作者是怎么说的。<br><img src="http://oonaavjvi.bkt.clouddn.com/QQ%E6%88%AA%E5%9B%BE20170503011729.jpg" alt="人渣的本愿"></p>
<blockquote>
<ol>
<li>被花火瞪着的男生感觉心里美滋滋</li>
<li>在电车上拍到痴汉的脸成功解救闺蜜</li>
<li>花火带球过人下面的男生眼睛都看直了</li>
<li>男主想着别人的时候对花火的评价</li>
</ol>
</blockquote>
<p>作为女主角，花火自然被赋予了善良、单纯、勇敢、漂亮这样的特质。可以看出，作者也是很喜欢这个角色的。如果说接下来的剧情说的不是情感纠纷，而是漫游仙境，花火在这样的童话故事里，也是毫无违和感的。</p>
<h6 id="在这部动漫里的设定里，花火的受欢迎程度显得有些高得离奇，她可能确实挺可爱，但也并不是最出众的。"><a href="#在这部动漫里的设定里，花火的受欢迎程度显得有些高得离奇，她可能确实挺可爱，但也并不是最出众的。" class="headerlink" title="在这部动漫里的设定里，花火的受欢迎程度显得有些高得离奇，她可能确实挺可爱，但也并不是最出众的。"></a>在这部动漫里的设定里，花火的受欢迎程度显得有些高得离奇，她可能确实挺可爱，但也并不是最出众的。</h6><h6 id="其实这一点是非常真实的，在现实中，有人统计过，追求者最多的女生往往不是那种“女神”级别的。比起漂亮、美丽的女生，可爱的、身高稍微有点矮、性格比较直率的女生受到追求的机会更大。（原因可能是，男生觉得这种的比较容易接近，难度会低一些？）"><a href="#其实这一点是非常真实的，在现实中，有人统计过，追求者最多的女生往往不是那种“女神”级别的。比起漂亮、美丽的女生，可爱的、身高稍微有点矮、性格比较直率的女生受到追求的机会更大。（原因可能是，男生觉得这种的比较容易接近，难度会低一些？）" class="headerlink" title="其实这一点是非常真实的，在现实中，有人统计过，追求者最多的女生往往不是那种“女神”级别的。比起漂亮、美丽的女生，可爱的、身高稍微有点矮、性格比较直率的女生受到追求的机会更大。（原因可能是，男生觉得这种的比较容易接近，难度会低一些？）"></a>其实这一点是非常真实的，在现实中，有人统计过，追求者最多的女生往往不是那种“女神”级别的。比起<strong>漂亮</strong>、<strong>美丽</strong>的女生，<strong><em>可爱</em></strong>的、身高稍微有点<strong><em>矮</em></strong>、性格比较<strong><em>直率</em></strong>的女生受到追求的机会更大。（原因可能是，男生觉得这种的比较容易接近，难度会低一些？）</h6><h4 id="除此之外？"><a href="#除此之外？" class="headerlink" title="除此之外？"></a>除此之外？</h4><p><img src="http://oonaavjvi.bkt.clouddn.com/QQ%E6%88%AA%E5%9B%BE20170503012200.jpg" alt="image"></p>
<blockquote>
<p>花火在第一话里拒绝了一个男生</p>
</blockquote>
<p>就像一枚硬币总有正反两面一样，<strong>率直</strong>也有它负的一面。拒绝就拒绝，也不必把话说得这么难听。别人把喜欢说出来，也是需要<strong>勇气</strong>的，显然，花火在这个时间点，还不能理解这一点。<br>这里也伤到了一部分观众的心了吧。</p>
<h4 id="但是呢？"><a href="#但是呢？" class="headerlink" title="但是呢？"></a>但是呢？</h4><p><img src="http://oonaavjvi.bkt.clouddn.com/QQ%E6%88%AA%E5%9B%BE20170503134503.jpg" alt="image"></p>
<blockquote>
<p>花火质问男主为什么不明确的拒绝他的青梅竹马</p>
</blockquote>
<p>这部动漫对于人物的描写，细节多的令人惊讶。看似随意的一件小事，实际上为后文埋下了伏笔。这里便给出了花火之前不留情面的理由了，并不是单纯的<strong><em>性格差</em></strong>，因为她</p>
<blockquote>
<h4 id="觉得这样做对对方更好"><a href="#觉得这样做对对方更好" class="headerlink" title="觉得这样做对对方更好"></a>觉得这样做对对方更好</h4></blockquote>
<h6 id="的确，与明确的拒绝相比，故意对明明不喜欢的人透露模棱两可的答复，是一种非常不健康的行为。因为当事人的目的，可能仅仅是为了得到更多来自追求者，无私的好意。现实生活中，充斥着这样的人，和这种观念。-认为年轻貌美是一种可以物化的资本，可以毫无罪恶感的从所谓的备胎身上榨取好处，这本质上是一种对自我价值的贬低。说得通俗一点，就是婊子嘛。这是不能被接受的。"><a href="#的确，与明确的拒绝相比，故意对明明不喜欢的人透露模棱两可的答复，是一种非常不健康的行为。因为当事人的目的，可能仅仅是为了得到更多来自追求者，无私的好意。现实生活中，充斥着这样的人，和这种观念。-认为年轻貌美是一种可以物化的资本，可以毫无罪恶感的从所谓的备胎身上榨取好处，这本质上是一种对自我价值的贬低。说得通俗一点，就是婊子嘛。这是不能被接受的。" class="headerlink" title="的确，与明确的拒绝相比，故意对明明不喜欢的人透露模棱两可的答复，是一种非常不健康的行为。因为当事人的目的，可能仅仅是为了得到更多来自追求者，无私的好意。现实生活中，充斥着这样的人，和这种观念。 认为年轻貌美是一种可以物化的资本，可以毫无罪恶感的从所谓的备胎身上榨取好处，这本质上是一种对自我价值的贬低。说得通俗一点，就是婊子嘛。这是不能被接受的。"></a>的确，与明确的拒绝相比，<strong>故意</strong>对明明不喜欢的人透露模棱两可的答复，是一种非常不健康的行为。因为当事人的目的，可能仅仅是为了得到更多来自追求者，无私的好意。现实生活中，充斥着这样的人，和这种观念。 认为<strong>年轻貌美</strong>是一种可以物化的资本，可以毫无罪恶感的从所谓的<strong><em>备胎</em></strong>身上榨取好处，这本质上是一种对自我价值的贬低。说得通俗一点，就是婊子嘛。这是不能被接受的。</h6><p>在这里可以看出，花火在这里虽然想法还存在幼稚的地方，但是基本的方向判断是正确的。可以说，是善良的。</p>
<h3 id="花火为什么喜欢老师？"><a href="#花火为什么喜欢老师？" class="headerlink" title="花火为什么喜欢老师？"></a>花火为什么喜欢老师？</h3><p><img src="http://oonaavjvi.bkt.clouddn.com/QQ%E6%88%AA%E5%9B%BE20170503211733.jpg" alt="image"><br>老师的家庭缺少母亲，女主的家庭缺少父亲，于是在某种程度上，老师（大哥哥）在花火的<strong>童年</strong>充当了父亲的角色，并且成为了花火<strong>憧憬</strong>的初恋对象。</p>
<h6 id="这部动画的插叙很多，一些问题的重要细节和背景都是中间穿插着交代的，乍一看很容易让人觉得有点矫情。"><a href="#这部动画的插叙很多，一些问题的重要细节和背景都是中间穿插着交代的，乍一看很容易让人觉得有点矫情。" class="headerlink" title="这部动画的插叙很多，一些问题的重要细节和背景都是中间穿插着交代的，乍一看很容易让人觉得有点矫情。"></a>这部动画的插叙很多，一些问题的重要细节和背景都是中间穿插着交代的，乍一看很容易让人觉得有点矫情。</h6><h4 id="喜欢到什么程度？"><a href="#喜欢到什么程度？" class="headerlink" title="喜欢到什么程度？"></a>喜欢到什么程度？</h4><p><img src="http://oonaavjvi.bkt.clouddn.com/QQ%E6%88%AA%E5%9B%BE20170503135326.png" alt="image"></p>
<blockquote>
<p>帅呆了！  </p>
</blockquote>
<p>花火自己是这么说的。这部动画里，老师（大哥哥）更多的是一个“老实人”的形象，与<strong>帅</strong>字并不怎么沾边。<br>恋爱可以蒙蔽人的眼睛，这一点也是十分真实的。</p>
<h6 id="有一个词叫做爱慕，憧憬的感情很容易滋生恋爱。可能因为憧憬，花火把老师的印象美化了。可能幼稚，可能失真，但是没有人能否定花火对他的“大哥哥”的恋爱。"><a href="#有一个词叫做爱慕，憧憬的感情很容易滋生恋爱。可能因为憧憬，花火把老师的印象美化了。可能幼稚，可能失真，但是没有人能否定花火对他的“大哥哥”的恋爱。" class="headerlink" title="有一个词叫做爱慕，憧憬的感情很容易滋生恋爱。可能因为憧憬，花火把老师的印象美化了。可能幼稚，可能失真，但是没有人能否定花火对他的“大哥哥”的恋爱。"></a>有一个词叫做爱慕，憧憬的感情很容易滋生恋爱。可能因为憧憬，花火把老师的印象美化了。可能幼稚，可能失真，但是没有人能否定花火对他的“大哥哥”的恋爱。</h6><h4 id="花火的恋爱观"><a href="#花火的恋爱观" class="headerlink" title="花火的恋爱观"></a>花火的恋爱观</h4><p><img src="http://oonaavjvi.bkt.clouddn.com/QQ%E6%88%AA%E5%9B%BE20170503140055.jpg" alt="image"></p>
<blockquote>
<p>花火因为跟一个帅哥（男主）谈恋爱，被认为经验丰富，同学约她出去进行恋爱相谈，为一个在发小和学长之间摇摆的女生出谋划策。她却不能对对方的想法产生共鸣。</p>
</blockquote>
<p>在现实生活中，这是一件很普通的事情，<del>这也是现实世界的可怕之处</del>。不管怎么说，花火对于大哥哥的感情是一种更加纯粹的喜欢。而不是把候选者参数化，因为对方“条件好”才要跟他在一起。</p>
<h6 id="笔者曾经讨论过这个问题，恋爱关系存在两种：因为喜欢的恋爱，和因为寂寞的恋爱。这两种关系是有明确的优劣之分的。-这部动画的态度很明确，对出于寂寞的恋爱，持的是谨慎否定态度。"><a href="#笔者曾经讨论过这个问题，恋爱关系存在两种：因为喜欢的恋爱，和因为寂寞的恋爱。这两种关系是有明确的优劣之分的。-这部动画的态度很明确，对出于寂寞的恋爱，持的是谨慎否定态度。" class="headerlink" title="笔者曾经讨论过这个问题，恋爱关系存在两种：因为喜欢的恋爱，和因为寂寞的恋爱。这两种关系是有明确的优劣之分的。 这部动画的态度很明确，对出于寂寞的恋爱，持的是谨慎否定态度。"></a>笔者曾经讨论过这个问题，恋爱关系存在两种：因为喜欢的恋爱，和因为寂寞的恋爱。这两种关系是有明确的优劣之分的。 这部动画的态度很明确，对出于寂寞的恋爱，持的是谨慎否定态度。</h6><h4 id="延伸阅读：何谓出于寂寞的恋爱？"><a href="#延伸阅读：何谓出于寂寞的恋爱？" class="headerlink" title="延伸阅读：何谓出于寂寞的恋爱？"></a>延伸阅读：何谓出于寂寞的恋爱？</h4><h6 id="笔者把他解释为一种想谈恋爱的因素大于喜欢对方的因素的恋爱。这也是现实生活中普遍存在着的。也就是说，跟你谈恋爱，可以；换一个人，也行。当事者看重的是发生恋爱关系，跟谁？不那么重要。"><a href="#笔者把他解释为一种想谈恋爱的因素大于喜欢对方的因素的恋爱。这也是现实生活中普遍存在着的。也就是说，跟你谈恋爱，可以；换一个人，也行。当事者看重的是发生恋爱关系，跟谁？不那么重要。" class="headerlink" title="笔者把他解释为一种想谈恋爱的因素大于喜欢对方的因素的恋爱。这也是现实生活中普遍存在着的。也就是说，跟你谈恋爱，可以；换一个人，也行。当事者看重的是发生恋爱关系，跟谁？不那么重要。"></a>笔者把他解释为一种<strong>想谈恋爱的因素大于喜欢对方的因素的恋爱</strong>。这也是现实生活中普遍存在着的。也就是说，跟你谈恋爱，可以；换一个人，也行。当事者看重的是<strong><em>发生恋爱关系</em></strong>，跟谁？不那么重要。</h6><p><img src="http://oonaavjvi.bkt.clouddn.com/QQ%E6%88%AA%E5%9B%BE20170503014423.jpg" alt="image"></p>
<h6 id="不那么重要的意思并不是不重要，就好像为了秀给别人看而谈恋爱，对方就必须长得帅。"><a href="#不那么重要的意思并不是不重要，就好像为了秀给别人看而谈恋爱，对方就必须长得帅。" class="headerlink" title="不那么重要的意思并不是不重要，就好像为了秀给别人看而谈恋爱，对方就必须长得帅。"></a>不那么重要的意思并不是不重要，就好像为了秀给别人看而谈恋爱，对方就必须长得帅。</h6><p><img src="http://oonaavjvi.bkt.clouddn.com/QQ%E6%88%AA%E5%9B%BE20170503134857.jpg" alt="image"></p>
<blockquote>
<p>男主和女主的关系一开始也属于“出于寂寞的恋爱”这一类，但是实际并不像乃莉子说的那样</p>
</blockquote>
<h5 id="出于寂寞的恋爱可以有很多种"><a href="#出于寂寞的恋爱可以有很多种" class="headerlink" title="出于寂寞的恋爱可以有很多种"></a>出于寂寞的恋爱可以有很多种</h5><ol>
<li>随波逐流型： 大家都有了，我也想要一个。</li>
<li>体现自我价值型： <img src="http://oonaavjvi.bkt.clouddn.com/QQ%E6%88%AA%E5%9B%BE20170504201030.jpg" alt="image">  <blockquote>
<p>喜欢被人喜欢的感觉。</p>
</blockquote>
</li>
<li>失恋反弹型： <img src="http://oonaavjvi.bkt.clouddn.com/QQ%E6%88%AA%E5%9B%BE20170504201139.jpg" alt="image"><img src="http://oonaavjvi.bkt.clouddn.com/QQ%E6%88%AA%E5%9B%BE20170506222843.jpg" alt="image"><img src="http://oonaavjvi.bkt.clouddn.com/QQ%E6%88%AA%E5%9B%BE20170503143050.jpg" alt="image"><blockquote>
<p>分了？再找一个去啊。</p>
</blockquote>
</li>
<li>被逼无奈型： 家里催我相亲了。。</li>
<li><del>秀给人看型： 俗称朋友圈制霸。活在别人的眼睛和评论里，常常在人前表现亲密，看起来比实际幸福。跟吃饭之前没拍照，饭就白吃了是一个道理。</del></li>
</ol>
<h6 id="寂寞型的恋爱，也是可以转换成真爱的。如果恋爱关系中只有一方是寂寞型的，那么对于另一方是极不负责的。如果恋爱中的双方都是出于寂寞型的，那么更加认真的一方受到伤害的可能更大。"><a href="#寂寞型的恋爱，也是可以转换成真爱的。如果恋爱关系中只有一方是寂寞型的，那么对于另一方是极不负责的。如果恋爱中的双方都是出于寂寞型的，那么更加认真的一方受到伤害的可能更大。" class="headerlink" title="寂寞型的恋爱，也是可以转换成真爱的。如果恋爱关系中只有一方是寂寞型的，那么对于另一方是极不负责的。如果恋爱中的双方都是出于寂寞型的，那么更加认真的一方受到伤害的可能更大。"></a>寂寞型的恋爱，也是可以转换成真爱的。如果恋爱关系中只有一方是寂寞型的，那么对于另一方是极不负责的。如果恋爱中的双方都是出于寂寞型的，那么更加认真的一方受到伤害的可能更大。</h6><h3 id="大哥哥有喜欢的人了怎么办？"><a href="#大哥哥有喜欢的人了怎么办？" class="headerlink" title="大哥哥有喜欢的人了怎么办？"></a>大哥哥有喜欢的人了怎么办？</h3><p><img src="http://oonaavjvi.bkt.clouddn.com/QQ%E6%88%AA%E5%9B%BE20170505162434.jpg" alt="image"></p>
<blockquote>
<p>女老师让大哥哥想起了他死去的妈妈</p>
</blockquote>
<p>大哥哥（男老师）喜欢女老师一开始就说了，但是理由在十分后面的地方才交待的。而且这个理由也有点敷衍的意思？反正我是不服的。女老师跟他妈妈很像，感情他妈妈也是个婊子？ 可能是受制于篇幅吧，这个动画里，男老师的地位有点路人。</p>
<p>不过根据经验来看，婊子玩腻了也总有人自告奋勇接盘。而且这也不是这个故事的重点，或许可以解释的更清楚，但是这也没什么问题。</p>
<h4 id="这该如何是好！"><a href="#这该如何是好！" class="headerlink" title="这该如何是好！"></a>这该如何是好！</h4><p><img src="http://oonaavjvi.bkt.clouddn.com/rz020.jpg" alt="image"><br><img src="http://oonaavjvi.bkt.clouddn.com/QQ%E6%88%AA%E5%9B%BE20170504213526.jpg" alt="image"></p>
<blockquote>
<p>女主爱的执著</p>
</blockquote>
<h6 id="大多数动漫过低的估计了人们对喜欢的人的执着，也过高的估计了两厢情愿的概率。这部动画就说了几个注定不能成功的单相思的故事。"><a href="#大多数动漫过低的估计了人们对喜欢的人的执着，也过高的估计了两厢情愿的概率。这部动画就说了几个注定不能成功的单相思的故事。" class="headerlink" title="大多数动漫过低的估计了人们对喜欢的人的执着，也过高的估计了两厢情愿的概率。这部动画就说了几个注定不能成功的单相思的故事。"></a>大多数动漫过低的估计了人们对喜欢的人的执着，也过高的估计了两厢情愿的概率。这部动画就说了几个注定不能成功的单相思的故事。</h6><p><img src="http://oonaavjvi.bkt.clouddn.com/rz021.jpg" alt="image"></p>
<blockquote>
<p>不光是女主，大家都很执着，并且痛苦</p>
</blockquote>
<h5 id="有歌云："><a href="#有歌云：" class="headerlink" title="有歌云："></a>有歌云：</h5><blockquote>
<h6 id="终于等到你，还好我没放弃"><a href="#终于等到你，还好我没放弃" class="headerlink" title="终于等到你，还好我没放弃"></a>终于等到你，还好我没放弃</h6></blockquote>
<h6 id="这不是在鼓励人们把单相思坚持下去吗？"><a href="#这不是在鼓励人们把单相思坚持下去吗？" class="headerlink" title="这不是在鼓励人们把单相思坚持下去吗？"></a>这不是在鼓励人们把单相思坚持下去吗？</h6><p><img src="http://oonaavjvi.bkt.clouddn.com/rz031.jpg" alt="image"></p>
<blockquote>
<p>花火听着大哥哥开心的聊着关于女老师的事情（注意表情）</p>
</blockquote>
<h6 id="你可以选择坚持，坚持就需要付出，但是恋爱这回事，付出跟回报就从来没有成正比过。没有什么你牺牲的多，对方就该选择你这回事。-那我们应该坚持到什么程度呢？到有了竞争对手为止？收到对方明确的拒绝为止？到对方有男女朋友位置？到结婚为止？到生了孩子为止？或者，到自己遇到下一段真正的感情为止？"><a href="#你可以选择坚持，坚持就需要付出，但是恋爱这回事，付出跟回报就从来没有成正比过。没有什么你牺牲的多，对方就该选择你这回事。-那我们应该坚持到什么程度呢？到有了竞争对手为止？收到对方明确的拒绝为止？到对方有男女朋友位置？到结婚为止？到生了孩子为止？或者，到自己遇到下一段真正的感情为止？" class="headerlink" title="你可以选择坚持，坚持就需要付出，但是恋爱这回事，付出跟回报就从来没有成正比过。没有什么你牺牲的多，对方就该选择你这回事。 那我们应该坚持到什么程度呢？到有了竞争对手为止？收到对方明确的拒绝为止？到对方有男女朋友位置？到结婚为止？到生了孩子为止？或者，到自己遇到下一段真正的感情为止？"></a>你可以选择坚持，坚持就需要付出，但是恋爱这回事，付出跟回报就从来没有成正比过。没有什么你牺牲的多，对方就该选择你这回事。 那我们应该坚持到什么程度呢？到有了竞争对手为止？收到对方明确的拒绝为止？到对方有男女朋友位置？到结婚为止？到生了孩子为止？或者，到自己遇到下一段真正的感情为止？</h6><h6 id="如果一直等不到，我们什么时候应该放弃呢？"><a href="#如果一直等不到，我们什么时候应该放弃呢？" class="headerlink" title="如果一直等不到，我们什么时候应该放弃呢？"></a>如果一直等不到，我们什么时候应该放弃呢？</h6><h6 id="笔者觉得不管如何选择，这个问题是不存在对错的，放弃的早可以是一种善良，放弃的迟可以是一种忠贞。都是无可厚非的。反之-放弃的早可以是一种敷衍，放弃得迟可以是一种变态。"><a href="#笔者觉得不管如何选择，这个问题是不存在对错的，放弃的早可以是一种善良，放弃的迟可以是一种忠贞。都是无可厚非的。反之-放弃的早可以是一种敷衍，放弃得迟可以是一种变态。" class="headerlink" title="笔者觉得不管如何选择，这个问题是不存在对错的，放弃的早可以是一种善良，放弃的迟可以是一种忠贞。都是无可厚非的。反之,放弃的早可以是一种敷衍，放弃得迟可以是一种变态。"></a>笔者觉得不管如何选择，这个问题是不存在对错的，放弃的早可以是一种善良，放弃的迟可以是一种忠贞。都是无可厚非的。反之,放弃的早可以是一种敷衍，放弃得迟可以是一种变态。</h6><h3 id="花火为什么要跟麦在一起？"><a href="#花火为什么要跟麦在一起？" class="headerlink" title="花火为什么要跟麦在一起？"></a>花火为什么要跟麦在一起？</h3><p>上面这个语境实在太悲伤了。刚刚升入高中的花火还没有<strong>勇气</strong>独自承受这些，她也选择进行一场，<strong><em>出于寂寞的恋爱</em></strong>。<br><img src="http://oonaavjvi.bkt.clouddn.com/rz022.jpg" alt="image"></p>
<blockquote>
<p>男主和女主看着各自追求的人，进行职场恋情。</p>
</blockquote>
<h4 id="麦是一个怎样的人？"><a href="#麦是一个怎样的人？" class="headerlink" title="麦是一个怎样的人？"></a>麦是一个怎样的人？</h4><p><img src="http://oonaavjvi.bkt.clouddn.com/rz026.jpg" alt="image"></p>
<blockquote>
<p>在花火最脆弱的时候，男主没有选择乘人之危</p>
</blockquote>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz027.jpg" alt="image"></p>
<blockquote>
<p>为了打破乃莉子对他的幻想，故意表现人渣</p>
</blockquote>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz028.jpg" alt="image"></p>
<blockquote>
<p>面对乃莉子的死缠烂打，麦差点就要犯错了，不过在最后一刻还是停了下来。</p>
</blockquote>
<p>麦（男主）是一个有原则，有底线的<strong><em>男孩</em></strong>，甚至是有些善良的。当然你要是问我，他的底线怎么都在床上体现了？我也。。</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz040.jpg" alt="image"></p>
<blockquote>
<p>麦劝女主多交些女性朋友</p>
</blockquote>
<p>总之，麦这个人是有作为男主角的品质的，虽然经验丰富，但是并不滥情。如果能跟女主配成一对的话，也不是不能接受。</p>
<h6 id="在现实生活中，男生中间似乎有这样的风气。在一段恋情的结束之前，上床了就算不亏，没上过床就是亏大了。-更有甚者，利用花言巧语、酒精、甚至一些非法的手段骗取一夜情。说得通俗点就是人渣，这是不能被接受的。"><a href="#在现实生活中，男生中间似乎有这样的风气。在一段恋情的结束之前，上床了就算不亏，没上过床就是亏大了。-更有甚者，利用花言巧语、酒精、甚至一些非法的手段骗取一夜情。说得通俗点就是人渣，这是不能被接受的。" class="headerlink" title="在现实生活中，男生中间似乎有这样的风气。在一段恋情的结束之前，上床了就算不亏，没上过床就是亏大了。 更有甚者，利用花言巧语、酒精、甚至一些非法的手段骗取一夜情。说得通俗点就是人渣，这是不能被接受的。"></a>在现实生活中，男生中间似乎有这样的风气。在一段恋情的结束之前，上床了就算不亏，没上过床就是亏大了。 更有甚者，利用花言巧语、酒精、甚至一些非法的手段骗取一夜情。说得通俗点就是人渣，这是不能被接受的。</h6><h4 id="麦为什么喜欢女老师？"><a href="#麦为什么喜欢女老师？" class="headerlink" title="麦为什么喜欢女老师？"></a>麦为什么喜欢女老师？</h4><p>女老师好在哪笔者不是很懂，姑且把作者给的解释贴出来。<br><img src="http://oonaavjvi.bkt.clouddn.com/rz029.jpg" alt="image"></p>
<blockquote>
<p>因为男主得不到女老师，所以喜欢</p>
</blockquote>
<p>虽然笔者不是很懂女老师好在哪，但是这种心情是很普遍的。</p>
<blockquote>
<h4 id="得不到的永远是最好的"><a href="#得不到的永远是最好的" class="headerlink" title="得不到的永远是最好的"></a>得不到的永远是最好的</h4></blockquote>
<p>此外，这里提到了很有意思的一点，男主说：</p>
<blockquote>
<p>我想改变你</p>
</blockquote>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz030.jpg" alt="image"></p>
<blockquote>
<p>男主知道女老师经常乱搞，仍然喜欢女老师，但是希望改变他。</p>
</blockquote>
<h6 id="这一点后面还会提到，我会详细阐释。"><a href="#这一点后面还会提到，我会详细阐释。" class="headerlink" title="这一点后面还会提到，我会详细阐释。"></a>这一点后面还会提到，我会详细阐释。</h6><h4 id="花火为什么选择麦（男主）？"><a href="#花火为什么选择麦（男主）？" class="headerlink" title="花火为什么选择麦（男主）？"></a>花火为什么选择麦（男主）？</h4><p>既然是处于寂寞的恋爱，不是谁都行吗？为什么花火选择了跟麦组成一对？仅仅是像乃莉子说的一样，因为麦长得帅？<br><img src="http://oonaavjvi.bkt.clouddn.com/024.jpg" alt="image"></p>
<blockquote>
<p>花火跟麦组成一对的时候，她打趣的问麦会不会真的喜欢上她。麦的回答很干脆。</p>
</blockquote>
<p>她是认真的吗！？被帅哥喜欢上不好吗？我们来回忆一下，关于花火我们知道了什么。<br><img src="http://oonaavjvi.bkt.clouddn.com/rz023.jpg" alt="image">¥</p>
<blockquote>
<p>花火从头到尾没有缺少过追求者</p>
</blockquote>
<p>花火从来不缺少追求者，但她的心里只有他的“大哥哥”，她对于追求者是无奈地，<strong><em>在这个时间点</em></strong>，甚至还有些不屑。她很善良，不愿意敷衍别人的感情，不愿意伤害别人。而男主又是一个绝不会喜欢上她的人，在某些地方与她也有些相似。<br><img src="http://oonaavjvi.bkt.clouddn.com/rz024.jpg" alt="image"></p>
<blockquote>
<p>为了弥补得不到真爱内心产生的空虚感，花火选择了暂时跟麦在一起</p>
</blockquote>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz025.jpg" alt="image"></p>
<blockquote>
<p>如果有一方的成功，关系就结束</p>
</blockquote>
<p>对这两个人来说，<strong>最美好的东西，绝对无法得到。</strong> 这件事情，令人感到窒息般的绝望。<strong><em>他们既不愿意放弃，又不想伤害别人</em></strong>。选择了互相为对方消化这份绝望。</p>
<h6 id="得不到真爱，有什么更好的出路吗？坚持很痛，但是不想放弃。去死？不至于吧，朋友们会劝说"><a href="#得不到真爱，有什么更好的出路吗？坚持很痛，但是不想放弃。去死？不至于吧，朋友们会劝说" class="headerlink" title="得不到真爱，有什么更好的出路吗？坚持很痛，但是不想放弃。去死？不至于吧，朋友们会劝说"></a>得不到真爱，有什么更好的出路吗？坚持很痛，但是不想放弃。去死？不至于吧，朋友们会劝说</h6><blockquote>
<ul>
<li>天涯何处无芳草</li>
<li>不要在一棵树上吊死</li>
<li>要不再找一个？</li>
</ul>
</blockquote>
<p>好啊，花火选择了再找一个。而且不给任何人带来伤害，与男主互相利用。这在<strong>出于寂寞的恋爱</strong>中，也属于比较好的，可以接受的一种了。</p>
<h2 id="约第3-7话的剧情"><a href="#约第3-7话的剧情" class="headerlink" title="约第3~7话的剧情"></a>约第3~7话的剧情</h2><p>剧情到这里，说的一直都是一个单相思的少女，虽然她跟相似境遇的男主拼凑了一对，但是在这个时间点，还真没有什么。<br>从大约第三话开始，剧情急转直下，走向深渊，直到第六话，沉入谷底。</p>
<h3 id="负面的东西开始越来越多了"><a href="#负面的东西开始越来越多了" class="headerlink" title="负面的东西开始越来越多了"></a>负面的东西开始越来越多了</h3><h4 id="《人渣的本愿》中唯一的人渣"><a href="#《人渣的本愿》中唯一的人渣" class="headerlink" title="《人渣的本愿》中唯一的人渣"></a>《人渣的本愿》中唯一的人渣</h4><p><img src="http://oonaavjvi.bkt.clouddn.com/rz032.jpg" alt="image"></p>
<blockquote>
<p>女老师单纯的想毁掉花火的恋爱，对男老师并无兴趣</p>
</blockquote>
<p>当然，能被外力毁掉的恋爱，内部很可能也有一些问题。比方说这里，男老师对花火本身就没什么意思。 但是女老师的人品有问题，这点是洗不干净的。</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz033.jpg" alt="image"></p>
<blockquote>
<p>在女老师的设计下，花火刚好撞上“大哥哥”向女老师表白的瞬间</p>
</blockquote>
<p>要说三观惊奇，女老师确实三观惊奇。</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz034.jpg" alt="image"></p>
<blockquote>
<p>花火找女老师当面对质的时候，女老师的表现</p>
</blockquote>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz035.jpg" alt="image"></p>
<blockquote>
<p>女老师的这番话，戳到了花火的痛处</p>
</blockquote>
<p>真爱型恋爱与寂寞型恋爱是有优劣之分的，这一点花火也清楚。 发生的事情就是发生了，不管过程再怎么无奈，花火和麦的关系在外人看来就是不清不楚的。这一点，在决定要开始这段关系的时候，就应该做好觉悟的。</p>
<h5 id="这里我们思考一个问题"><a href="#这里我们思考一个问题" class="headerlink" title="这里我们思考一个问题"></a>这里我们思考一个问题</h5><p>花火现在知道了女老师根本不喜欢男老师，为什么不找到“大哥哥”告一状？她把这件事情忍下来，憋在心里不是才比较奇怪吗？</p>
<p>其实，作者借了男主之口，回答过这个问题了。<br><img src="http://oonaavjvi.bkt.clouddn.com/rz036.jpg" alt="image"></p>
<blockquote>
<p>乃莉子到男主（麦）面前告了花火一状，麦的反应</p>
</blockquote>
<p>乃莉子这一状告的并没有挽回男主对她的青睐，相反，这其实还为她在男主心里减了分。试想，要是花火去“大哥哥”面前告了一状，然后剧情演变成主角们互相勾心斗角，那这部番真的没什么可看的了。</p>
<h6 id="这部番细节丰富，逻辑缜密。这样的剧情才有深度挖掘的价值。"><a href="#这部番细节丰富，逻辑缜密。这样的剧情才有深度挖掘的价值。" class="headerlink" title="这部番细节丰富，逻辑缜密。这样的剧情才有深度挖掘的价值。"></a>这部番细节丰富，逻辑缜密。这样的剧情才有深度挖掘的价值。</h6><p><img src="http://oonaavjvi.bkt.clouddn.com/rz037.jpg" alt="image"></p>
<p>这里没什么好解释的了，作者的就差在女老师脸上写上“婊子”两个字了。花火还没有见识过这种人，这打破了她的世界观。</p>
<h4 id="矛盾中挣扎的闺蜜"><a href="#矛盾中挣扎的闺蜜" class="headerlink" title="矛盾中挣扎的闺蜜"></a>矛盾中挣扎的闺蜜</h4><p>在笔者看来，其实女老师的所作所为，确实对花火的世界观构成震撼，使它摇摇欲坠。然而压死骆驼的最后一根稻草，其实是花火唯一的好朋友——小绘，的变故。</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz038.jpg" alt="image"></p>
<blockquote>
<p>小绘在花火家留宿，跟花火同床共枕的时候，终于没忍住，把真心话抖了出来</p>
</blockquote>
<p>而且这个地方有个细节，小绘已经看出来，花火对男主不是真心的。</p>
<h6 id="正是因为花火与麦的关系是出于寂寞的，才会有空子可钻。-笔者想，作者一定是想借此表达，出于寂寞恋爱的危险性。所以，这部动漫的主题是非常正面的。"><a href="#正是因为花火与麦的关系是出于寂寞的，才会有空子可钻。-笔者想，作者一定是想借此表达，出于寂寞恋爱的危险性。所以，这部动漫的主题是非常正面的。" class="headerlink" title="正是因为花火与麦的关系是出于寂寞的，才会有空子可钻。 笔者想，作者一定是想借此表达，出于寂寞恋爱的危险性。所以，这部动漫的主题是非常正面的。"></a>正是因为花火与麦的关系是<strong><em>出于寂寞</em></strong>的，才会有空子可钻。 笔者想，作者一定是想借此表达，出于寂寞恋爱的危险性。所以，这部动漫的主题是非常正面的。</h6><p><img src="http://oonaavjvi.bkt.clouddn.com/rz039.jpg" alt="image"></p>
<blockquote>
<p>小绘在一定能够程度上已经黑化了</p>
</blockquote>
<p>但是，谁又能说小绘对花火不是真心的呢？甚至，她的处境更加绝望，从一开始就要背负“同性恋”的名头，看不到希望。</p>
<h6 id="“喜欢”这种话，一旦说出口，就停不下来了。一旦把“喜欢”说出口，两个人关系的性质就将发生改变，可能再也做不成朋友了。所以说，表白是需要勇气的。"><a href="#“喜欢”这种话，一旦说出口，就停不下来了。一旦把“喜欢”说出口，两个人关系的性质就将发生改变，可能再也做不成朋友了。所以说，表白是需要勇气的。" class="headerlink" title="“喜欢”这种话，一旦说出口，就停不下来了。一旦把“喜欢”说出口，两个人关系的性质就将发生改变，可能再也做不成朋友了。所以说，表白是需要勇气的。"></a>“喜欢”这种话，一旦说出口，就停不下来了。一旦把“喜欢”说出口，两个人关系的性质就将发生改变，可能再也做不成朋友了。所以说，表白是需要<strong><em>勇气</em></strong>的。</h6><h5 id="我们来分析花火现在的心情"><a href="#我们来分析花火现在的心情" class="headerlink" title="我们来分析花火现在的心情"></a>我们来分析花火现在的心情</h5><p>她亲眼目睹了从小喜欢到大的“大哥哥”向别人表白，并且那个人还是个人渣。<br><img src="http://oonaavjvi.bkt.clouddn.com/rz041.jpg" alt="image"></p>
<blockquote>
<p>女老师特地向花火通报了这事</p>
</blockquote>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz042.jpg" alt="image"></p>
<blockquote>
<p>为了不把痛苦传递到麦身上，花火想自己忍下来</p>
</blockquote>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz043.jpg" alt="image"></p>
<blockquote>
<p>小绘是花火唯一的朋友</p>
</blockquote>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz044.jpg" alt="image"><br><img src="http://oonaavjvi.bkt.clouddn.com/rz051.jpg" alt="image"></p>
<blockquote>
<p>曾经</p>
</blockquote>
<p>如果这个时候，闺蜜能够成为花火的支柱，花火可能就能少走些弯路。然而实际情况是，闺蜜非但没有成为花火的靠山，却诱导着花火向更加黑暗的深渊走去。</p>
<h6 id="在这部动漫里，没有谁是为了别人活的，没有谁的感情更重要，谁的不那么重要，大家各有各的乐趣和痛苦，这点非常真实。"><a href="#在这部动漫里，没有谁是为了别人活的，没有谁的感情更重要，谁的不那么重要，大家各有各的乐趣和痛苦，这点非常真实。" class="headerlink" title="在这部动漫里，没有谁是为了别人活的，没有谁的感情更重要，谁的不那么重要，大家各有各的乐趣和痛苦，这点非常真实。"></a>在这部动漫里，没有谁是为了别人活的，没有谁的感情更重要，谁的不那么重要，大家各有各的乐趣和痛苦，这点非常真实。</h6><h5 id="我们在这里思考一个问题，可爱的女生怎么会缺少朋友呢？"><a href="#我们在这里思考一个问题，可爱的女生怎么会缺少朋友呢？" class="headerlink" title="我们在这里思考一个问题，可爱的女生怎么会缺少朋友呢？"></a>我们在这里思考一个问题，可爱的女生怎么会缺少朋友呢？</h5><h6 id="在现实生活中，长得很漂亮的女生，如果个性太张扬，或者跟男生关系太好可能会被女生孤立。-不过在这里，花火的女性朋友少显然不是这个原因。-花火出生在单亲家庭，父亲的离开给她的童年蒙上阴影，她的性格可能倾向于有些孤僻，尤其在青春期这段时间可能格外突出。-再者前面我们提到，花火与同学恋爱相谈的时候，并不能与他们产生共鸣。可能也就无意于与这些人交往吧。"><a href="#在现实生活中，长得很漂亮的女生，如果个性太张扬，或者跟男生关系太好可能会被女生孤立。-不过在这里，花火的女性朋友少显然不是这个原因。-花火出生在单亲家庭，父亲的离开给她的童年蒙上阴影，她的性格可能倾向于有些孤僻，尤其在青春期这段时间可能格外突出。-再者前面我们提到，花火与同学恋爱相谈的时候，并不能与他们产生共鸣。可能也就无意于与这些人交往吧。" class="headerlink" title="在现实生活中，长得很漂亮的女生，如果个性太张扬，或者跟男生关系太好可能会被女生孤立。 不过在这里，花火的女性朋友少显然不是这个原因。 花火出生在单亲家庭，父亲的离开给她的童年蒙上阴影，她的性格可能倾向于有些孤僻，尤其在青春期这段时间可能格外突出。 再者前面我们提到，花火与同学恋爱相谈的时候，并不能与他们产生共鸣。可能也就无意于与这些人交往吧。"></a>在现实生活中，长得很漂亮的女生，如果个性太张扬，或者跟男生关系太好可能会被女生孤立。 不过在这里，花火的女性朋友少显然不是这个原因。 花火出生在单亲家庭，父亲的离开给她的童年蒙上阴影，她的性格可能倾向于有些孤僻，尤其在青春期这段时间可能格外突出。 再者前面我们提到，花火与同学恋爱相谈的时候，并不能与他们产生共鸣。可能也就无意于与这些人交往吧。</h6><h3 id="花火的改变"><a href="#花火的改变" class="headerlink" title="花火的改变"></a>花火的改变</h3><h4 id="花火现在的心情"><a href="#花火现在的心情" class="headerlink" title="花火现在的心情"></a>花火现在的心情</h4><p><img src="http://oonaavjvi.bkt.clouddn.com/rz046.jpg" alt="image"></p>
<blockquote>
<p>逃离太阳系</p>
</blockquote>
<h6 id="在大多数小说，电影，漫画里，角色都是个性鲜明，且一成不变的。而在现实生活中，没有谁，是一成不变的。这部动画中，在经历一些变故之后，于是，花火变了。"><a href="#在大多数小说，电影，漫画里，角色都是个性鲜明，且一成不变的。而在现实生活中，没有谁，是一成不变的。这部动画中，在经历一些变故之后，于是，花火变了。" class="headerlink" title="在大多数小说，电影，漫画里，角色都是个性鲜明，且一成不变的。而在现实生活中，没有谁，是一成不变的。这部动画中，在经历一些变故之后，于是，花火变了。"></a>在大多数小说，电影，漫画里，角色都是个性鲜明，且一成不变的。而在现实生活中，没有谁，是一成不变的。这部动画中，在经历一些变故之后，于是，花火变了。</h6><p><img src="http://oonaavjvi.bkt.clouddn.com/rz045.jpg" alt="image"></p>
<blockquote>
<p>花火这里开始有以女老师为目标的想法</p>
</blockquote>
<p>花火被女老师夺走了太多，痛苦不堪，因为女老师是个婊子。那如果花火放弃她的原则，是不是就能把失去的夺回来了呢？<br>这里，花火可能开始考虑这样的问题。</p>
<h6 id="人是社会性动物，环境对人的影响力是巨大的。-对于一个孩子来说，除了家里的长辈之外，影响力最大的就是学校里的老师和同学了。在成长的过程中，或多或少的会对这些人进行模仿，依样学样。-人在改变的过程中，有时候是分不清对错的。"><a href="#人是社会性动物，环境对人的影响力是巨大的。-对于一个孩子来说，除了家里的长辈之外，影响力最大的就是学校里的老师和同学了。在成长的过程中，或多或少的会对这些人进行模仿，依样学样。-人在改变的过程中，有时候是分不清对错的。" class="headerlink" title="人是社会性动物，环境对人的影响力是巨大的。 对于一个孩子来说，除了家里的长辈之外，影响力最大的就是学校里的老师和同学了。在成长的过程中，或多或少的会对这些人进行模仿，依样学样。 人在改变的过程中，有时候是分不清对错的。"></a>人是社会性动物，环境对人的影响力是巨大的。 对于一个孩子来说，除了家里的长辈之外，影响力最大的就是学校里的老师和同学了。在成长的过程中，或多或少的会对这些人进行模仿，依样学样。 人在改变的过程中，有时候是分不清对错的。</h6><h4 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h4><p><img src="http://oonaavjvi.bkt.clouddn.com/rz050.jpg" alt="image"></p>
<blockquote>
<p>花火要黑化了</p>
</blockquote>
<p>很多人看到这里都要弃了吧，腹黑的作者没有给女主及时送上一个救世主，一个能够影响到她，并且给她带来正能量的人。男主和“大哥哥”心里都是女老师，而闺蜜为了能够把花火留在身边不择手段。</p>
<p>不过这才是这部动画的主题，<strong>人是不能够被打倒的，就算是花火这样的弱不禁风的女孩，也要一步一脚印从深渊和泥潭中，靠自己的力量走出来。</strong></p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz047.jpg" alt="image"></p>
<blockquote>
<p>花火为“大哥哥”痛过了，这里便理解了闺蜜的痛，心生同情，放任与闺蜜非正常的关系发展了下去</p>
</blockquote>
<p>出于<strong><em>同情</em></strong>的恋爱关系，对对方也是一种<strong>伤害</strong>。这里不得不说的一点是，花火除了<strong><em>同情</em></strong>之外，为了弥补失恋的寂寞产生的空虚，实际上也夹带的利用了闺蜜的私心。</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz048.jpg" alt="image"></p>
<blockquote>
<p>花火跟麦提出正式交往的时候，脑子里却想着以此跟女老师较劲</p>
</blockquote>
<p>这里mark一下，这是花火对不起麦的地方。</p>
<h4 id="愈演愈烈"><a href="#愈演愈烈" class="headerlink" title="愈演愈烈"></a>愈演愈烈</h4><p><img src="http://oonaavjvi.bkt.clouddn.com/rz055.jpg" alt="image"></p>
<blockquote>
<p>花火无法战胜由失恋带来的寂寞感</p>
</blockquote>
<h6 id="有实验证明，失恋能够带来切实的痛感，能够在很短的时间打倒一个强大的人，这实在令人感到不可思议。"><a href="#有实验证明，失恋能够带来切实的痛感，能够在很短的时间打倒一个强大的人，这实在令人感到不可思议。" class="headerlink" title="有实验证明，失恋能够带来切实的痛感，能够在很短的时间打倒一个强大的人，这实在令人感到不可思议。"></a>有实验证明，失恋能够带来切实的痛感，能够在很短的时间打倒一个强大的人，这实在令人感到不可思议。</h6><p><img src="http://oonaavjvi.bkt.clouddn.com/QQ%E6%88%AA%E5%9B%BE20170504133406.jpg" alt="image"></p>
<blockquote>
<p>花火可能已经是自暴自弃了，幸运的是男主并没有乘人之危</p>
</blockquote>
<h6 id="失恋会让人产生一种类似于溺水的感觉，每次呼吸都有凛冽的海水灌进肺里，所有飘过的东西，哪怕是一片树叶都要紧紧的抱住，汲取他的温暖。"><a href="#失恋会让人产生一种类似于溺水的感觉，每次呼吸都有凛冽的海水灌进肺里，所有飘过的东西，哪怕是一片树叶都要紧紧的抱住，汲取他的温暖。" class="headerlink" title="失恋会让人产生一种类似于溺水的感觉，每次呼吸都有凛冽的海水灌进肺里，所有飘过的东西，哪怕是一片树叶都要紧紧的抱住，汲取他的温暖。"></a>失恋会让人产生一种类似于<strong><em>溺水</em></strong>的感觉，每次呼吸都有凛冽的海水灌进肺里，所有飘过的东西，哪怕是一片树叶都要紧紧的抱住，汲取他的温暖。</h6><p><img src="http://oonaavjvi.bkt.clouddn.com/rz049.jpg" alt="image"></p>
<blockquote>
<p>花火开始戴耳环了</p>
</blockquote>
<p>耳环有象征意义，花火想要<strong><em>变坏</em></strong>。第一次戴耳环跟完全不喜欢的人渣男约会，这一话是这部动漫最黑暗的部分。</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz054.jpg" alt="image"></p>
<blockquote>
<p>花火开始考虑一些本来没必要考虑的东西</p>
</blockquote>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz056.jpg" alt="image"></p>
<blockquote>
<p>花火也弄明白了一些本不用弄明白的东西</p>
</blockquote>
<p>这里花火明白了，她与女老师决定性的差距。女老师在情场是一个“愉快犯”，她以被男人追捧、被女人嫉妒为乐趣。而花火正是为了跟女老师较劲，取得某种心理平衡，挽回自尊，才踏入这片<strong><em>沼泽</em></strong>。所以她一开始就输了。</p>
<h6 id="经历了失恋之后，尤其对方人品很差的情况，很多人会产生“我要变坏”这样的想法，拒绝付出，成为感情的剥削者。-这种情感是幼稚的，会把负面的感情和价值观传递到无辜的人身上。最终自己也不会获得真心，实在是害人害己。"><a href="#经历了失恋之后，尤其对方人品很差的情况，很多人会产生“我要变坏”这样的想法，拒绝付出，成为感情的剥削者。-这种情感是幼稚的，会把负面的感情和价值观传递到无辜的人身上。最终自己也不会获得真心，实在是害人害己。" class="headerlink" title="经历了失恋之后，尤其对方人品很差的情况，很多人会产生“我要变坏”这样的想法，拒绝付出，成为感情的剥削者。 这种情感是幼稚的，会把负面的感情和价值观传递到无辜的人身上。最终自己也不会获得真心，实在是害人害己。"></a>经历了失恋之后，尤其对方人品很差的情况，很多人会产生“我要变坏”这样的想法，拒绝付出，成为感情的剥削者。 这种情感是幼稚的，会把负面的感情和价值观传递到无辜的人身上。最终自己也不会获得真心，实在是害人害己。</h6><h5 id="那，花火下一步，是不是要把自己变成一个情场愉快犯呢？"><a href="#那，花火下一步，是不是要把自己变成一个情场愉快犯呢？" class="headerlink" title="那，花火下一步，是不是要把自己变成一个情场愉快犯呢？"></a>那，花火下一步，是不是要把自己变成一个<strong><em>情场愉快犯</em></strong>呢？</h5><p><img src="http://oonaavjvi.bkt.clouddn.com/rz057.jpg" alt="image"></p>
<blockquote>
<p>花火开始分析自己的心态和处境</p>
</blockquote>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz119.jpg" alt="image"></p>
<blockquote>
<p>小绘曾是花火唯一的朋友</p>
</blockquote>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz058.jpg" alt="image"></p>
<blockquote>
<p>花火意识到自己的软弱</p>
</blockquote>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz059.jpg" alt="image"></p>
<blockquote>
<p>花火终于明白了自己变得软弱的理由</p>
</blockquote>
<h6 id="恋爱，就是要把内心最柔软的部分暴露出来。有的心，浇灌的是爱的养料，被小心呵护着成长。有的心，浇灌的是锋利的匕首，被无情的践踏。"><a href="#恋爱，就是要把内心最柔软的部分暴露出来。有的心，浇灌的是爱的养料，被小心呵护着成长。有的心，浇灌的是锋利的匕首，被无情的践踏。" class="headerlink" title="恋爱，就是要把内心最柔软的部分暴露出来。有的心，浇灌的是爱的养料，被小心呵护着成长。有的心，浇灌的是锋利的匕首，被无情的践踏。"></a>恋爱，就是要把内心最柔软的部分暴露出来。有的心，浇灌的是爱的养料，被小心呵护着成长。有的心，浇灌的是锋利的匕首，被无情的践踏。</h6><p>花火内心最柔软的部分，就是对“大哥哥”的恋爱了。而这不仅没有得到回馈，还被女老师践踏。因此，她变得软弱，变得需要依靠一切可以依靠的东西。</p>
<p>我们多希望，花火成为住在城堡里，阁楼上的公主。有一个深爱着她的王子，无私的为她奉献。让他远离世俗的尘土，让她不食人间烟火。在她要倒下的时候，把她抱起来。 遗憾的是，腹黑的作者，就是要花火，<strong>自己站起来</strong>。</p>
<h4 id="我们好好聊一聊。"><a href="#我们好好聊一聊。" class="headerlink" title="我们好好聊一聊。"></a>我们好好聊一聊。</h4><p>笔者要强调一点！观众朋友们很多时候有一个误区，认为这部动画的目的就是告诉你，不要当好人，做人的终极目的就是成为人渣，人渣心理斗争也不容易，人渣是可以被原谅的。然而，对于花火的黑化，作者的态度<br><img src="http://oonaavjvi.bkt.clouddn.com/rz052.jpg" alt="image"></p>
<blockquote>
<p>花火内心的正义感化身成她原来的样子把她数落了一番</p>
</blockquote>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz053.jpg" alt="image"></p>
<blockquote>
<p>作者把黑化的花火的心态描绘成癫狂的状态</p>
<h2 id="是反对的！"><a href="#是反对的！" class="headerlink" title="是反对的！"></a>是反对的！</h2></blockquote>
<p>如果，这部动漫到这里就结束的话，跟国产“出国，打胎，白血病”那种的<strong><em>三观不正</em></strong>青春剧也就没有区别了。<br>但是笔者要强调的是，这部动漫，<strong>不是</strong>教育观众看完了去当一个人渣。到这里，这部动漫才进行了一半，一些事都还有挽回的余地，并且，剧情也开始要转向了。</p>
<h2 id="8-12话的剧情"><a href="#8-12话的剧情" class="headerlink" title="8~12话的剧情"></a>8~12话的剧情</h2><h3 id="现在回头还来得及吗？"><a href="#现在回头还来得及吗？" class="headerlink" title="现在回头还来得及吗？"></a>现在回头还来得及吗？</h3><p>虽然说女老师和黑化后的闺蜜，对花火产生了不好的影响，使她走上错误的道路。但是归根结底，花火改变的最根本的原因还是，<strong>失恋</strong>。</p>
<h6 id="就像表白需要勇气一样，承受失恋也是需要勇气的。只是没有勇气而已，如何就变成罪恶了？-把一个人的温暖转移到另一个人的身上，是不可取的，是一种逃避，是一种懦弱，最终只会重蹈覆辙。正确的做法是，直面失恋，正视失恋，向前走下去。"><a href="#就像表白需要勇气一样，承受失恋也是需要勇气的。只是没有勇气而已，如何就变成罪恶了？-把一个人的温暖转移到另一个人的身上，是不可取的，是一种逃避，是一种懦弱，最终只会重蹈覆辙。正确的做法是，直面失恋，正视失恋，向前走下去。" class="headerlink" title="就像表白需要勇气一样，承受失恋也是需要勇气的。只是没有勇气而已，如何就变成罪恶了？ 把一个人的温暖转移到另一个人的身上，是不可取的，是一种逃避，是一种懦弱，最终只会重蹈覆辙。正确的做法是，直面失恋，正视失恋，向前走下去。"></a>就像表白需要<strong>勇气</strong>一样，承受失恋也是需要<strong>勇气</strong>的。只是没有<strong><em>勇气</em></strong>而已，如何就变成罪恶了？ <strong><em>把一个人的温暖转移到另一个人的身上</em></strong>，是不可取的，是一种逃避，是一种懦弱，最终只会重蹈覆辙。正确的做法是，直面失恋，正视失恋，向前走下去。</h6><p>花火向男主提出交往确实是出于利用的目的，但是男主也并未为之付出过什么。不过花火的幼稚和私心，确实对闺蜜小绘造成了伤害，不过小绘自己也要负责任的。花火跟被女老师玩腻的男人约过两次会，不过两人之间并没有什么实质的关系，还让花火看清了与女老师之间的差距。</p>
<p>花火面前的路有两条，要么任凭被自己的懦弱淹没，像女老师一样，让虚伪的追捧和嫉妒充实自己。要么…</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz060.jpg" alt="image"></p>
<blockquote>
<p>花火决定鼓起勇气</p>
</blockquote>
<h6 id="人跌倒的时候可能有很多理由，但是爬起来却不需要太多理由。"><a href="#人跌倒的时候可能有很多理由，但是爬起来却不需要太多理由。" class="headerlink" title="人跌倒的时候可能有很多理由，但是爬起来却不需要太多理由。"></a>人跌倒的时候可能有很多理由，但是爬起来却不需要太多理由。</h6><h3 id="小小的正能量们"><a href="#小小的正能量们" class="headerlink" title="小小的正能量们"></a>小小的正能量们</h3><h4 id="最可爱的乃莉子"><a href="#最可爱的乃莉子" class="headerlink" title="最可爱的乃莉子"></a>最可爱的乃莉子</h4><p><img src="http://oonaavjvi.bkt.clouddn.com/rz061.jpg" alt="image"></p>
<blockquote>
<p>乃莉子从一开始就用拙劣的手法，三番五次想拆散男主和女主</p>
</blockquote>
<p>最开始，乃莉子这个角色给人感觉很像一般后宫漫中的龙套角色。但是作者在后面却浓墨重彩的描写了她。</p>
<h6 id="普通的后宫动漫中，经常有后宫们互相争风吃醋的桥段。实际上，就算有这种事，也是伴随着剧烈的痛苦的。这点非常不真实。"><a href="#普通的后宫动漫中，经常有后宫们互相争风吃醋的桥段。实际上，就算有这种事，也是伴随着剧烈的痛苦的。这点非常不真实。" class="headerlink" title="普通的后宫动漫中，经常有后宫们互相争风吃醋的桥段。实际上，就算有这种事，也是伴随着剧烈的痛苦的。这点非常不真实。"></a>普通的后宫动漫中，经常有后宫们互相争风吃醋的桥段。实际上，就算有这种事，也是伴随着剧烈的痛苦的。这点非常不真实。</h6><p><img src="http://oonaavjvi.bkt.clouddn.com/rz062.jpg" alt="image"></p>
<blockquote>
<p>虽说花火并不是出于恶意，但是却做了跟女老师一样的事</p>
</blockquote>
<p>花火听了这些却无法反驳，这就好像在说她和大哥哥一样。</p>
<h5 id="这里要分清楚一点"><a href="#这里要分清楚一点" class="headerlink" title="这里要分清楚一点"></a>这里要分清楚一点</h5><p>虽然男主和女主之间是<strong><em>出于寂寞的恋爱</em></strong>，作者谨慎反对这种关系。但是男主不喜欢乃莉子并不是花火的错。所以，花火对乃莉子并不需要道歉。</p>
<p>女老师的情况就不一样了，女老师为了让花火痛苦，甚至委屈自己去勾搭一个一点都不喜欢，甚至有些厌倦的男人。虽说男老师不喜欢花火不是女老师的错，但是女老师这种损己不利人的行为实在可恶。</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz063.jpg" alt="image"></p>
<blockquote>
<p>乃莉子嘲讽花火今天是一个人，并且表明自己对感情会坚持到最后</p>
</blockquote>
<p>乃莉子是个好女孩，我们希望世界上能够多一些这样的人。</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz065.jpg" alt="image"></p>
<blockquote>
<p>原来乃莉子也为自己的感情寻找过替代品</p>
</blockquote>
<p>没关系，知错就改还是好孩子。</p>
<p>发生在自己身上的事，才能够刻骨铭心，才能转换成经验和教训，<strong>让自己成为更好的人</strong>。乃莉子虽然年纪小，爱情观却很正直成熟。不过啊，这句对不起别对空气说啊，要对该说的人说。</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz064.jpg" alt="image"></p>
<blockquote>
<p>被稀里糊涂的骂了一通，花火的表情却好像想通了什么</p>
</blockquote>
<p>原来，<strong>失恋</strong>让花火脆弱到，需要依靠身边的一切。想都没想过可以<strong>靠自己走下去</strong>。</p>
<h5 id="看过来看过来，上面这里便是这部动漫的主题了"><a href="#看过来看过来，上面这里便是这部动漫的主题了" class="headerlink" title="==看过来看过来，上面这里便是这部动漫的主题了=="></a>==看过来看过来，上面这里便是这部动漫的主题了==</h5><h4 id="长头发的表弟"><a href="#长头发的表弟" class="headerlink" title="长头发的表弟"></a>长头发的表弟</h4><p><img src="http://oonaavjvi.bkt.clouddn.com/rz066.jpg" alt="image"></p>
<blockquote>
<p>曾经追过小绘的表弟出现了</p>
</blockquote>
<p>与乃莉子相比，表弟的戏份就显得比较仓促了。可能也是由于篇幅的问题吧，我们对表弟没有一个饱满的认识。不过他的出现确实是为了推动剧情向正面发展而设计的。</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz067.jpg" alt="image"></p>
<blockquote>
<p>表弟由于过于直接被嫌弃了</p>
</blockquote>
<p>可以看出，表弟是一个很简单的人。喜欢一个人表现的非常直接，对于喜欢的人的恋爱对象，很坦率的也喜欢。</p>
<p>这里可能大家要问了，为什么花火不能坦率地喜欢上女老师呢？抛开性别血缘不谈（不谈不是说不重要，这部作品是一个本格的伦理剧，作者只是把这些元素当做噱头来卖，作者并没有着太多笔墨在这上面），如果女老师是一个足够优秀的人，并且也足够的爱男老师，这对于花火来说，也更好接受吧。</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz068.jpg" alt="image"></p>
<blockquote>
<p>表弟说的太直白了，这下小绘就没有犹豫的空间了</p>
</blockquote>
<p>小绘与花火的失恋旅行，笔者也相信小绘是以<strong>结束这段畸形的关系</strong>为原因约的花火，但是她真的有这么坚强吗？ 这里，表弟的出现就显得非常有必要了。</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz070.jpg" alt="image"></p>
<blockquote>
<p>表弟对花火也是直奔主题</p>
</blockquote>
<h6 id="这部动漫是非常正面的，表弟不过是借作者口，把话说了出来而已，笔者的观点并不是空口无凭的。"><a href="#这部动漫是非常正面的，表弟不过是借作者口，把话说了出来而已，笔者的观点并不是空口无凭的。" class="headerlink" title="这部动漫是非常正面的，表弟不过是借作者口，把话说了出来而已，笔者的观点并不是空口无凭的。"></a>这部动漫是非常正面的，表弟不过是借作者口，把话说了出来而已，笔者的观点并不是空口无凭的。</h6><h3 id="做个了断吧"><a href="#做个了断吧" class="headerlink" title="做个了断吧"></a>做个了断吧</h3><h4 id="与小绘和解"><a href="#与小绘和解" class="headerlink" title="与小绘和解"></a>与小绘和解</h4><p><img src="http://oonaavjvi.bkt.clouddn.com/rz071.jpg" alt="image"></p>
<blockquote>
<p>小绘想要单方面结束关系</p>
</blockquote>
<p>只有这样的话，事情也算解决了，但是最后，只有小绘一个人承受了痛苦，而且她与花火可能再也做不成朋友了。</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz072.jpg" alt="image"></p>
<blockquote>
<p>花火不想失去这个朋友，但是小绘听后歇斯底里般的爆发了</p>
</blockquote>
<p>花火这个要求其实非常任性，不信你可以想一想，身边分手后还能当朋友的，有几个。</p>
<p>不过呢，小绘这番话也是十分任性的。花火是没有义务设身处地为小绘着想的。</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz073.jpg" alt="image"></p>
<blockquote>
<p>花火深刻认识到了自己一时的懦弱，对小绘造成了多大的伤害</p>
</blockquote>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz074.jpg" alt="image"></p>
<blockquote>
<p>两个人意识流般的对话，是十分美好的结局</p>
</blockquote>
<p>可能因为小绘是女生吧，分手时还能有这般待遇。这是一段刻骨铭心的感情结束后，最好的结果。</p>
<p>承受失恋需要多大<strong>勇气</strong>，忘掉一个人有多难。小绘自己的话，应该做不到吧。花火即使如此也愿意等，不再是之前的肉体上的慰藉，这次她要跟小绘再一次成为<strong>朋友</strong>。</p>
<p>相信花火的温柔，能够成为小绘的<strong>勇气</strong>。</p>
<h4 id="麦与乃莉子"><a href="#麦与乃莉子" class="headerlink" title="麦与乃莉子"></a>麦与乃莉子</h4><p><img src="http://oonaavjvi.bkt.clouddn.com/rz076.jpg" alt="image"></p>
<blockquote>
<p>乃莉子这次明知道麦在敷衍自己，还是答应跟她约会了</p>
</blockquote>
<p>抱歉一下让时光倒流了，乃莉子是一个世界观很正直的人。原本，她是一个一定要等到麦回心转意，才会跟他正大光明的在一起的女孩。但是这一次，乃莉子还是被爱情打败了。<br><img src="http://oonaavjvi.bkt.clouddn.com/rz077.jpg" alt="image"></p>
<blockquote>
<p>乃莉子明知道这样做是不对的</p>
</blockquote>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz078.jpg" alt="image"></p>
<blockquote>
<p>乃莉子在约会的过程中下了决心，让这次约会成为最后并且永远珍藏的回忆</p>
</blockquote>
<p><em>反正都是最后一次了，尽量制造一些美好的回忆吧</em>。乃莉子带着这样的想法，打扮得漂漂亮亮的开始了与麦最后的约会。<br>乃莉子是如此的喜欢麦，以至于觉得这一生都不会再爱了。但是即使如此，她仍然下决心为这段无法实现的恋情画上句点。</p>
<p>但是…<br><img src="http://oonaavjvi.bkt.clouddn.com/rz079.jpg" alt="image"></p>
<blockquote>
<p>她的眼泪决堤般的流了出来</p>
</blockquote>
<p>麦发现，平时一向死缠烂打的青梅竹马追求者，现在却如此无助，这使他产生了一种怜悯之情。男人是一种保护欲过剩的动物。</p>
<p>在乃莉子与麦之间，没有一个表弟式的人物，迫使她客观看待自己。乃莉子的三观很正，但并不代表她有那么强大。 乃莉子认为麦之后，再也不会遇到真心喜欢的人了，这点很难成为现实，但是乃莉子在这个时间点就是落入了这个怪圈。 她试图让自己接受<strong><em>一辈子不需要真爱</em></strong>，但是没能成功。</p>
<h6 id="征服一个男人，比起展示各种才能，逻辑思维，或者对他有多体贴。可能几滴眼泪就能解决问题。-尤其是平时性格很坚强，独立的女生，在喜欢的男生面前如果能表现出，脆弱，需要帮助的一面，可能起到事半功倍的效果。"><a href="#征服一个男人，比起展示各种才能，逻辑思维，或者对他有多体贴。可能几滴眼泪就能解决问题。-尤其是平时性格很坚强，独立的女生，在喜欢的男生面前如果能表现出，脆弱，需要帮助的一面，可能起到事半功倍的效果。" class="headerlink" title="征服一个男人，比起展示各种才能，逻辑思维，或者对他有多体贴。可能几滴眼泪就能解决问题。 尤其是平时性格很坚强，独立的女生，在喜欢的男生面前如果能表现出，脆弱，需要帮助的一面，可能起到事半功倍的效果。"></a>征服一个男人，比起展示各种才能，逻辑思维，或者对他有多体贴。可能几滴眼泪就能解决问题。 尤其是平时性格很坚强，独立的女生，在喜欢的男生面前如果能表现出，脆弱，需要帮助的一面，可能起到事半功倍的效果。</h6><p><img src="http://oonaavjvi.bkt.clouddn.com/rz080.jpg" alt="image"></p>
<blockquote>
<p>乃莉子懂得自己可以用眼泪骗取同情，不过这部动漫中她还是第一次流眼泪。</p>
</blockquote>
<p>乃莉子决定为了触碰最喜欢的人，而放弃原则</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz082.jpg" alt="image"></p>
<blockquote>
<p>由于麦最终守住了底线，乃莉子也看开了</p>
</blockquote>
<p>这点前面提到过，麦终究不是人渣。之所以会有今天，是因为乃莉子没有自己想象的强大。<br>鼓起勇气很艰难，接受事实很痛苦。只要人的目标是走出来，那么过程矫情点，有反复，依然是可以容忍的。</p>
<h5 id="—乃莉子麦end—"><a href="#—乃莉子麦end—" class="headerlink" title="—乃莉子麦end—"></a>—乃莉子麦end—</h5><h4 id="麦与花火的新的约定"><a href="#麦与花火的新的约定" class="headerlink" title="麦与花火的新的约定"></a>麦与花火的新的约定</h4><p><img src="http://oonaavjvi.bkt.clouddn.com/rz083.jpg" alt="image"></p>
<blockquote>
<p>暑假的尾声，两人身着白衣，在山丘上交换了约定</p>
</blockquote>
<p>这个场景是非常写意的，这一话在《人渣的本愿》中，也是非常关键的转折。花火在这一话里鼓起了勇气，麦在这一话里守住了底线。没看到这一话就弃了的人是十分可惜的。</p>
<h4 id="花火与男老师（大哥哥）"><a href="#花火与男老师（大哥哥）" class="headerlink" title="花火与男老师（大哥哥）"></a>花火与男老师（大哥哥）</h4><p><img src="http://oonaavjvi.bkt.clouddn.com/rz085.jpg" alt="image"></p>
<blockquote>
<p>花火终于对大哥哥表白了</p>
</blockquote>
<p>花火这里连用了一串排比，表达对大哥哥深深的恋爱。</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz084.jpg" alt="image"></p>
<blockquote>
<p>大哥哥轻轻的抱住了花火，和花火一起哭得像个孩子。花火多年的单相思结束了</p>
</blockquote>
<p>花火或许早就该这么做，她花了这么长的时间才鼓起表白的<strong>勇气</strong>。之所以花火明明知道大哥哥不喜欢她，还要最后表一个白，是因为她需要为自己多年的单相思做一个了断。好让自己跟麦有一个全新的开始。</p>
<p>大哥哥（男老师）真是一个温柔的人。即使如此他也愿意陪同花火走完这一程。</p>
<h5 id="—花火大哥哥End—"><a href="#—花火大哥哥End—" class="headerlink" title="—花火大哥哥End—"></a>—花火大哥哥End—</h5><h4 id="麦与女老师"><a href="#麦与女老师" class="headerlink" title="麦与女老师"></a>麦与女老师</h4><p><img src="http://oonaavjvi.bkt.clouddn.com/rz086.jpg" alt="image"></p>
<blockquote>
<p>前面提到过，男主知道女老师喜欢乱搞，他想改变她。</p>
</blockquote>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz087.jpg" alt="image"></p>
<blockquote>
<p>麦对花火爽约了</p>
</blockquote>
<p>不过这一次与之前不同，花火已经鼓起过勇气了。</p>
<h5 id="为什么会这样呢？"><a href="#为什么会这样呢？" class="headerlink" title="为什么会这样呢？"></a>为什么会这样呢？</h5><p><img src="http://oonaavjvi.bkt.clouddn.com/rz089.jpg" alt="image"></p>
<blockquote>
<p>麦读懂了女老师的寂寞</p>
</blockquote>
<p>对于女老师的心态，作者给出了这样的解释。<strong><em>她是幼稚而孤独的人</em></strong>。笔者想这并不能成为她，玩弄别人感情的理由，但是这里，麦已经深深的喜欢上她了。 在知道女老师的幼稚与寂寞之后，却越发的觉得可爱可怜。希望成为那个，能够使女老师为之改变的人。</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz088.jpg" alt="image"></p>
<blockquote>
<p>麦知道，女老师有一天会变好，他希望成为改变女老师的人。</p>
</blockquote>
<p>然而问题就出在这里，在任何情况下，我们都没有<strong>权力</strong>，和<strong>能力</strong>去改变另一个人。<br>麦的想法是对的，但是做法太幼稚了。</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz090.jpg" alt="image"></p>
<blockquote>
<p>麦是幼稚的，但是他发现，女老师被一个不是自己的男人改变了。他意识到那个人才是女老师命中注定的。</p>
</blockquote>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz091.jpg" alt="image"></p>
<blockquote>
<p>麦发现，改变之后的女老师，如此美丽。</p>
</blockquote>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz093.jpg" alt="image"></p>
<blockquote>
<p>干脆利落，麦放弃了</p>
</blockquote>
<p>一段美好的感情，我们总是不愿意破坏的。女老师的未婚夫该是一个多么美好的人，是他让女老师变成了一个更好的人。而这是麦做梦都想做到的事，但是麦又有什么资格插手这段感情呢？</p>
<p>与之相比起来，命运对花火是不公平的。这很无奈。</p>
<h5 id="—麦与女老师end—"><a href="#—麦与女老师end—" class="headerlink" title="—麦与女老师end—"></a>—麦与女老师end—</h5><h3 id="尾声"><a href="#尾声" class="headerlink" title="尾声"></a>尾声</h3><h4 id="男老师与女老师"><a href="#男老师与女老师" class="headerlink" title="男老师与女老师"></a>男老师与女老师</h4><p>前面提到了，因为女老师像男老师死去的妈妈，所以男老师对女老师一见钟情。但是。。</p>
<h5 id="女老师真的对男老师动心了吗？"><a href="#女老师真的对男老师动心了吗？" class="headerlink" title="女老师真的对男老师动心了吗？"></a>女老师真的对男老师动心了吗？</h5><p><img src="http://oonaavjvi.bkt.clouddn.com/rz095.jpg" alt="image"></p>
<blockquote>
<p>女老师一开始觉得男老师很无聊</p>
</blockquote>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz094.jpg" alt="image"></p>
<blockquote>
<p>后来女老师越发觉得男老师很奇怪</p>
</blockquote>
<p><strong>还真动心了</strong>！女老师一开始，根本不喜欢男老师，甚至还有点厌恶，她就是为了让花火痛苦才勾搭的男老师。但是男老师，用耐心和善意逐渐征服了他。</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz096.jpg" alt="image"></p>
<blockquote>
<p>被以前玩过的男生当面拆穿，女老师决定向男老师坦白，然后断掉跟他的关系</p>
</blockquote>
<h6 id="这里有一个细节，女主能清楚的认识自己的所作所为。并且觉得如果被人知道，就不会被喜欢了。这或许说明她的内心还有基本的道德标准概念。"><a href="#这里有一个细节，女主能清楚的认识自己的所作所为。并且觉得如果被人知道，就不会被喜欢了。这或许说明她的内心还有基本的道德标准概念。" class="headerlink" title="这里有一个细节，女主能清楚的认识自己的所作所为。并且觉得如果被人知道，就不会被喜欢了。这或许说明她的内心还有基本的道德标准概念。"></a>这里有一个细节，女主能清楚的认识自己的所作所为。并且觉得如果被人知道，就不会被喜欢了。这或许说明她的内心还有基本的道德标准概念。</h6><p>然而。。</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz097.jpg" alt="image"></p>
<blockquote>
<p>男老师毫不犹豫的接纳了她</p>
</blockquote>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz098.jpg" alt="image"></p>
<blockquote>
<p>这是怎样的精神</p>
</blockquote>
<p>笔者看到这个地方的时候也十分惊讶，男老师的行为着实令人跌破眼镜。但是，我们没有资格去judge别人的感情，在这里，男老师就是有这么喜欢女老师，以至于可以接受女老师经常乱搞的过去，和可以出轨的未来。</p>
<p>然后，喜欢一个人，就要喜欢她/他的一切。不要批评他/她的过去，因为对方也无力挽回；不要要求她/他改变，我们没有那个资格。</p>
<h6 id="对于死宅来说，什么“萝莉”，“御姐”，“正太”之类的东西。本身就带有把人参数化的意思，不应该带到三次元来。喜欢一个人，喜欢的是这个人本身，而不应该是各种“参数”的堆砌。"><a href="#对于死宅来说，什么“萝莉”，“御姐”，“正太”之类的东西。本身就带有把人参数化的意思，不应该带到三次元来。喜欢一个人，喜欢的是这个人本身，而不应该是各种“参数”的堆砌。" class="headerlink" title="对于死宅来说，什么“萝莉”，“御姐”，“正太”之类的东西。本身就带有把人参数化的意思，不应该带到三次元来。喜欢一个人，喜欢的是这个人本身，而不应该是各种“参数”的堆砌。"></a>对于死宅来说，什么“萝莉”，“御姐”，“正太”之类的东西。本身就带有把人<strong><em>参数化</em></strong>的意思，不应该带到三次元来。喜欢一个人，喜欢的是这个人本身，而不应该是各种“参数”的堆砌。</h6><p>这里，女老师虽然嘴上说着“我会到处出轨”这样的话，我们可以相信，她的良心已经不允许自己这么做了。</p>
<h5 id="关于改变一个人"><a href="#关于改变一个人" class="headerlink" title="关于改变一个人"></a>关于改变一个人</h5><p>人是没有资格去改变别人的，但是，<strong>爱情</strong>着实可以<strong>改变</strong>一个人。<br><img src="http://oonaavjvi.bkt.clouddn.com/rz099.jpg" alt="image"></p>
<blockquote>
<p>这番话花火并没有说过，而是在男主（麦）的脑海中，托花火之口说出的</p>
</blockquote>
<p>男主最终也意识到，到底应该怎么做。男主觉得女老师乱搞是不对的，想要改变她，这是好事。<br>但是，仅仅是几句说教就能改变一个人的吗？</p>
<p>正确的做法是，让自己变成一个更好的人，用自己的行为，为对方树立典范。</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz100.jpg" alt="image"></p>
<blockquote>
<p>男老师接纳女老师的一切</p>
</blockquote>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz101.jpg" alt="image"></p>
<blockquote>
<p>女老师被男老师不图回报的善意所震撼</p>
</blockquote>
<p>这么一来，麦和男老师就高下立判了。麦是一个还不错的小伙子，但是与男老师相比，尚且稚嫩。麦无力给女老师带来灵魂上的冲击，也就永远无法影响到她，而被女老师主导着节奏，越来越迷茫。他输的应该是心服口服的。</p>
<h6 id="很多人感情生活进行的不顺利，就把罪责全部推到对方身上。试问，你有像男老师一样对付出的感情不索取任何回报的觉悟吗？-不明白这一点，而拼命的付出，把好意强加到对方身上。很多时候，被追求方提出分手的原因都是，感觉还不清追求方的好意。"><a href="#很多人感情生活进行的不顺利，就把罪责全部推到对方身上。试问，你有像男老师一样对付出的感情不索取任何回报的觉悟吗？-不明白这一点，而拼命的付出，把好意强加到对方身上。很多时候，被追求方提出分手的原因都是，感觉还不清追求方的好意。" class="headerlink" title="很多人感情生活进行的不顺利，就把罪责全部推到对方身上。试问，你有像男老师一样对付出的感情不索取任何回报的觉悟吗？ 不明白这一点，而拼命的付出，把好意强加到对方身上。很多时候，被追求方提出分手的原因都是，感觉还不清追求方的好意。"></a>很多人感情生活进行的不顺利，就把罪责全部推到对方身上。试问，你有像男老师一样对付出的感情不索取任何回报的觉悟吗？ 不明白这一点，而拼命的付出，把好意强加到对方身上。很多时候，被追求方提出分手的原因都是，<strong><em>感觉还不清追求方的好意</em></strong>。</h6><h4 id="我们来清算一下花火与麦的关系"><a href="#我们来清算一下花火与麦的关系" class="headerlink" title="我们来清算一下花火与麦的关系"></a>我们来清算一下花火与麦的关系</h4><p><img src="http://oonaavjvi.bkt.clouddn.com/rz103.jpg" alt="image"></p>
<blockquote>
<p>花火发现自己对麦已经产生了新的感情</p>
</blockquote>
<p>花火从小就喜欢“大哥哥”，这种喜欢起源于<strong>憧憬</strong>。但是花火与大哥哥真的合适吗？ 没有人知道，花火对大哥哥的喜欢或许已经成为了一种执着。</p>
<p>花火跟麦的关系，最初是<strong><em>出于寂寞</em></strong>的恋爱。但是他们俩实在太像了，又同时足够优秀。或许，真的他们俩在一起更合适一点，只是，他们出现在对方的生命中的时间，晚了那么一点点，被大哥哥大姐姐们捷足先登。又或者，可能早了一点点，因为这个时候他们还不够成熟，会犯一些错误。</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz104.jpg" alt="image"></p>
<blockquote>
<p>在麦的内心独白中，发出声音的总是花火的面孔</p>
</blockquote>
<p>麦也并不讨厌花火，他们是以一种互相利用的关系在一起的。但是他们彼此，对初恋太过执着了，执着到可能，看不清自己真正的内心。</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz102.jpg" alt="image"></p>
<blockquote>
<p>花火打算跟麦好好开始，但是麦爽约了</p>
</blockquote>
<p>前面提到过，他们约定互相对初恋做一个了断后，好好的在一起，但是麦到最后一刻还是放弃不了女老师。<br>这个地方，很明确的，麦对不起花火。</p>
<p>花火在鼓起勇气之后，又一次哭了。但是我们不要忘记</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz048.jpg" alt="image"></p>
<blockquote>
<p>花火还利用过麦呢</p>
</blockquote>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz105.jpg" alt="image"></p>
<blockquote>
<p>前面提到过，这点花火对不起麦</p>
</blockquote>
<p>很多人在这里骂男主<strong><em>渣男</em></strong>，但是他们俩确实是互相伤害过的。花火对男主的伤害也并不轻，公平的说，他们俩做的是半斤对八两。</p>
<p><strong>缘分</strong>就是在对的时间，遇到对的人。然而花火与麦就是在错误的时间相遇了，他们互相青睐，又互相辜负。他们这样还有资格在一起吗？</p>
<h3 id="尾声-1"><a href="#尾声-1" class="headerlink" title="尾声"></a>尾声</h3><p>剧情终于进入了尾声，短短12话的剧情，能够有如此丰富的内容，能说这部作品不优秀吗？</p>
<h5 id="—三个月后—"><a href="#—三个月后—" class="headerlink" title="—三个月后—"></a>—三个月后—</h5><h4 id="花火的成长"><a href="#花火的成长" class="headerlink" title="花火的成长"></a>花火的成长</h4><p><img src="http://oonaavjvi.bkt.clouddn.com/rz106.jpg" alt="image"></p>
<blockquote>
<p>花火学会了认真对待他人的感情</p>
</blockquote>
<p>这一段与第一话形成强烈的呼应，有些事情，只有亲身经历之后才能明白。恋爱的<strong>刻骨铭心</strong>和表白的<strong>勇气</strong>。</p>
<p>花火现在想的应该是，<em>我对他没有意思，他该有多痛</em>。<strong><em>对不起</em></strong>是明确的拒绝，<strong><em>谢谢</em></strong>透出的是微微的善意。花火真的成长了，她成为了更加美好的人，她终于有一天会得到幸福的。</p>
<h4 id="大家的成长"><a href="#大家的成长" class="headerlink" title="大家的成长"></a>大家的成长</h4><h5 id="小绘"><a href="#小绘" class="headerlink" title="小绘"></a>小绘</h5><p><img src="http://oonaavjvi.bkt.clouddn.com/rz075.jpg" alt="image"></p>
<blockquote>
<p>小绘换了发型，三个月以来没有跟花火有过交流</p>
</blockquote>
<p>花火真的不想失去小绘这个朋友啊。可能还会隐隐作痛，三个月的时间让小绘放下了。好了，这下花火和小绘终于做回了朋友，这条线终于可以说是happy end了。</p>
<h5 id="—小绘花火End—"><a href="#—小绘花火End—" class="headerlink" title="—小绘花火End—"></a><strong>—小绘花火End—</strong></h5><h5 id="乃莉子"><a href="#乃莉子" class="headerlink" title="乃莉子"></a>乃莉子</h5><p><img src="http://oonaavjvi.bkt.clouddn.com/rz107.jpg" alt="image"></p>
<blockquote>
<p>学园祭的表演，乃莉子美呆了</p>
</blockquote>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz108.jpg" alt="image"></p>
<blockquote>
<p>花火又想起了，或者说作者又强调了本动画的主题</p>
</blockquote>
<p>乃莉子并没有放弃麦，但她也开始学会不那么依赖他。</p>
<h5 id="女老师"><a href="#女老师" class="headerlink" title="女老师"></a>女老师</h5><p><img src="http://oonaavjvi.bkt.clouddn.com/rz109.jpg" alt="image"></p>
<blockquote>
<p>女老师祝福了花火，并且试图道歉</p>
</blockquote>
<p>这个地方非常有意思，而且很多人没有看懂。 女老师把抛花硬塞给了花火，意思分明就是祝愿花火早日找到真爱。最后一句看似调侃的话，隐含着：</p>
<blockquote>
<p>男老师是我抢来的<br>男老师我是不会让给你的<br>祝你幸福</p>
</blockquote>
<p>的意思。</p>
<p>女老师作为<strong><em>原人渣</em></strong>，能不能被原谅我们不讨论。男老师愿意接受，这就够了。<br>但是今天，女老师真的要变好，这一点我们是欢迎的。</p>
<p>而且，男老师不喜欢花火，并不是女老师的锅。女老师需要负责的是，为了使花火痛苦，恶意与男老师发展关系。最后哪知道假戏成真了！女老师这里应该感谢花火吧，她对花火道歉，或许也没指望被原谅？</p>
<h4 id="花火与麦怎么办？"><a href="#花火与麦怎么办？" class="headerlink" title="花火与麦怎么办？"></a>花火与麦怎么办？</h4><p><img src="http://oonaavjvi.bkt.clouddn.com/rz110.jpg" alt="image"></p>
<blockquote>
<p>官方梳理了男女主角的关系</p>
</blockquote>
<p>虽然这段感情的开始是出于寂寞的，但是两个人之间确实产生了真情。</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz111.jpg" alt="image"></p>
<blockquote>
<p>花火想叫住麦，而欲言又止</p>
</blockquote>
<p>虽然之前有麦对花火的爽约，这里花火显然打算原谅麦。</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz112.jpg" alt="image"></p>
<blockquote>
<p>麦拒绝了花火？</p>
</blockquote>
<h5 id="错了！"><a href="#错了！" class="headerlink" title="错了！"></a>错了！</h5><p>麦的这句“对不起”，可不是要拒绝花火，他是无法原谅自己。很多人可能没有仔细分析，但是麦说完之后给了一个花火一个人在公园里等待的镜头。</p>
<p>麦认为他现在没有资格跟花火在一起。</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz113.jpg" alt="image"></p>
<blockquote>
<p>两人进行了很长很长的谈话，挑选措辞，互相勉励</p>
</blockquote>
<p>不是两个人不够恩爱，只能怪他们在不恰当的时候相遇了。两个人互相为对方舔舐过伤口，也互相辜负。不是说犯的错误不能被原谅，而是一些伤口需要被时间抚平。</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz114.jpg" alt="image"></p>
<blockquote>
<p>花火与麦没有在一起</p>
</blockquote>
<p>男老师和女老师走到了一起，花火与麦算是正式失恋了。这一次，他们没有选择用身体安慰对方，而是仔细挑选着措辞，用话语安慰。他们真的成长了。</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz115.jpg" alt="image"></p>
<blockquote>
<p>最后终于解释了ED的含义</p>
</blockquote>
<p>花火和麦，他们要出发去找真的东西。或许有一天，待时光抚平了哀叹，他们还会相遇，我们也是这么希望的。但是那种可能性，就好像两条平行线交叉的概率。</p>
<p><img src="http://oonaavjvi.bkt.clouddn.com/rz116.jpg" alt="image"></p>
<blockquote>
<p>人总是要为年轻付出一些代价，只有这样才能够成长。</p>
</blockquote>
<h5 id="—花火与麦End—"><a href="#—花火与麦End—" class="headerlink" title="—花火与麦End—"></a>—花火与麦End—</h5><h3 id="写在后面的一些话"><a href="#写在后面的一些话" class="headerlink" title="写在后面的一些话"></a>写在后面的一些话</h3><p>到这里，动画版12话的内容就结束了。看到这里的朋友们，不知道有没有这样的想法。《人渣的本愿》在看似滥情的外表下，说的是一个非常积极向上的，关于<strong>勇气</strong>和<strong>成长</strong>的故事。对于情感问题的讨论，这部动画可以说是做到了面面俱到。笔者虽然愚钝，也解读出了这么多东西。</p>
<h5 id="回过头来，我们看一看OP"><a href="#回过头来，我们看一看OP" class="headerlink" title="回过头来，我们看一看OP"></a>回过头来，我们看一看OP</h5><p><img src="http://oonaavjvi.bkt.clouddn.com/rz117.jpg" alt="image"></p>
<blockquote>
<p>开场抱着一束玫瑰花的花火，和一个挥舞着玫瑰花的，跟花火有同样发型的白衣少女</p>
</blockquote>
<p>OP强烈地暗示，花火跟这个白衣少女是一个人。</p>
<h5 id="最后一话，公布了白衣少女的身份"><a href="#最后一话，公布了白衣少女的身份" class="headerlink" title="最后一话，公布了白衣少女的身份"></a>最后一话，公布了白衣少女的身份</h5><p><img src="http://oonaavjvi.bkt.clouddn.com/rz118.jpg" alt="image"></p>
<blockquote>
<p>OP中的白衣少女其实是曾经的女老师</p>
</blockquote>
<h5 id="这是想告诉我们什么？"><a href="#这是想告诉我们什么？" class="headerlink" title="这是想告诉我们什么？"></a>这是想告诉我们什么？</h5><p>这部作品中没有明说，笔者猜测，作者想说的可能是。真与假，善于恶，好与坏，很多时候就隔着一层纸。我们说女老师是这部作品中唯一一个人渣，但是如果她能早一点遇到男老师呢？她的身边如果能有一群给她带来正能量的朋友呢？或许她也不至于走上歧途，或者能够更早的醒悟过来。</p>
<p>那花火呢？ 或许在这部作品中，如果哪里出了一点问题，花火会不会堕落，成为女老师一样的情场愉快犯呢？<br><img src="http://oonaavjvi.bkt.clouddn.com/rz049.jpg" alt="image"></p>
<p>花火到底是幸还是不幸？她没有跟最喜欢的大哥哥在一起，也失去了麦(男主)。但是呢？她身边有一群美好的人们，在她寂寞的时候安慰她，在她快要坠入深渊的时候，一巴掌把她打醒。<strong>最终让花火靠自己的力量站了起来，靠自己的双脚前进</strong>。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://oonaavjvi.bkt.clouddn.com/main1.jpg&quot; alt=&quot;人渣的本愿&quot;&gt;&lt;/p&gt;
&lt;p&gt;《人渣的本愿》完结已经有两个月了，想看的应该早就看完了。这部动画描写了很多滥情的戏码，但是表达了一个非常积极正面的主题。这部动漫对情感问题的刻画入微，短短12话的内容可以说做到了面面俱到，是十分优秀的作品。但是，笔者观察到网上充满了对这部动画的曲解，和不公平的评论，也有一些好一点的，比如lex的&lt;a href=&quot;http://www.bilibili.com/video/av9501531/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;【Lex】三观炸裂？男默女泪？小评《人渣的本愿》&lt;/a&gt;。说的是不错的，但是太过照顾一部分人的心情了。另外一些称赞的声音也完全没有说到点子上。故作此文。&lt;/p&gt;
&lt;p&gt;这部动漫细节非常多，大量使用插叙，用了很多漫画的分镜技术表达复杂的感情。当然，开车情节也很多，还有百合元素。如果想仔细看的话，可以说很有内容很有看头。吃瓜群众们光是看看开车，享受一下优秀的制作，也是非常不错的。&lt;br&gt;话还是要从女主角，花火，说起。&lt;/p&gt;
&lt;h1 id=&quot;警告！本文深度剧透，没看过作品的请谨慎观看&quot;&gt;&lt;a href=&quot;#警告！本文深度剧透，没看过作品的请谨慎观看&quot; class=&quot;headerlink&quot; title=&quot;==警告！本文深度剧透，没看过作品的请谨慎观看==&quot;&gt;&lt;/a&gt;==警告！本文深度剧透，没看过作品的请谨慎观看==&lt;/h1&gt;
    
    </summary>
    
      <category term="日志" scheme="https://xhxt2008.github.io/categories/%E6%97%A5%E5%BF%97/"/>
    
    
      <category term="动漫" scheme="https://xhxt2008.github.io/tags/%E5%8A%A8%E6%BC%AB/"/>
    
      <category term="人渣的本愿" scheme="https://xhxt2008.github.io/tags/%E4%BA%BA%E6%B8%A3%E7%9A%84%E6%9C%AC%E6%84%BF/"/>
    
      <category term="剧透" scheme="https://xhxt2008.github.io/tags/%E5%89%A7%E9%80%8F/"/>
    
      <category term="评论" scheme="https://xhxt2008.github.io/tags/%E8%AF%84%E8%AE%BA/"/>
    
  </entry>
  
</feed>
